import asyncio
import os
import shutil
from pathlib import Path
from typing import Any, Dict, List, Optional

from flask import json
from torch import AwaitType
# from zmq import Message

from app.config import PROJECT_ROOT, config
from app.exceptions import ToolError
from app.llm import LLM
from app.logger import logger
from app.prompt.game_data_analysis import AGGREGATION_REPORT_PROMPT, RUN_PROMPT_CH
from app.schema import AgentMemoryManager, Message
from app.tool.base import BaseTool, ToolResult


class SpawnMultiDataAnalysisTool(BaseTool):
    """
    ä¸€ä¸ªç”¨äºåœ¨ç›®å½•ä¸­æ‰¾åˆ°æ‰€æœ‰CSV/TSVæ–‡ä»¶å¹¶å¹¶è¡Œè¿è¡Œæ•°æ®åˆ†æä»£ç†çš„å·¥å…·ã€‚
    """

    name: str = "spawn_multi_data_analysis_tool"
    description: str = (
        "å¹¶è¡Œåˆ†ææŒ‡å®šç›®å½•ä¸­çš„æ‰€æœ‰CSVå’ŒTSVæ–‡ä»¶ã€‚"
        "è¿™é‡Œï¼Œç›®æ ‡ç›®å½•æŒ‡çš„æ˜¯å­˜å‚¨å¤šä¸ªå­è¡¨ï¼ˆå³å¤šä¸ªCSV/TSVæ–‡ä»¶ï¼‰çš„æ–‡ä»¶å¤¹ã€‚"
        "å¯¹äºè¯¥ç›®å½•ä¸­çš„æ¯ä¸ªæ–‡ä»¶ï¼Œå®ƒä¼šç”Ÿæˆä¸€ä¸ªdata_analysisä»£ç†ï¼Œè¯¥ä»£ç†å°†å…¶ç»“æœ"
        "ä¿å­˜åˆ°ä¸è¾“å…¥æ–‡ä»¶è·¯å¾„å¯¹åº”çš„ä¸“ç”¨å­æ–‡ä»¶å¤¹ä¸­ã€‚"
        "æ”¯æŒæ ¹æ®analysis_aspecté…ç½®é€‰æ‹©æ€§åˆ†æç‰¹å®šç±»å‹çš„å­è¡¨ã€‚"
    )
    parameters: dict = {
        "type": "object",
        "properties": {
            "directory_path": {
                "type": "string",
                "description": "åŒ…å«å¤šä¸ªå­è¡¨ï¼ˆCSV/TSVæ–‡ä»¶ï¼‰çš„ç›®å½•çš„ç»å¯¹è·¯å¾„ã€‚",
            },
            "max_concurrent_agents": {
                "type": "integer",
                "description": "å¹¶è¡Œè¿è¡Œçš„ä»£ç†çš„æœ€å¤§æ•°é‡ã€‚",
                "default": 4,
            },
            "max_retries_per_file": {
                "type": "integer",
                "description": "å¦‚æœåˆ†æå¤±è´¥ï¼Œå•ä¸ªæ–‡ä»¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚",
                "default": 1,
            },
            "max_retry_rounds": {
                "type": "integer",
                "description": "å¤±è´¥ä»»åŠ¡çš„æœ€å¤§é‡è¯•è½®æ•°ã€‚",
                "default": 2,
            },
            "analysis_aspect": {
                "type": "object",
                "description": "åˆ†ææ–¹é¢é…ç½®ï¼ŒæŒ‡å®šè¦åˆ†æçš„å­è¡¨ç±»å‹ã€‚ä¾‹å¦‚: {'fps' : False, 'lag' : True, 'memory' : True, 'power' : False, 'temperature' : False}",
                "default": None,
            },
        },
        "required": ["directory_path"],
    }

    async def execute(
        self,
        directory_path: str,
        max_concurrent_agents: int = 4,
        max_retries_per_file: int = 1,
        max_retry_rounds: int = 2,
        analysis_aspect: Optional[Dict[str, bool]] = None,
    ) -> ToolResult:
        """
        æ‰§è¡Œç»™å®šç›®å½•ä¸­æ–‡ä»¶çš„å¹¶è¡Œåˆ†æã€‚
        """
        if not os.path.isdir(directory_path):
            raise ToolError(f"Directory not found: {directory_path}")

        parent_directory = os.path.dirname(os.path.abspath(directory_path))
        logger.info(f"The parent directory of {directory_path} is {parent_directory}")

        if config.run_flow_config.meta_data_name is None:
            raise ToolError(f"Metadata file name is not configured in run_flow_config.")
        # Find the metadata file in the parent directory
        metadata_file_path = os.path.join(
            PROJECT_ROOT, "workspace", config.run_flow_config.meta_data_name
        )

        files_to_analyze = self._list_files(directory_path, analysis_aspect)
        if not files_to_analyze:
            if analysis_aspect:
                enabled_aspects = [k for k, v in analysis_aspect.items() if v]
                return ToolResult(
                    output=f"No CSV or TSV files found matching the enabled analysis aspects: {enabled_aspects}. "
                    f"Checked directory: {directory_path}"
                )
            else:
                return ToolResult(
                    output="No CSV or TSV files found in the directory to analyze."
                )

        task_queue = asyncio.Queue()
        for file_path in files_to_analyze:
            await task_queue.put((file_path, metadata_file_path))

        results = []

        # Create worker tasks
        worker_tasks = []
        for i in range(max_concurrent_agents):
            task = asyncio.create_task(
                self._worker(f"worker-{i+1}", task_queue, max_retries_per_file, results)
            )
            worker_tasks.append(task)

        # Wait for all items in the queue to be processed
        await task_queue.join()

        # Cancel worker tasks
        for task in worker_tasks:
            task.cancel()

        # Wait for tasks to finish cancellation
        await asyncio.gather(*worker_tasks, return_exceptions=True)

        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„ä»»åŠ¡
        successful_tasks = [r for r in results if r["success"]]
        failed_tasks = [r for r in results if not r["success"]]

        # é‡è¯•æœºåˆ¶ï¼šä¸ºå¤±è´¥çš„ä»»åŠ¡é‡æ–°åˆ†é…worker
        retry_round = 1

        while failed_tasks and retry_round <= max_retry_rounds:
            logger.info(
                f"å¼€å§‹ç¬¬ {retry_round} è½®é‡è¯•ï¼Œå¤„ç† {len(failed_tasks)} ä¸ªå¤±è´¥ä»»åŠ¡"
            )

            # ä¸ºå¤±è´¥çš„ä»»åŠ¡åˆ›å»ºæ–°çš„é˜Ÿåˆ—
            retry_queue = asyncio.Queue()
            for failed_task in failed_tasks:
                await retry_queue.put((failed_task["file_path"], metadata_file_path))

            # æ¸…ç©ºresultsä¸­çš„å¤±è´¥ä»»åŠ¡è®°å½•ï¼Œå‡†å¤‡é‡æ–°è®°å½•
            results = [r for r in results if r["success"]]

            # åˆ›å»ºé‡è¯•workerä»»åŠ¡ï¼ˆä½¿ç”¨è¾ƒå°‘çš„å¹¶å‘æ•°ï¼‰
            retry_workers = min(max_concurrent_agents, len(failed_tasks))
            retry_worker_tasks = []
            for i in range(retry_workers):
                task = asyncio.create_task(
                    self._worker(
                        f"retry-worker-{retry_round}-{i+1}",
                        retry_queue,
                        max_retries_per_file,
                        results,
                    )
                )
                retry_worker_tasks.append(task)

            # ç­‰å¾…é‡è¯•é˜Ÿåˆ—å¤„ç†å®Œæˆ
            await retry_queue.join()

            # å–æ¶ˆé‡è¯•workerä»»åŠ¡
            for task in retry_worker_tasks:
                task.cancel()

            # ç­‰å¾…ä»»åŠ¡å®Œæˆå–æ¶ˆ
            await asyncio.gather(*retry_worker_tasks, return_exceptions=True)

            # é‡æ–°ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„ä»»åŠ¡
            successful_tasks = [r for r in results if r["success"]]
            failed_tasks = [r for r in results if not r["success"]]

            logger.info(
                f"ç¬¬ {retry_round} è½®é‡è¯•å®Œæˆï¼ŒæˆåŠŸ {len([r for r in results if r['success'] and f'retry-worker-{retry_round}' in r['worker_name']])} ä¸ªï¼Œå¤±è´¥ {len([r for r in results if not r['success'] and f'retry-worker-{retry_round}' in r['worker_name']])} ä¸ª"
            )
            retry_round += 1

        # æ„å»ºè¯¦ç»†çš„ç»“æœæ¶ˆæ¯
        total_successful = len(successful_tasks)
        total_failed = len(failed_tasks)

        # æ·»åŠ åˆ†ææ–¹é¢çš„ä¿¡æ¯
        result_message = ""
        if analysis_aspect:
            enabled_aspects = [k for k, v in analysis_aspect.items() if v]
            disabled_aspects = [k for k, v in analysis_aspect.items() if not v]
            result_message += f"ğŸ“Š åˆ†æé…ç½®:\n"
            result_message += f"- å¯ç”¨çš„åˆ†ææ–¹é¢: {enabled_aspects}\n"
            result_message += f"- ç¦ç”¨çš„åˆ†ææ–¹é¢: {disabled_aspects}\n"
            result_message += f"- ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶æ•°: {len(files_to_analyze)}\n\n"

        result_message += f"åˆ†æå®Œæˆï¼å¤„ç†äº† {len(files_to_analyze)} ä¸ªæ–‡ä»¶ï¼Œæœ€ç»ˆæˆåŠŸ {total_successful} ä¸ªï¼Œå¤±è´¥ {total_failed} ä¸ªã€‚"

        if retry_round > 1:
            result_message += f"ï¼ˆç»è¿‡ {retry_round-1} è½®é‡è¯•ï¼‰"
        result_message += "\n\n"

        # ç»Ÿè®¡å„è½®æ¬¡çš„æˆåŠŸæƒ…å†µ
        initial_successful = len(
            [
                r
                for r in successful_tasks
                if not r["worker_name"].startswith("retry-worker")
            ]
        )
        retry_successful = total_successful - initial_successful

        if retry_round > 1:
            result_message += f"è¯¦ç»†ç»Ÿè®¡:\n"
            result_message += f"- åˆå§‹è½®æˆåŠŸ: {initial_successful} ä¸ª\n"
            result_message += f"- é‡è¯•è½®æˆåŠŸ: {retry_successful} ä¸ª\n"
            result_message += f"- æœ€ç»ˆå¤±è´¥: {total_failed} ä¸ª\n\n"

        if successful_tasks:
            result_message += "æˆåŠŸçš„ä»»åŠ¡:\n"
            for task in successful_tasks:
                result_message += f"- æ–‡ä»¶: {task['file_path']}\n"
                result_message += f"  è¾“å‡ºç›®å½•: {task['output_dir']}\n"
                result_message += f"  å†…å­˜è·¯å¾„: {task['memory_path']}\n"
                if task.get("copied_report_path"):
                    result_message += f"  å¤åˆ¶çš„æŠ¥å‘Š: {task['copied_report_path']}\n"
                result_message += f"  å¤„ç†worker: {task['worker_name']}\n\n"

        if failed_tasks:
            result_message += "å¤±è´¥çš„ä»»åŠ¡:\n"
            for task in failed_tasks:
                result_message += f"- æ–‡ä»¶: {task['file_path']}\n"
                result_message += f"  è¾“å‡ºç›®å½•: {task['output_dir']}\n"
                result_message += f"  å¤„ç†worker: {task['worker_name']}\n\n"

        classify_result = self._classify_all_success_result(results)

        # ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        generated_summary_reports = await self._generate_category_summary_reports(
            classify_result
        )

        # åœ¨ç»“æœæ¶ˆæ¯ä¸­æ·»åŠ åˆ†ç±»æ±‡æ€»ä¿¡æ¯
        if generated_summary_reports:
            result_message += "\nç”Ÿæˆçš„åˆ†ç±»æ±‡æ€»æŠ¥å‘Š:\n"
            for category, report_path in generated_summary_reports.items():
                if report_path:
                    result_message += f"- {category.upper()} åˆ†ç±»æŠ¥å‘Š: {report_path}\n"

        return ToolResult(output=result_message)

    def _find_metadata_file(self, directory: str) -> str | None:
        """åœ¨ç»™å®šç›®å½•ä¸­æŸ¥æ‰¾å…ƒæ•°æ®æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼Œ*metadata.jsonæˆ–*column_analysis.jsonï¼‰ã€‚"""
        try:
            for filename in os.listdir(directory):
                if filename.lower().endswith(("metadata.json", "column_analysis.json")):
                    return os.path.join(directory, filename)
        except OSError as e:
            logger.error(f"Error scanning directory {directory} for metadata file: {e}")
        return None

    def _list_files(
        self, directory_path: str, analysis_aspect: Optional[Dict[str, bool]] = None
    ) -> list[str]:
        """
        é€’å½’åˆ—å‡ºç›®å½•åŠå…¶æ‰€æœ‰å­ç›®å½•ä¸­çš„ CSV å’Œ TSV æ–‡ä»¶è·¯å¾„ã€‚

        Args:
            directory_path: ç›®å½•è·¯å¾„
            analysis_aspect: åˆ†ææ–¹é¢é…ç½®ï¼Œå¦‚æœæä¾›åˆ™åªè¿”å›åŒ¹é…çš„æ–‡ä»¶

        Returns:
            åŒ¹é…æ¡ä»¶çš„CSV/TSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            matched_files = []
            for root, _, files in os.walk(directory_path):
                for f in files:
                    if f.lower().endswith((".csv", ".tsv")):
                        file_path = os.path.join(root, f)

                        # å¦‚æœæ²¡æœ‰æŒ‡å®šanalysis_aspectï¼ŒåŒ…å«æ‰€æœ‰æ–‡ä»¶
                        if not analysis_aspect:
                            matched_files.append(file_path)
                            continue

                        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ¹é…å¯ç”¨çš„åˆ†ææ–¹é¢
                        filename_lower = f.lower()
                        should_include = False

                        for aspect, enabled in analysis_aspect.items():
                            if enabled:
                                # æ”¯æŒå¤šç§åŒ¹é…æ¨¡å¼
                                aspect_lower = aspect.lower()

                                # ç›´æ¥åŒ…å«åŒ¹é…
                                if aspect_lower in filename_lower:
                                    should_include = True
                                    logger.info(
                                        f"åŒ…å«æ–‡ä»¶ {f}: åŒ¹é…å¯ç”¨çš„åˆ†ææ–¹é¢ '{aspect}' (ç›´æ¥åŒ¹é…)"
                                    )
                                    break

                                # æ”¯æŒå¸¸è§çš„å˜ä½“åŒ¹é…
                                aspect_variants = self._get_aspect_variants(
                                    aspect_lower
                                )
                                for variant in aspect_variants:
                                    if variant in filename_lower:
                                        should_include = True
                                        logger.info(
                                            f"åŒ…å«æ–‡ä»¶ {f}: åŒ¹é…å¯ç”¨çš„åˆ†ææ–¹é¢ '{aspect}' (å˜ä½“åŒ¹é…: {variant})"
                                        )
                                        break

                                if should_include:
                                    break

                        if should_include:
                            matched_files.append(file_path)
                        else:
                            logger.debug(f"è·³è¿‡æ–‡ä»¶ {f}: ä¸åŒ¹é…ä»»ä½•å¯ç”¨çš„åˆ†ææ–¹é¢")

            # è®°å½•è¿‡æ»¤ç»“æœ
            if analysis_aspect:
                enabled_aspects = [k for k, v in analysis_aspect.items() if v]
                logger.info(
                    f"æ ¹æ®åˆ†ææ–¹é¢ {enabled_aspects} è¿‡æ»¤ï¼Œæ‰¾åˆ° {len(matched_files)} ä¸ªåŒ¹é…æ–‡ä»¶"
                )

            return matched_files
        except OSError as e:
            raise ToolError(f"Error reading directory {directory_path}: {e}")

    def _get_aspect_variants(self, aspect: str) -> List[str]:
        """
        è·å–åˆ†ææ–¹é¢çš„å¸¸è§å˜ä½“ï¼Œç”¨äºæ›´çµæ´»çš„æ–‡ä»¶ååŒ¹é…

        Args:
            aspect: åˆ†ææ–¹é¢åç§° (å¦‚ 'fps', 'lag', 'memory' ç­‰)

        Returns:
            åŒ…å«å„ç§å¯èƒ½å˜ä½“çš„åˆ—è¡¨
        """
        variants = [aspect]  # åŒ…å«åŸå§‹åç§°

        # å®šä¹‰å¸¸è§çš„å˜ä½“æ˜ å°„
        variant_mapping = {
            "fps": ["fps", "framerate", "frame_rate", "frame", "render"],
            "lag": ["lag", "latency", "delay", "ping", "network"],
            "memory": ["memory", "mem", "ram", "heap", "storage"],
            "power": ["power", "battery", "energy", "consumption", "watt"],
            "temperature": ["temperature", "temp", "thermal", "heat", "celsius"],
            "cpu": ["cpu", "processor", "compute", "core"],
            "gpu": ["gpu", "graphics", "render", "shader"],
            "disk": ["disk", "storage", "io", "read", "write"],
            "network": ["network", "net", "wifi", "connection", "bandwidth"],
        }

        # å¦‚æœæ‰¾åˆ°æ˜ å°„ï¼Œä½¿ç”¨æ˜ å°„çš„å˜ä½“
        if aspect in variant_mapping:
            variants.extend(variant_mapping[aspect])

        # ç§»é™¤é‡å¤é¡¹å¹¶ä¿æŒåŸå§‹é¡ºåº
        seen = set()
        unique_variants = []
        for variant in variants:
            if variant not in seen:
                seen.add(variant)
                unique_variants.append(variant)

        return unique_variants

    def _detect_file_analysis_aspect(self, file_path: str) -> Optional[str]:
        """
        æ ¹æ®æ–‡ä»¶åæ£€æµ‹è¯¥æ–‡ä»¶å±äºå“ªä¸ªåˆ†ææ–¹é¢

        Args:
            file_path: CSV/TSVæ–‡ä»¶è·¯å¾„

        Returns:
            åŒ¹é…çš„åˆ†ææ–¹é¢åç§°ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™è¿”å›None
        """
        filename_lower = os.path.basename(file_path).lower()

        # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„åˆ†ææ–¹é¢
        all_aspects = [
            "fps",
            "lag",
            "memory",
            "power",
            "temperature",
            "cpu",
            "gpu",
            "disk",
            "network",
        ]

        for aspect in all_aspects:
            aspect_variants = self._get_aspect_variants(aspect)
            for variant in aspect_variants:
                if variant in filename_lower:
                    logger.debug(
                        f"æ–‡ä»¶ {os.path.basename(file_path)} åŒ¹é…åˆ†ææ–¹é¢ '{aspect}' (å˜ä½“: {variant})"
                    )
                    return aspect

        logger.debug(f"æ–‡ä»¶ {os.path.basename(file_path)} æœªåŒ¹é…ä»»ä½•å·²çŸ¥çš„åˆ†ææ–¹é¢")
        return None

    async def _worker(
        self, name: str, queue: asyncio.Queue, max_retries: int, results: list
    ):
        """å¸¦æœ‰é‡è¯•æœºåˆ¶çš„å·¥ä½œå‡½æ•°ï¼Œç”¨äºå¤„ç†é˜Ÿåˆ—ä¸­çš„æ–‡ä»¶ã€‚"""
        # post import
        from app.agent.DataAnalysisExpert import DataAnalysisExpert
        from app.agent.game_data_analysis import GameDataAnalysisAgent

        while True:
            try:
                file_path, metadata_path = await queue.get()
                logger.info(f"[{name}] picked up task: {file_path}")

                filename = os.path.basename(file_path)
                parent_dir = os.path.dirname(file_path)
                output_dir_name = os.path.splitext(filename)[0]
                output_dir = os.path.join(parent_dir, output_dir_name)
                total_file_path = (
                    list(Path(config.workspace_root).glob("*_cleaned.csv"))[0]
                    if config.workspace_root
                    else None
                )

                # ä»config.workspace_rootè¯»å–additional_request.json
                additional_request_path = os.path.join(
                    config.workspace_root, "additional_request.json"
                )
                additional_request = {}
                try:
                    with open(additional_request_path, "r", encoding="utf-8") as f:
                        additional_request = json.load(f)
                    logger.info(
                        f"[{name}] æˆåŠŸè¯»å–é¢å¤–è¯·æ±‚é…ç½®: {additional_request_path}"
                    )
                except FileNotFoundError:
                    logger.debug(
                        f"[{name}] æœªæ‰¾åˆ°é¢å¤–è¯·æ±‚é…ç½®æ–‡ä»¶: {additional_request_path}"
                    )
                except Exception as e:
                    logger.warning(f"[{name}] è¯»å–é¢å¤–è¯·æ±‚é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

                # æ£€æµ‹å½“å‰æ–‡ä»¶å±äºå“ªä¸ªåˆ†ææ–¹é¢
                file_aspect = self._detect_file_analysis_aspect(file_path)
                current_additional_request = None
                if file_aspect and additional_request.get(file_aspect):
                    current_additional_request = additional_request[file_aspect]
                    logger.info(
                        f"[{name}] æ–‡ä»¶ {filename} å±äº '{file_aspect}' åˆ†ææ–¹é¢ï¼Œæ‰¾åˆ°é¢å¤–è¯·æ±‚"
                    )
                else:
                    logger.debug(
                        f"[{name}] æ–‡ä»¶ {filename} æ— å¯¹åº”çš„é¢å¤–è¯·æ±‚ (æ–¹é¢: {file_aspect})"
                    )

                retry_count = 0
                task_succeeded = False
                memory_path = None

                while not task_succeeded and retry_count < max_retries:
                    if retry_count > 0:
                        logger.warning(
                            f"[{name}] Retrying task for {file_path} (Attempt {retry_count + 1}/{max_retries})"
                        )
                        # Clean up directory for a fresh start
                        if os.path.exists(output_dir):
                            shutil.rmtree(output_dir)

                    os.makedirs(output_dir, exist_ok=True)

                    prompt = RUN_PROMPT_CH.format(
                        total_file_path=total_file_path,
                        csv_file_path=file_path,
                        output_folder_path=output_dir,
                        metadata_file_path=metadata_path if metadata_path else "N/A",
                    )

                    # å¦‚æœæœ‰é¢å¤–è¯·æ±‚ï¼Œé™„åŠ åˆ°promptä¸­
                    if current_additional_request:
                        prompt += f"\n\n## é¢å¤–åˆ†æè¦æ±‚ ({file_aspect}):\n{current_additional_request}"
                        logger.info(f"[{name}] å·²å°†é¢å¤–è¯·æ±‚æ·»åŠ åˆ°promptä¸­")

                    logger.info(
                        f"[{name}] Running agent for {file_path} with prompt:\n{prompt}"
                    )

                    try:
                        # agent = GameDataAnalysisAgent()
                        agent = DataAnalysisExpert()
                        await agent.update_system_prompt(csv_file_path=file_path)
                        result = await agent.run(prompt)
                        memory_manager = AgentMemoryManager()

                        # --- Post-verification Logic ---
                        llm_verified = await self._verify_completion_with_llm(result)
                        rules_verified = self._verify_completion_with_rules(output_dir)

                        if llm_verified or rules_verified:
                            logger.info(
                                f"[{name}] Task for {file_path} completed and verified successfully."
                            )
                            # Save the final successful result
                            output_md_path = os.path.join(
                                output_dir, "analysis_summary.md"
                            )
                            with open(output_md_path, "w", encoding="utf-8") as f:
                                f.write(str(result))
                            memory_path = await memory_manager.save_memory(agent)

                            # å¤åˆ¶å¹¶ä¿®å¤Final_Report.mdæ–‡ä»¶
                            copied_report_path = self._copy_and_fix_report(output_dir)
                            if copied_report_path:
                                logger.info(
                                    f"[{name}] æŠ¥å‘Šå·²å¤åˆ¶å¹¶ä¿®å¤åˆ°: {copied_report_path}"
                                )
                            else:
                                logger.warning(f"[{name}] æŠ¥å‘Šå¤åˆ¶å¤±è´¥: {output_dir}")

                            task_succeeded = True
                        else:
                            logger.warning(
                                f"[{name}] Verification failed for {file_path}. LLM: {llm_verified}, Rules: {rules_verified}."
                            )
                            retry_count += 1

                    except Exception as e:
                        logger.error(
                            f"[{name}] Agent execution failed for {file_path}: {e}",
                            exc_info=True,
                        )
                        retry_count += 1

                # è®°å½•ä»»åŠ¡ç»“æœ
                copied_report_path = None
                if task_succeeded:
                    # å¦‚æœä»»åŠ¡æˆåŠŸï¼Œå°è¯•è·å–å¤åˆ¶çš„æŠ¥å‘Šè·¯å¾„
                    parent_dir = os.path.dirname(output_dir)
                    dir_name = os.path.basename(output_dir)
                    potential_report_path = os.path.join(
                        parent_dir, f"{dir_name}_Final_Report.md"
                    )
                    if os.path.exists(potential_report_path):
                        copied_report_path = potential_report_path

                task_result = {
                    "file_path": file_path,
                    "output_dir": output_dir,
                    "memory_path": memory_path,
                    "copied_report_path": copied_report_path,
                    "success": task_succeeded,
                    "worker_name": name,
                }
                results.append(task_result)

                if not task_succeeded:
                    logger.error(
                        f"[{name}] Task for {file_path} failed permanently after {max_retries} attempts."
                    )

            except asyncio.CancelledError:
                logger.info(f"[{name}] was cancelled.")
                break
            except Exception as e:
                logger.error(
                    f"[{name}] encountered an unexpected error: {e}", exc_info=True
                )
                break
            finally:
                # This must be called for each item gotten from the queue
                queue.task_done()

    async def _verify_completion_with_llm(self, agent_result: str) -> bool:
        """é€šè¿‡è¯¢é—®LLMè¯„ä¼°ä»£ç†çš„ç»“æœæ¥éªŒè¯ä»»åŠ¡å®Œæˆæƒ…å†µã€‚"""
        if not agent_result or not isinstance(agent_result, str):
            return False

        verification_prompt = f"""
        Based on the following output from a data analysis agent, did the agent successfully complete its analysis task and generate a meaningful summary?
        The task was to analyze a CSV file and produce findings. A successful completion should contain clear conclusions, not just code or error messages.

        Agent's final output:
        ---
        {agent_result[-1000:]}
        ---

        Based on this output, was the task successfully completed? Please answer with only "true" or "false".
        """
        try:
            llm = LLM()  # Assumes a default LLM configuration
            response = await llm.ask(
                messages=[{"role": "user", "content": verification_prompt}]
            )
            return (
                response.strip().lower() == "true" or "true" in response.strip().lower()
            )
        except Exception as e:
            logger.error(f"LLM verification failed: {e}")
            return False  # Default to failure if LLM call fails

    def _verify_completion_with_rules(self, output_dir: str) -> bool:
        """é€šè¿‡æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦éç©ºä»¥åŠæ˜¯å¦å­˜åœ¨Final_Report.mdæ–‡ä»¶æ¥éªŒè¯ä»»åŠ¡å®Œæˆæƒ…å†µã€‚"""
        if not os.path.isdir(output_dir):
            return False

        # Check if there are any files or subdirectories in the output directory
        if not os.listdir(output_dir):
            return False

        # Check if Final_Report.md exists in the output directory
        final_report_path = os.path.join(output_dir, "Final_Report.md")
        if not os.path.exists(final_report_path):
            logger.warning(
                f"ä»»åŠ¡éªŒè¯å¤±è´¥ï¼šåœ¨è¾“å‡ºç›®å½• {output_dir} ä¸­æœªæ‰¾åˆ° Final_Report.md æ–‡ä»¶"
            )
            return False

        # Additional check: ensure Final_Report.md is not empty
        try:
            with open(final_report_path, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if not content:
                    logger.warning(
                        f"ä»»åŠ¡éªŒè¯å¤±è´¥ï¼šFinal_Report.md æ–‡ä»¶ä¸ºç©º {final_report_path}"
                    )
                    return False
        except Exception as e:
            logger.warning(
                f"ä»»åŠ¡éªŒè¯å¤±è´¥ï¼šæ— æ³•è¯»å– Final_Report.md æ–‡ä»¶ {final_report_path}: {e}"
            )
            return False

        logger.info(
            f"ä»»åŠ¡éªŒè¯æˆåŠŸï¼šåœ¨è¾“å‡ºç›®å½• {output_dir} ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„ Final_Report.md æ–‡ä»¶"
        )
        return True

    def _copy_and_fix_report(self, output_dir: str) -> str | None:
        """
        å¤åˆ¶Final_Report.mdæ–‡ä»¶åˆ°ä¸Šä¸€çº§ç›®å½•å¹¶ä¿®å¤å›¾ç‰‡å¼•ç”¨è·¯å¾„

        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ŒåŒ…å«Final_Report.mdæ–‡ä»¶

        Returns:
            å¤åˆ¶åçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            # æ„å»ºæºæ–‡ä»¶è·¯å¾„
            source_report_path = os.path.join(output_dir, "Final_Report.md")
            if not os.path.exists(source_report_path):
                logger.warning(f"æºæŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {source_report_path}")
                return None

            # è·å–ç›®å½•ä¿¡æ¯
            parent_dir = os.path.dirname(output_dir)
            dir_name = os.path.basename(output_dir)

            # æ„å»ºç›®æ ‡æ–‡ä»¶è·¯å¾„
            target_report_name = f"{dir_name}_Final_Report.md"
            target_report_path = os.path.join(parent_dir, target_report_name)

            # è¯»å–æºæ–‡ä»¶å†…å®¹
            with open(source_report_path, "r", encoding="utf-8") as f:
                content = f.read()

            # ä¿®å¤å›¾ç‰‡å¼•ç”¨è·¯å¾„
            fixed_content = self._fix_image_references(content, output_dir)

            # å†™å…¥ç›®æ ‡æ–‡ä»¶
            with open(target_report_path, "w", encoding="utf-8") as f:
                f.write(fixed_content)

            logger.info(f"æŠ¥å‘Šå¤åˆ¶æˆåŠŸ: {source_report_path} -> {target_report_path}")
            return target_report_path

        except Exception as e:
            logger.error(f"å¤åˆ¶å¹¶ä¿®å¤æŠ¥å‘Šå¤±è´¥ {output_dir}: {e}", exc_info=True)
            return None

    def _fix_image_references(self, content: str, output_dir: str) -> str:
        """
        ä¿®å¤Markdownå†…å®¹ä¸­çš„å›¾ç‰‡å¼•ç”¨è·¯å¾„

        Args:
            content: åŸå§‹Markdownå†…å®¹
            output_dir: è¾“å‡ºç›®å½•çš„ç»å¯¹è·¯å¾„

        Returns:
            ä¿®å¤åçš„Markdownå†…å®¹
        """
        import re

        # åŒ¹é…Markdownå›¾ç‰‡è¯­æ³•: ![alt_text](image_path)
        image_pattern = r"!\[([^\]]*)\]\(([^)]+)\)"

        def replace_image_path(match):
            alt_text = match.group(1)
            image_path = match.group(2)

            # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œåˆ™ä¸ä¿®æ”¹
            if os.path.isabs(image_path):
                return match.group(0)

            # æ„å»ºç»å¯¹è·¯å¾„
            absolute_image_path = os.path.join(output_dir, image_path)

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(absolute_image_path):
                return f"![{alt_text}]({absolute_image_path})"
            else:
                logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {absolute_image_path}")
                return match.group(0)  # ä¿æŒåŸæ ·

        # æ›¿æ¢æ‰€æœ‰å›¾ç‰‡å¼•ç”¨
        fixed_content = re.sub(image_pattern, replace_image_path, content)
        return fixed_content

    def _classify_all_success_result(
        self, result: List[Dict[str, Any]]
    ) -> Dict[str, List[Dict[str, Any]]]:

        classify_result = {
            "fps": [],
            "lag": [],
            "memory": [],
            "other": [],
            "power": [],
            "temperature": [],
        }

        """
        {
                    "file_path": file_path,
                    "output_dir": output_dir,
                    "memory_path": memory_path,
                    "copied_report_path": copied_report_path,
                    "success": task_succeeded,
                    "worker_name": name,
                }
        """

        for task in result:
            task_file_path = task.get("file_path", "")
            task_file_name = os.path.basename(task_file_path)
            for key in classify_result.keys():
                if key in task_file_name.lower():
                    classify_result[key].append(task)
                    break

        return classify_result

    async def _generate_category_summary_reports(
        self, classify_result: Dict[str, List[Dict[str, Any]]]
    ) -> Dict[str, Optional[str]]:
        """
        ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆç»¼åˆæ±‡æ€»æŠ¥å‘Š

        Args:
            classify_result: åˆ†ç±»åçš„ä»»åŠ¡ç»“æœ

        Returns:
            æ¯ä¸ªåˆ†ç±»ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„å­—å…¸
        """
        generated_reports = {}

        for category, tasks in classify_result.items():
            if not tasks:  # è·³è¿‡ç©ºåˆ†ç±»
                generated_reports[category] = None
                continue

            # åªå¤„ç†æˆåŠŸçš„ä»»åŠ¡
            successful_tasks = [task for task in tasks if task.get("success", False)]
            if not successful_tasks:
                generated_reports[category] = None
                continue

            logger.info(
                f"å¼€å§‹ä¸º {category.upper()} åˆ†ç±»ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šï¼ŒåŒ…å« {len(successful_tasks)} ä¸ªæˆåŠŸä»»åŠ¡"
            )

            try:
                report_path = await self._create_category_summary_report(
                    category, successful_tasks
                )
                generated_reports[category] = report_path
                if report_path:
                    logger.info(
                        f"{category.upper()} åˆ†ç±»æ±‡æ€»æŠ¥å‘Šç”ŸæˆæˆåŠŸ: {report_path}"
                    )

                    # ä½¿ç”¨LLMå¯¹ç”Ÿæˆçš„æ±‡æ€»æŠ¥å‘Šè¿›è¡Œæ·±åº¦åˆ†æ
                    aggregation_success = await self._aggragete_categoty_report(
                        report_path
                    )
                    if aggregation_success:
                        logger.info(f"{category.upper()} åˆ†ç±»æŠ¥å‘ŠLLMæ±‡æ€»åˆ†æå®Œæˆ")
                    else:
                        logger.warning(f"{category.upper()} åˆ†ç±»æŠ¥å‘ŠLLMæ±‡æ€»åˆ†æå¤±è´¥")
                else:
                    logger.warning(f"{category.upper()} åˆ†ç±»æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
            except Exception as e:
                logger.error(
                    f"ç”Ÿæˆ {category.upper()} åˆ†ç±»æ±‡æ€»æŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}",
                    exc_info=True,
                )
                generated_reports[category] = None

        return generated_reports

    async def _create_category_summary_report(
        self, category: str, tasks: List[Dict[str, Any]]
    ) -> Optional[str]:
        """
        ä¸ºç‰¹å®šåˆ†ç±»åˆ›å»ºæ±‡æ€»æŠ¥å‘Š

        Args:
            category: åˆ†ç±»åç§° (fps, lag, memory, etc.)
            tasks: è¯¥åˆ†ç±»ä¸‹çš„æˆåŠŸä»»åŠ¡åˆ—è¡¨

        Returns:
            ç”Ÿæˆçš„æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        try:
            # æ”¶é›†æ‰€æœ‰æŠ¥å‘Šå†…å®¹
            device_reports = {}

            for task in tasks:
                copied_report_path = task.get("copied_report_path")
                if not copied_report_path or not os.path.exists(copied_report_path):
                    continue

                # ä»æ–‡ä»¶åæå–è®¾å¤‡å‹å·ä¿¡æ¯
                report_filename = os.path.basename(copied_report_path)
                # ä¾‹å¦‚ï¼šfps_1_subtable_Final_Report.md -> è®¾å¤‡å‹å·: 1
                device_id = self._extract_device_id_from_filename(report_filename)

                # è¯»å–æŠ¥å‘Šå†…å®¹
                with open(copied_report_path, "r", encoding="utf-8") as f:
                    report_content = f.read()

                device_reports[device_id] = {
                    "content": report_content,
                    "file_path": task.get("file_path", ""),
                    "output_dir": task.get("output_dir", ""),
                    "report_path": copied_report_path,
                }

            if not device_reports:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ° {category.upper()} åˆ†ç±»çš„æœ‰æ•ˆæŠ¥å‘Šæ–‡ä»¶")
                return None

            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            summary_content = await self._generate_summary_content(
                category, device_reports
            )

            # ç¡®å®šæ±‡æ€»æŠ¥å‘Šçš„ä¿å­˜ä½ç½®
            first_task = tasks[0]
            base_dir = os.path.dirname(
                os.path.dirname(first_task.get("output_dir", ""))
            )
            summary_report_path = os.path.join(
                base_dir, f"{category.upper()}_Summary_Report.md"
            )

            # å†™å…¥æ±‡æ€»æŠ¥å‘Š
            with open(summary_report_path, "w", encoding="utf-8") as f:
                f.write(summary_content)

            return summary_report_path

        except Exception as e:
            logger.error(
                f"åˆ›å»º {category.upper()} åˆ†ç±»æ±‡æ€»æŠ¥å‘Šå¤±è´¥: {e}", exc_info=True
            )
            return None

    def _extract_device_id_from_filename(self, filename: str) -> str:
        """
        ä»æ–‡ä»¶åä¸­æå–è®¾å¤‡ID

        Args:
            filename: æŠ¥å‘Šæ–‡ä»¶åï¼Œå¦‚ fps_1_subtable_Final_Report.md

        Returns:
            è®¾å¤‡IDï¼Œå¦‚ "1"
        """
        import re

        # åŒ¹é…æ¨¡å¼ï¼šåˆ†ç±»_æ•°å­—_subtable
        pattern = r"(\w+)_(\d+)_subtable"
        match = re.search(pattern, filename.lower())

        if match:
            return match.group(2)  # è¿”å›æ•°å­—éƒ¨åˆ†
        else:
            # å¦‚æœæ— æ³•æå–ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ ‡è¯†
            return filename.replace("_Final_Report.md", "").replace("_", "-")

    async def _generate_summary_content(
        self, category: str, device_reports: Dict[str, Dict[str, Any]]
    ) -> str:
        """
        ç”Ÿæˆåˆ†ç±»æ±‡æ€»æŠ¥å‘Šå†…å®¹

        Args:
            category: åˆ†ç±»åç§°
            device_reports: è®¾å¤‡æŠ¥å‘Šå­—å…¸

        Returns:
            æ±‡æ€»æŠ¥å‘Šçš„Markdownå†…å®¹
        """
        from datetime import datetime

        # æŠ¥å‘Šå¤´éƒ¨
        summary_lines = [
            f"# {category.upper()} æ€§èƒ½åˆ†ææ±‡æ€»æŠ¥å‘Š",
            "",
            f"**ç”Ÿæˆæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**åˆ†æåˆ†ç±»:** {category.upper()}",
            f"**è®¾å¤‡æ•°é‡:** {len(device_reports)}",
            "",
            "## æŠ¥å‘Šæ¦‚è¿°",
            "",
            f"æœ¬æŠ¥å‘Šæ±‡æ€»äº† {len(device_reports)} ä¸ªè®¾å¤‡çš„ {category.upper()} æ€§èƒ½åˆ†æç»“æœã€‚",
            "æ¯ä¸ªè®¾å¤‡çš„è¯¦ç»†åˆ†æå¦‚ä¸‹ï¼š",
            "",
            "---",
            "",
        ]

        # ä¸ºæ¯ä¸ªè®¾å¤‡æ·»åŠ æŠ¥å‘Šå†…å®¹
        for device_id, report_data in sorted(device_reports.items()):
            summary_lines.extend(
                [
                    f"## è®¾å¤‡ {device_id} - {category.upper()} æ€§èƒ½åˆ†æ",
                    "",
                    f"**åŸå§‹æ–‡ä»¶:** `{os.path.basename(report_data['file_path'])}`",
                    f"**è¾“å‡ºç›®å½•:** `{report_data['output_dir']}`",
                    f"**è¯¦ç»†æŠ¥å‘Š:** `{report_data['report_path']}`",
                    "",
                    "### åˆ†æç»“æœ",
                    "",
                ]
            )

            # æ·»åŠ åŸå§‹æŠ¥å‘Šå†…å®¹ï¼Œä½†å»é™¤é‡å¤çš„æ ‡é¢˜
            content = report_data["content"]
            content = self._clean_report_content_for_summary(content, device_id)

            summary_lines.append(content)
            summary_lines.extend(["", "---", ""])

        # æ·»åŠ æ±‡æ€»ç»“è®ºéƒ¨åˆ†
        summary_lines.extend(
            [
                "## æ€»ä½“ç»“è®º",
                "",
                f"é€šè¿‡å¯¹ {len(device_reports)} ä¸ªè®¾å¤‡çš„ {category.upper()} æ€§èƒ½åˆ†æï¼Œæˆ‘ä»¬å¾—åˆ°äº†å„è®¾å¤‡çš„è¯¦ç»†æ€§èƒ½è¡¨ç°ã€‚",
                "å…·ä½“çš„æ€§èƒ½å¯¹æ¯”å’Œå»ºè®®è¯·å‚è€ƒå„è®¾å¤‡çš„è¯¦ç»†åˆ†æç»“æœã€‚",
                "",
                "### è®¾å¤‡åˆ—è¡¨",
                "",
            ]
        )

        # æ·»åŠ è®¾å¤‡åˆ—è¡¨
        for device_id, report_data in sorted(device_reports.items()):
            summary_lines.append(
                f"- **è®¾å¤‡ {device_id}**: {os.path.basename(report_data['file_path'])}"
            )

        summary_lines.extend(
            ["", f"*æŠ¥å‘Šç”Ÿæˆå®Œæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"]
        )

        return "\n".join(summary_lines)

    def _clean_report_content_for_summary(self, content: str, device_id: str) -> str:
        """
        æ¸…ç†æŠ¥å‘Šå†…å®¹ï¼Œä½¿å…¶é€‚åˆåŒ…å«åœ¨æ±‡æ€»æŠ¥å‘Šä¸­

        Args:
            content: åŸå§‹æŠ¥å‘Šå†…å®¹
            device_id: è®¾å¤‡ID

        Returns:
            æ¸…ç†åçš„å†…å®¹
        """
        lines = content.split("\n")
        cleaned_lines = []

        skip_main_title = True  # è·³è¿‡ä¸»æ ‡é¢˜

        for line in lines:
            # è·³è¿‡é¡¶çº§æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯ # å¼€å¤´çš„ï¼‰
            if line.startswith("# ") and skip_main_title:
                skip_main_title = False
                continue

            # è°ƒæ•´æ ‡é¢˜çº§åˆ«ï¼ˆå°† ## æ”¹ä¸º ###ï¼Œ### æ”¹ä¸º ####ï¼‰
            if line.startswith("## "):
                line = "###" + line[2:]
            elif line.startswith("### "):
                line = "####" + line[3:]

            cleaned_lines.append(line)

        return "\n".join(cleaned_lines)

    async def _aggragete_categoty_report(self, report_path: str) -> bool:
        """
        ä½¿ç”¨LLMå¯¹åˆ†ç±»æ±‡æ€»æŠ¥å‘Šè¿›è¡Œæ·±åº¦åˆ†æï¼Œå¹¶å°†ç»“è®ºè¿½åŠ åˆ°æŠ¥å‘Šæœ«å°¾

        Args:
            report_path: åˆ†ç±»æ±‡æ€»æŠ¥å‘Šçš„æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦æˆåŠŸè¿½åŠ æ±‡æ€»ç»“è®º
        """
        try:
            if not os.path.exists(report_path):
                logger.warning(f"æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {report_path}")
                return False

            # è¯»å–ç°æœ‰æŠ¥å‘Šå†…å®¹
            with open(report_path, "r", encoding="utf-8") as f:
                current_content = f.read()

            # æå–åˆ†ç±»åç§°
            report_filename = os.path.basename(report_path)
            category = report_filename.replace("_Summary_Report.md", "").lower()

            logger.info(f"å¼€å§‹ä½¿ç”¨LLMåˆ†æ {category.upper()} åˆ†ç±»æ±‡æ€»æŠ¥å‘Š")

            # ä½¿ç”¨LLMè¿›è¡Œæ±‡æ€»åˆ†æ
            aggregation_result = await self._perform_llm_aggregation(
                current_content, category
            )

            if not aggregation_result:
                logger.warning(f"LLMæ±‡æ€»åˆ†æå¤±è´¥: {report_path}")
                return False

            # æ„å»ºè¦è¿½åŠ çš„å†…å®¹
            from datetime import datetime

            append_content = [
                "",
                "---",
                "",
                "## ğŸ¤– AIæ™ºèƒ½æ±‡æ€»åˆ†æ",
                "",
                f"**åˆ†ææ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                f"**åˆ†æå¯¹è±¡:** {category.upper()} æ€§èƒ½æ•°æ®",
                "",
                "### ç»¼åˆåˆ†æç»“è®º",
                "",
                aggregation_result,
                "",
                "---",
                "",
                "*æœ¬åˆ†æç”±AIç³»ç»ŸåŸºäºä»¥ä¸Šæ•°æ®è‡ªåŠ¨ç”Ÿæˆï¼Œæ—¨åœ¨æä¾›å®¢è§‚çš„æ€§èƒ½è¯„ä¼°å’Œå»ºè®®ã€‚*",
            ]

            # å°†æ±‡æ€»ç»“è®ºè¿½åŠ åˆ°æŠ¥å‘Šæœ«å°¾
            updated_content = current_content + "\n".join(append_content)

            # å†™å›æ–‡ä»¶
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(updated_content)

            logger.info(f"LLMæ±‡æ€»åˆ†æå·²è¿½åŠ åˆ°æŠ¥å‘Š: {report_path}")
            return True

        except Exception as e:
            logger.error(f"LLMæ±‡æ€»åˆ†æå¤±è´¥ {report_path}: {e}", exc_info=True)
            return False

    async def _perform_llm_aggregation(
        self, report_content: str, category: str
    ) -> Optional[str]:
        """
        ä½¿ç”¨LLMå¯¹æŠ¥å‘Šå†…å®¹è¿›è¡Œæ±‡æ€»åˆ†æ

        Args:
            report_content: å®Œæ•´çš„æ±‡æ€»æŠ¥å‘Šå†…å®¹
            category: åˆ†ç±»åç§° (fps, lag, memory, etc.)

        Returns:
            LLMç”Ÿæˆçš„æ±‡æ€»åˆ†æç»“è®º
        """
        try:
            # æˆªå–æŠ¥å‘Šå†…å®¹ä»¥é¿å…è¶…å‡ºtokené™åˆ¶
            content_for_analysis = report_content

            # æ„å»ºä¸“é—¨çš„æ±‡æ€»æç¤ºè¯
            aggregation_prompt = AGGREGATION_REPORT_PROMPT.format(
                category=category.upper(), report_content=content_for_analysis
            )

            llm = LLM()
            response = await llm.ask(
                messages=[Message.user_message(aggregation_prompt)]
            )

            if not response or len(response.strip()) < 50:
                logger.warning(f"LLMè¿”å›çš„æ±‡æ€»åˆ†æè¿‡çŸ­æˆ–ä¸ºç©º: {category}")
                return None

            logger.info(f"LLMæˆåŠŸç”Ÿæˆ {category.upper()} åˆ†ç±»çš„æ±‡æ€»åˆ†æ")
            return response.strip()

        except Exception as e:
            logger.error(f"LLMæ±‡æ€»åˆ†ææ‰§è¡Œå¤±è´¥ {category}: {e}", exc_info=True)
            return None
