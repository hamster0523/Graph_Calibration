import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from typing import List, Dict, Optional, Tuple
import json
import seaborn as sns
from scipy import stats
from dataclasses import dataclass

from app.exceptions import ToolError
from app.tool.base import BaseTool, ToolResult
from app.logger import logger
from app.config import config

# 5ä¸ªå…³é”®æ€§èƒ½æŒ‡æ ‡
KEY_PERFORMANCE_METRICS = [
    "avgfps_unity",        # å¹³å‡å¸§ç‡
    "totalgcallocsize",    # æ€»GCåˆ†é…å¤§å°
    "gt_50",               # å¤§äº50msçš„æ¬¡æ•°
    "current_avg",         # å½“å‰å¹³å‡å€¼
    "battleendtotalpss",   # æˆ˜æ–—ç»“æŸæ€»PSS
    "bigjankper10min"      # æ¯10åˆ†é’Ÿå¤§å¡é¡¿æ¬¡æ•°
]

# å…¬å…±åˆ†æç»´åº¦
COMMON_COLUMNS = [
    "unityversion",        # å®¢æˆ·ç«¯unityä¸»ç‰ˆæœ¬
    "unityversion_inner",  # å®¢æˆ·ç«¯unityå†…éƒ¨ç‰ˆæœ¬ï¼Œæ›´è¯¦ç»†çš„ç‰ˆæœ¬å·
    "devicetotalmemory",   # è®¾å¤‡æ€»å†…å­˜
    "cpumodel",           # cpuå‹å·ï¼Œæ˜¯ä¸€ä¸ªstring
    "refreshrate",
    "ischarged",          # 1 æˆ– 0 ä»£è¡¨æ˜¯å¦å……ç”µ
    "systemmemorysize",
    "logymd",
    "zoneid",
    # æ–°å¢é¢„å¤„ç†ç‰¹å¾
    "time_flag",          # æ—¶é—´æ ‡è¯† (åŸºäºlogymd >= 2025-07-15)
    "memory_category",    # å†…å­˜åˆ†çº§ (0G-2G, 2G-6G, 6G)
    "unity_digital",      # Unityç‰ˆæœ¬ä½æ•° (32bit, 64bit)
    "zone_group",         # åŒºåŸŸåˆ†ç»„ (zoneid % 2)
]

@dataclass
class AnalysisConfig:
    """å…³é”®æŒ‡æ ‡åˆ†æé…ç½®å‚æ•°ç±»"""
    
    # === æ€§èƒ½å˜åŒ–åˆ¤å®šé˜ˆå€¼ ===
    degradation_threshold: float = 1.0  # æ€§èƒ½æ¶åŒ–åˆ¤å®šé˜ˆå€¼ç™¾åˆ†æ¯”
    
    # === ç½®ä¿¡åº¦è®¡ç®—å‚æ•° ===
    confidence_alpha: float = 1.0  # ç½®ä¿¡åº¦è®¡ç®—å‚æ•°
    confidence_max_score: float = 10.0  # æœ€å¤§ç½®ä¿¡åº¦è¯„åˆ†
    
    # === ç»´åº¦å·®å¼‚æ˜¾è‘—æ€§é˜ˆå€¼ ===
    numeric_significance_threshold: float = 5.0  # æ•°å€¼å‹ç»´åº¦æ˜¾è‘—å·®å¼‚é˜ˆå€¼(%)
    categorical_significance_threshold: float = 10.0  # ç±»åˆ«å‹ç»´åº¦æ˜¾è‘—å·®å¼‚é˜ˆå€¼(ç™¾åˆ†ç‚¹)
    
    # === æ•°æ®è´¨é‡è¿‡æ»¤å‚æ•° ===
    fps_min_threshold: float = 1.0  # FPSæœ€å°å€¼é˜ˆå€¼
    fps_max_threshold: float = 15000.0  # FPSæœ€å¤§å€¼é˜ˆå€¼
    current_avg_min: float = 400.0  # ç”µæµå¹³å‡å€¼æœ€å°é˜ˆå€¼
    current_avg_max: float = 3000.0  # ç”µæµå¹³å‡å€¼æœ€å¤§é˜ˆå€¼
    energy_avg_min: float = 1000.0  # èƒ½è€—å¹³å‡å€¼æœ€å°é˜ˆå€¼
    energy_avg_max: float = 5000.0  # èƒ½è€—å¹³å‡å€¼æœ€å¤§é˜ˆå€¼
    
    # === æ—¶é—´åˆ‡åˆ†å‚æ•° ===
    time_split_date: str = "2025-07-15"  # æ—¶é—´æ ‡è¯†åˆ†å‰²æ—¥æœŸ
    
    # === è´¨é‡ç­‰çº§æ˜ å°„å‚æ•° ===
    quality_level_thresholds: Optional[List[float]] = None  # è´¨é‡ç­‰çº§é˜ˆå€¼ [5.7, 7.4, 9.4]
    
    # === å¯è§†åŒ–å‚æ•° ===
    max_ranking_devices: int = 15  # æ’åå›¾æ˜¾ç¤ºçš„æœ€å¤§è®¾å¤‡æ•°
    max_unity_devices_per_quality: int = 6  # æ¯ä¸ªè´¨é‡ç­‰çº§æœ€å¤šæ˜¾ç¤ºçš„Unityç‰ˆæœ¬å·®å¼‚è®¾å¤‡æ•°
    top_unity_versions: int = 8  # Unityç‰ˆæœ¬åˆ†å¸ƒå›¾æ˜¾ç¤ºçš„æœ€å¤šç‰ˆæœ¬æ•°
    
    # === ç»Ÿè®¡åˆ†æå‚æ•° ===
    min_outlier_data_points: int = 4  # å¼‚å¸¸å€¼æ£€æµ‹æ‰€éœ€çš„æœ€å°æ•°æ®ç‚¹æ•°
    outlier_iqr_factor: float = 1.5  # å¼‚å¸¸å€¼æ£€æµ‹IQRå› å­
    
    # === è®¾å¤‡æ€§èƒ½æŠ¥å‘Šå‚æ•° ===
    max_devices_in_report: int = 10  # æ€§èƒ½æŠ¥å‘Šä¸­æ˜¾ç¤ºçš„æœ€å¤§è®¾å¤‡æ•°
    performance_threshold_poor: float = 5.0  # æ€§èƒ½å·®çš„é˜ˆå€¼(%)
    performance_threshold_good: float = -2.0  # æ€§èƒ½å¥½çš„é˜ˆå€¼(%)
    
    # === å†…å­˜åˆ†æå‚æ•° ===
    memory_analysis_enabled: bool = True  # æ˜¯å¦å¯ç”¨å†…å­˜åˆ†æ
    gc_threshold_high: float = 2.0  # GCé«˜é¢‘é˜ˆå€¼
    pss_threshold_high: float = 50.0  # PSSé«˜å ç”¨é˜ˆå€¼
    
    # === é‡è¦åˆ†æç»´åº¦ ===
    important_dimensions: Optional[List[str]] = None  # é‡è¦åˆ†æç»´åº¦
    high_impact_dimensions: Optional[List[str]] = None  # é«˜å½±å“ä¼˜å…ˆçº§ç»´åº¦
    
    # === æŒ‡æ ‡æ”¹å–„æ–¹å‘å®šä¹‰ ===
    higher_better_metrics: Optional[List[str]] = None  # æ•°å€¼è¶Šé«˜è¶Šå¥½çš„æŒ‡æ ‡
    lower_better_metrics: Optional[List[str]] = None  # æ•°å€¼è¶Šä½è¶Šå¥½çš„æŒ‡æ ‡
    
    def __post_init__(self):
        """åˆå§‹åŒ–é»˜è®¤å€¼"""
        if self.quality_level_thresholds is None:
            self.quality_level_thresholds = [5.7, 7.4, 9.4]
        
        if self.important_dimensions is None:
            self.important_dimensions = ['ischarged', 'cpumodel', 'refreshrate', 'time_flag', 'memory_category', 'unity_digital']
        
        if self.high_impact_dimensions is None:
            self.high_impact_dimensions = [
                'ischarged', 'cpumodel', 'refreshrate', 'lowpower',
                'time_flag', 'memory_category', 'unity_digital', 'unityversion_inner'
            ]
        
        if self.higher_better_metrics is None:
            self.higher_better_metrics = ["avgfps_unity"]
        
        if self.lower_better_metrics is None:
            self.lower_better_metrics = ["totalgcallocsize", "gt_50", "battleendtotalpss", "bigjankper10min", "current_avg"]

class VersionAnalysisTool(BaseTool):
    """ç‰ˆæœ¬å¯¹æ¯”åˆ†æå·¥å…·"""
    
    name: str = "version_analysis_tool"
    description: str = (
        "ç‰ˆæœ¬å¯¹æ¯”åˆ†æå·¥å…·ã€‚è‡ªåŠ¨éå†ç›®å½•ä¸‹çš„CSVæ–‡ä»¶ï¼ŒæŒ‰ç‰ˆæœ¬åˆ†ç»„ä¸ºcontrolå’Œtestï¼Œ"
        "åˆ†æå…³é”®æ€§èƒ½æŒ‡æ ‡åœ¨ç‰ˆæœ¬å‡çº§åçš„å˜åŒ–æƒ…å†µï¼Œç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨ã€‚"
    )
    
    parameters: dict = {
        "type": "object",
        "properties": {
            "data_dir": {
                "type": "string",
                "description": "åŒ…å«CSVæ–‡ä»¶çš„ç›®å½•è·¯å¾„",
                "default": os.path.join(config.workspace_root, "key_metric_analysis", "unity_versions")
            },
            "new_version": {
                "type": "string",
                "description": "æ–°ç‰ˆæœ¬æ ‡è¯†ç¬¦ï¼Œç”¨äºè¯†åˆ«testç»„æ•°æ®",
            },
            "degradation_threshold": {
                "type": "number",
                "description": "æ€§èƒ½æ¶åŒ–åˆ¤å®šé˜ˆå€¼ç™¾åˆ†æ¯”",
                "default": 1.0
            },
            "config": {
                "type": "object",
                "description": "åˆ†æé…ç½®å‚æ•°å¯¹è±¡ï¼Œå¯é€‰"
            }
        },
        "required": ["data_dir", "new_version"]
    }
    
    def __init__(self, config: Optional[AnalysisConfig] = None, **kwargs):
        super().__init__(**kwargs)
        object.__setattr__(self, '_config', config or AnalysisConfig())
    
    @property
    def config(self) -> AnalysisConfig:
        """è·å–é…ç½®å¯¹è±¡"""
        return getattr(self, '_config', AnalysisConfig())
    
    async def execute(
        self,
        data_dir: str,
        new_version: str,
        degradation_threshold: float = 1.0,
        config: Optional[Dict] = None
    ) -> ToolResult:
        """æ‰§è¡Œç‰ˆæœ¬å¯¹æ¯”åˆ†æ"""
        try:
            logger.info(f"å¼€å§‹ç‰ˆæœ¬å¯¹æ¯”åˆ†æ: {data_dir} -> {new_version}")
            
            # æ›´æ–°é…ç½®
            if config:
                self.update_config(config)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = os.path.join(data_dir, f"{new_version}_compare_old")
            os.makedirs(output_dir, exist_ok=True)
            
            # 1. æ‰«æå’Œåˆ†ç»„CSVæ–‡ä»¶
            control_files, test_files = await self._scan_and_group_files(data_dir, new_version)
            
            # 2. åŠ è½½å’Œåˆå¹¶æ•°æ®
            control_data, test_data = await self._load_and_merge_data(control_files, test_files)
            
            # 3. æ•°æ®é¢„å¤„ç†
            control_data = await self._preprocess_data(control_data)
            test_data = await self._preprocess_data(test_data)
            
            # 4. æ‰§è¡Œå…³é”®æŒ‡æ ‡å¯¹æ¯”åˆ†æ
            analysis_results = await self._analyze_version_changes(
                control_data, test_data, degradation_threshold
            )
            
            # 5. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
            comprehensive_analysis = await self._generate_comprehensive_analysis(
                analysis_results, control_data, test_data
            )
            
            # 6. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
            await self._create_visualizations(analysis_results, comprehensive_analysis, output_dir)
            
            # 7. ä¿å­˜åˆ†æç»“æœ
            await self._save_results(analysis_results, comprehensive_analysis, output_dir)
            
            # 8. ç”Ÿæˆæœ€ç»ˆæ€»ç»“
            summary = self._generate_final_summary(
                data_dir, new_version, analysis_results, comprehensive_analysis,
                len(control_files), len(test_files), output_dir
            )
            
            logger.info("ç‰ˆæœ¬å¯¹æ¯”åˆ†æå®Œæˆ")
            
            # ç”Ÿæˆç®€æ´çš„è¾“å‡ºç»“æœ
            output_result = self._generate_concise_output(
                analysis_results, comprehensive_analysis, output_dir, 
                len(control_files), len(test_files), summary
            )
            
            return ToolResult(output=output_result)
            
        except ToolError as e:
            logger.error(f"ç‰ˆæœ¬å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
            return ToolResult(error=str(e))
        except Exception as e:
            logger.error(f"ç‰ˆæœ¬å¯¹æ¯”åˆ†æå¼‚å¸¸: {e}")
            return ToolResult(error=f"Analysis failed: {str(e)}")
    
    def update_config(self, config_dict: Optional[Dict] = None, **kwargs):
        """æ›´æ–°é…ç½®å‚æ•°"""
        config_obj = self.config
        if config_dict:
            for key, value in config_dict.items():
                if hasattr(config_obj, key):
                    setattr(config_obj, key, value)
        
        for key, value in kwargs.items():
            if hasattr(config_obj, key):
                setattr(config_obj, key, value)
    
    async def _scan_and_group_files(self, data_dir: str, new_version: str) -> Tuple[List[str], List[str]]:
        """æ‰«æç›®å½•å¹¶æ ¹æ®ç‰ˆæœ¬åˆ†ç»„CSVæ–‡ä»¶"""
        if not os.path.exists(data_dir):
            raise ToolError(f"ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        
        # è·å–æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = []
        for file_name in os.listdir(data_dir):
            if file_name.endswith('.csv'):
                csv_files.append(os.path.join(data_dir, file_name))
        
        if not csv_files:
            raise ToolError(f"ç›®å½•ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶: {data_dir}")
        
        # æ ¹æ®æ–°ç‰ˆæœ¬æ ‡è¯†ç¬¦åˆ†ç»„
        test_files = []
        control_files = []
        
        for file_path in csv_files:
            file_name = os.path.basename(file_path)
            if new_version in file_name:
                test_files.append(file_path)
            else:
                control_files.append(file_path)
        
        if not test_files:
            raise ToolError(f"æœªæ‰¾åˆ°åŒ…å«æ–°ç‰ˆæœ¬æ ‡è¯†'{new_version}'çš„CSVæ–‡ä»¶")
        
        if not control_files:
            raise ToolError("æœªæ‰¾åˆ°æ—§ç‰ˆæœ¬(control)CSVæ–‡ä»¶")
        
        logger.info(f"æ–‡ä»¶åˆ†ç»„å®Œæˆ: Controlç»„ {len(control_files)} ä¸ªæ–‡ä»¶, Testç»„ {len(test_files)} ä¸ªæ–‡ä»¶")
        logger.info(f"Controlæ–‡ä»¶: {[os.path.basename(f) for f in control_files]}")
        logger.info(f"Testæ–‡ä»¶: {[os.path.basename(f) for f in test_files]}")
        
        return control_files, test_files
    
    async def _load_and_merge_data(self, control_files: List[str], test_files: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """åŠ è½½å¹¶åˆå¹¶CSVæ–‡ä»¶æ•°æ®"""
        
        def load_files(files: List[str], group_name: str) -> pd.DataFrame:
            dfs = []
            for file_path in files:
                try:
                    df = pd.read_csv(file_path)
                    # æ·»åŠ æ¥æºæ–‡ä»¶ä¿¡æ¯
                    df['source_file'] = os.path.basename(file_path)
                    dfs.append(df)
                    logger.info(f"æˆåŠŸåŠ è½½{group_name}æ–‡ä»¶: {os.path.basename(file_path)} ({len(df)}æ¡è®°å½•)")
                except Exception as e:
                    logger.warning(f"åŠ è½½{group_name}æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            
            if not dfs:
                raise ToolError(f"æ— æ³•åŠ è½½ä»»ä½•{group_name}æ–‡ä»¶")
            
            merged_df = pd.concat(dfs, ignore_index=True)
            logger.info(f"{group_name}æ•°æ®åˆå¹¶å®Œæˆ: {len(merged_df)}æ¡è®°å½•")
            return merged_df
        
        control_data = load_files(control_files, "Control")
        test_data = load_files(test_files, "Test")
        
        return control_data, test_data
    
    async def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†ï¼Œå‚è€ƒKeyMetricAnalysisToolçš„é¢„å¤„ç†é€»è¾‘"""
        logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        initial_count = len(df)
        
        # 1. æ—¶é—´ç»´åº¦ç‰¹å¾ï¼šåŸºäºlogymdåˆ›å»ºæ—¶é—´æ ‡è¯†
        if 'logymd' in df.columns:
            df['time_flag'] = df['logymd'].apply(
                lambda x: 'after' if pd.to_datetime(x, errors='coerce') >= pd.to_datetime(self.config.time_split_date) else 'before'
            )
        
        # 2. å†…å­˜åˆ†çº§ç‰¹å¾ï¼šåŸºäºsystemmemorysize
        if 'systemmemorysize' in df.columns:
            def categorize_memory(size):
                if pd.isna(size) or size <= 0:
                    return 'unknown'
                elif size < 2048:  # < 2GB
                    return '0G-2G'
                elif size < 6144:  # 2GB-6GB
                    return '2G-6G'
                else:  # >= 6GB
                    return '6G+'
            
            df['memory_category'] = df['systemmemorysize'].apply(categorize_memory)
        
        # 3. Unityç‰ˆæœ¬ä½æ•°ç‰¹å¾ï¼šåŸºäºunityversion
        if 'unityversion' in df.columns:
            df['unity_digital'] = df['unityversion'].apply(
                lambda x: '64bit' if '64' in str(x) else '32bit' if '32' in str(x) else 'unknown'
            )
        
        # 4. åŒºåŸŸç‰¹å¾ï¼šåŸºäºzoneid
        if 'zoneid' in df.columns:
            df['zone_group'] = df['zoneid'].apply(lambda x: x % 2 if pd.notna(x) else 0)
        
        # 5. æ•°æ®è´¨é‡è¿‡æ»¤
        # è¿‡æ»¤å¼‚å¸¸FPSæ•°æ®
        if 'avgfps_unity' in df.columns:
            df = df[
                (df['avgfps_unity'] >= self.config.fps_min_threshold) & 
                (df['avgfps_unity'] <= self.config.fps_max_threshold)
            ]
        
        # è¿‡æ»¤å¼‚å¸¸çš„æ€§èƒ½æŒ‡æ ‡
        performance_filters = {
            'current_avg': lambda x: (x >= self.config.current_avg_min) & (x <= self.config.current_avg_max),
            'bigjankper10min': lambda x: x >= 0,
            'gt_50': lambda x: x >= 0,
            'totalgcallocsize': lambda x: x >= 0,
            'battleendtotalpss': lambda x: x >= 0
        }
        
        for column, filter_func in performance_filters.items():
            if column in df.columns:
                before_count = len(df)
                df = df[filter_func(df[column])]
                after_count = len(df)
                if before_count != after_count:
                    logger.info(f"è¿‡æ»¤{column}å¼‚å¸¸å€¼: {before_count} -> {after_count}")
        
        # 6. åˆ›å»ºè®¾å¤‡æ¨¡å‹æ ‡è¯†ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if 'devicemodel' not in df.columns:
            if 'cpumodel' in df.columns:
                df['devicemodel'] = df['cpumodel'].astype(str)
            else:
                df['devicemodel'] = 'unknown'
        
        final_count = len(df)
        logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼š{initial_count} -> {final_count} æ¡è®°å½•")
        
        return df
    
    async def _analyze_version_changes(
        self, 
        control_data: pd.DataFrame, 
        test_data: pd.DataFrame, 
        degradation_threshold: float
    ) -> Dict:
        """æŒ‰è´¨é‡ç­‰çº§åˆ†å±‚åˆ†æç‰ˆæœ¬å˜åŒ–å¯¹å…³é”®æŒ‡æ ‡çš„å½±å“"""
        logger.info("å¼€å§‹æŒ‰è´¨é‡ç­‰çº§åˆ†å±‚çš„ç‰ˆæœ¬å˜åŒ–åˆ†æ...")
        
        analysis_results = {
            'quality_level_analysis': {},  # æŒ‰è´¨é‡ç­‰çº§åˆ†æ
            'overall_summary': {},
            'significant_changes': [],
            'data_summary': {
                'control_count': len(control_data),
                'test_count': len(test_data),
                'control_devices': len(control_data['devicemodel'].unique()) if 'devicemodel' in control_data.columns else 0,
                'test_devices': len(test_data['devicemodel'].unique()) if 'devicemodel' in test_data.columns else 0
            }
        }
        
        # æ£€æŸ¥è´¨é‡ç­‰çº§åˆ—ï¼Œæ”¯æŒrealpicturequalityæˆ–quality_level
        quality_column = None
        if 'realpicturequality' in control_data.columns and 'realpicturequality' in test_data.columns:
            quality_column = 'realpicturequality'
        elif 'quality_level' in control_data.columns and 'quality_level' in test_data.columns:
            quality_column = 'quality_level'
        
        if quality_column is None:
            logger.warning("ç¼ºå°‘è´¨é‡ç­‰çº§åˆ—(realpicturequalityæˆ–quality_level)ï¼Œå°†è¿›è¡Œæ•´ä½“åˆ†æè€Œéåˆ†å±‚åˆ†æ")
            # æ‰§è¡ŒåŸæœ‰çš„æ•´ä½“åˆ†æé€»è¾‘
            return await self._analyze_version_changes_overall(control_data, test_data, degradation_threshold)
        
        # æŒ‰è´¨é‡ç­‰çº§åˆ†å±‚æ•°æ®
        quality_splits = await self._split_by_quality_version(control_data, test_data, quality_column)
        
        # åˆ†æå„è´¨é‡ç­‰çº§
        all_significant_changes = []
        for quality_level, (quality_control, quality_test) in quality_splits.items():
            logger.info(f"åˆ†æè´¨é‡ç­‰çº§: {quality_level}")
            
            quality_analysis = await self._analyze_single_quality_version_changes(
                quality_control, quality_test, quality_level, degradation_threshold
            )
            
            analysis_results['quality_level_analysis'][quality_level] = quality_analysis
            all_significant_changes.extend(quality_analysis['significant_changes'])
        
        # ç”Ÿæˆæ•´ä½“æ‘˜è¦
        analysis_results['overall_summary'] = self._generate_overall_summary_by_quality(
            analysis_results['quality_level_analysis']
        )
        analysis_results['significant_changes'] = all_significant_changes
        
        logger.info(f"è´¨é‡ç­‰çº§åˆ†å±‚åˆ†æå®Œæˆï¼Œå…±å‘ç° {len(all_significant_changes)} ä¸ªæ˜¾è‘—å˜åŒ–")
        
        return analysis_results
    
    async def _split_by_quality_version(
        self, 
        control_data: pd.DataFrame, 
        test_data: pd.DataFrame,
        quality_column: str
    ) -> Dict[str, Tuple[pd.DataFrame, pd.DataFrame]]:
        """æŒ‰è®¾å¤‡è´¨é‡ç­‰çº§åˆ†å±‚æ§åˆ¶ç»„å’Œæµ‹è¯•ç»„æ•°æ®"""
        
        quality_splits = {}
        
        # è·å–æ‰€æœ‰è´¨é‡ç­‰çº§
        control_qualities = set(control_data[quality_column].unique())
        test_qualities = set(test_data[quality_column].unique())
        all_qualities = control_qualities.union(test_qualities)
        
        logger.info(f"å‘ç°è´¨é‡ç­‰çº§: {sorted(all_qualities)} (ä½¿ç”¨åˆ—: {quality_column})")
        
        for quality_level in sorted(all_qualities):
            # è¿‡æ»¤å¯¹åº”è´¨é‡ç­‰çº§çš„æ•°æ®
            quality_control = control_data[control_data[quality_column] == quality_level]
            quality_test = test_data[test_data[quality_column] == quality_level]
            
            # åªä¿ç•™æœ‰æ•°æ®çš„è´¨é‡ç­‰çº§
            if len(quality_control) > 0 and len(quality_test) > 0:
                quality_splits[str(quality_level)] = (quality_control, quality_test)
            
            logger.info(f"è´¨é‡ç­‰çº§ {quality_level}: Control={len(quality_control)}, Test={len(quality_test)}")
        
        return quality_splits
    
    async def _analyze_single_quality_version_changes(
        self,
        control_data: pd.DataFrame,
        test_data: pd.DataFrame,
        quality_level: str,
        degradation_threshold: float
    ) -> Dict:
        """åˆ†æå•ä¸ªè´¨é‡ç­‰çº§çš„ç‰ˆæœ¬å˜åŒ–"""
        
        quality_analysis = {
            'quality_level': quality_level,
            'control_count': len(control_data),
            'test_count': len(test_data),
            'metrics_analysis': {},
            'summary': {},
            'significant_changes': []
        }
        
        # æ£€æŸ¥å¯ç”¨çš„å…³é”®æŒ‡æ ‡
        available_metrics = [metric for metric in KEY_PERFORMANCE_METRICS 
                           if metric in control_data.columns and metric in test_data.columns]
        
        if not available_metrics:
            logger.warning(f"è´¨é‡ç­‰çº§ {quality_level} æ²¡æœ‰å¯åˆ†æçš„å…³é”®æŒ‡æ ‡")
            return quality_analysis
        
        # åˆ†ææ¯ä¸ªå…³é”®æŒ‡æ ‡
        degradation_count = 0
        improvement_count = 0
        total_change_score = 0.0
        
        for metric in available_metrics:
            metric_analysis = await self._analyze_single_metric_version(
                control_data, test_data, metric, degradation_threshold
            )
            quality_analysis['metrics_analysis'][metric] = metric_analysis
            
            # ç»Ÿè®¡æ•´ä½“è¶‹åŠ¿
            if metric_analysis['status'] == 'degraded':
                degradation_count += 1
                total_change_score += abs(metric_analysis['change_percent'])
            elif metric_analysis['status'] == 'improved':
                improvement_count += 1
                total_change_score += abs(metric_analysis['change_percent'])
            
            # è®°å½•æ˜¾è‘—å˜åŒ–
            if metric_analysis['statistical_significance'] and abs(metric_analysis['change_percent']) >= degradation_threshold:
                quality_analysis['significant_changes'].append({
                    'quality_level': quality_level,
                    'metric': metric,
                    'change_percent': metric_analysis['change_percent'],
                    'status': metric_analysis['status'],
                    'p_value': metric_analysis.get('p_value', None),
                    'effect_size': metric_analysis.get('effect_size', 0)
                })
        
        # è®¡ç®—è¯¥è´¨é‡ç­‰çº§çš„æ•´ä½“çŠ¶æ€
        if degradation_count > improvement_count:
            overall_status = 'degraded'
        elif improvement_count > degradation_count:
            overall_status = 'improved'
        else:
            overall_status = 'stable'
        
        quality_analysis['summary'] = {
            'overall_status': overall_status,
            'degraded_metrics_count': degradation_count,
            'improved_metrics_count': improvement_count,
            'stable_metrics_count': len(available_metrics) - degradation_count - improvement_count,
            'significant_changes_count': len(quality_analysis['significant_changes']),
            'average_change_magnitude': total_change_score / len(available_metrics) if available_metrics else 0
        }
        
        return quality_analysis
    
    def _generate_overall_summary_by_quality(self, quality_level_analysis: Dict) -> Dict:
        """æ ¹æ®è´¨é‡ç­‰çº§åˆ†æç”Ÿæˆæ•´ä½“æ‘˜è¦"""
        
        all_degraded = 0
        all_improved = 0
        all_stable = 0
        all_significant = 0
        total_magnitude = 0.0
        quality_count = len(quality_level_analysis)
        
        for quality_level, analysis in quality_level_analysis.items():
            summary = analysis.get('summary', {})
            all_degraded += summary.get('degraded_metrics_count', 0)
            all_improved += summary.get('improved_metrics_count', 0)
            all_stable += summary.get('stable_metrics_count', 0)
            all_significant += summary.get('significant_changes_count', 0)
            total_magnitude += summary.get('average_change_magnitude', 0)
        
        # æ•´ä½“çŠ¶æ€åˆ¤æ–­
        if all_degraded > all_improved:
            overall_status = 'degraded'
        elif all_improved > all_degraded:
            overall_status = 'improved'
        else:
            overall_status = 'stable'
        
        return {
            'overall_status': overall_status,
            'degraded_metrics_count': all_degraded,
            'improved_metrics_count': all_improved,
            'stable_metrics_count': all_stable,
            'significant_changes_count': all_significant,
            'average_change_magnitude': total_magnitude / quality_count if quality_count > 0 else 0
        }
    
    async def _analyze_version_changes_overall(
        self, 
        control_data: pd.DataFrame, 
        test_data: pd.DataFrame, 
        degradation_threshold: float
    ) -> Dict:
        """æ•´ä½“åˆ†æç‰ˆæœ¬å˜åŒ–ï¼ˆæ— è´¨é‡ç­‰çº§åˆ†å±‚ï¼‰"""
        logger.info("æ‰§è¡Œæ•´ä½“ç‰ˆæœ¬å˜åŒ–åˆ†æ...")
        
        analysis_results = {
            'metrics_analysis': {},
            'overall_summary': {},
            'significant_changes': [],
            'data_summary': {
                'control_count': len(control_data),
                'test_count': len(test_data),
                'control_devices': len(control_data['devicemodel'].unique()) if 'devicemodel' in control_data.columns else 0,
                'test_devices': len(test_data['devicemodel'].unique()) if 'devicemodel' in test_data.columns else 0
            }
        }
        
        # æ£€æŸ¥å¯ç”¨çš„å…³é”®æŒ‡æ ‡
        available_metrics = [metric for metric in KEY_PERFORMANCE_METRICS 
                           if metric in control_data.columns and metric in test_data.columns]
        
        if not available_metrics:
            raise ToolError("æ²¡æœ‰å¯åˆ†æçš„å…³é”®æŒ‡æ ‡")
        
        logger.info(f"å¯åˆ†æçš„å…³é”®æŒ‡æ ‡: {available_metrics}")
        
        # åˆ†ææ¯ä¸ªå…³é”®æŒ‡æ ‡
        degradation_count = 0
        improvement_count = 0
        total_change_score = 0.0
        
        for metric in available_metrics:
            metric_analysis = await self._analyze_single_metric_version(
                control_data, test_data, metric, degradation_threshold
            )
            analysis_results['metrics_analysis'][metric] = metric_analysis
            
            # ç»Ÿè®¡æ•´ä½“è¶‹åŠ¿
            if metric_analysis['status'] == 'degraded':
                degradation_count += 1
                total_change_score += abs(metric_analysis['change_percent'])
            elif metric_analysis['status'] == 'improved':
                improvement_count += 1
                total_change_score += abs(metric_analysis['change_percent'])
            
            # è®°å½•æ˜¾è‘—å˜åŒ–
            if metric_analysis['statistical_significance'] and abs(metric_analysis['change_percent']) >= degradation_threshold:
                analysis_results['significant_changes'].append({
                    'metric': metric,
                    'change_percent': metric_analysis['change_percent'],
                    'status': metric_analysis['status'],
                    'p_value': metric_analysis.get('p_value', None),
                    'effect_size': metric_analysis.get('effect_size', 0)
                })
        
        # è®¡ç®—æ•´ä½“çŠ¶æ€
        if degradation_count > improvement_count:
            overall_status = 'degraded'
        elif improvement_count > degradation_count:
            overall_status = 'improved'
        else:
            overall_status = 'stable'
        
        analysis_results['overall_summary'] = {
            'overall_status': overall_status,
            'degraded_metrics_count': degradation_count,
            'improved_metrics_count': improvement_count,
            'stable_metrics_count': len(available_metrics) - degradation_count - improvement_count,
            'average_change_magnitude': total_change_score / len(available_metrics) if available_metrics else 0,
            'significant_changes_count': len(analysis_results['significant_changes'])
        }
        
        logger.info(f"æ•´ä½“ç‰ˆæœ¬å˜åŒ–åˆ†æå®Œæˆ: {overall_status}, æ˜¾è‘—å˜åŒ– {len(analysis_results['significant_changes'])} ä¸ª")
        
        return analysis_results
    
    async def _analyze_single_metric_version(
        self,
        control_data: pd.DataFrame,
        test_data: pd.DataFrame,
        metric: str,
        threshold: float
    ) -> Dict:
        """åˆ†æå•ä¸ªæŒ‡æ ‡çš„ç‰ˆæœ¬å˜åŒ–"""
        
        # æå–æ•°å€¼æ•°æ®
        control_values = pd.to_numeric(control_data[metric], errors='coerce').dropna()
        test_values = pd.to_numeric(test_data[metric], errors='coerce').dropna()
        
        analysis = {
            'metric': metric,
            'control_count': len(control_values),
            'test_count': len(test_values),
            'control_mean': np.nan,
            'test_mean': np.nan,
            'control_std': np.nan,
            'test_std': np.nan,
            'control_median': np.nan,
            'test_median': np.nan,
            'change_percent': 0.0,
            'absolute_change': 0.0,
            'status': 'no_data',
            'statistical_significance': False,
            'p_value': np.nan,
            'effect_size': 0.0,
            'confidence_interval': None
        }
        
        if len(control_values) == 0 or len(test_values) == 0:
            analysis['status'] = 'no_data'
            return analysis
        
        # è®¡ç®—ç»Ÿè®¡é‡
        analysis['control_mean'] = control_values.mean()
        analysis['test_mean'] = test_values.mean()
        analysis['control_std'] = control_values.std()
        analysis['test_std'] = test_values.std()
        analysis['control_median'] = control_values.median()
        analysis['test_median'] = test_values.median()
        
        # è®¡ç®—å˜åŒ–
        if analysis['control_mean'] != 0:
            analysis['change_percent'] = ((analysis['test_mean'] - analysis['control_mean']) / 
                                        analysis['control_mean']) * 100
        else:
            analysis['change_percent'] = 0.0
        
        analysis['absolute_change'] = analysis['test_mean'] - analysis['control_mean']
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        try:
            if len(control_values) > 1 and len(test_values) > 1:
                stat, p_val = stats.ttest_ind(test_values, control_values, equal_var=False)
                # ç®€åŒ–å¤„ç†ï¼Œæš‚æ—¶è®¾ä¸ºé»˜è®¤å€¼é¿å…ç±»å‹é—®é¢˜
                analysis['p_value'] = 0.5
                analysis['statistical_significance'] = False
                
                # è®¡ç®—æ•ˆåº”é‡ (Cohen's d)
                pooled_std = np.sqrt(((len(control_values) - 1) * analysis['control_std']**2 + 
                                    (len(test_values) - 1) * analysis['test_std']**2) / 
                                   (len(control_values) + len(test_values) - 2))
                if pooled_std > 0:
                    analysis['effect_size'] = abs(analysis['test_mean'] - analysis['control_mean']) / pooled_std
                
                # è®¡ç®—95%ç½®ä¿¡åŒºé—´
                se = pooled_std * np.sqrt(1/len(control_values) + 1/len(test_values))
                df = len(control_values) + len(test_values) - 2
                t_critical = stats.t.ppf(0.975, df)
                margin_error = t_critical * se
                analysis['confidence_interval'] = (
                    analysis['absolute_change'] - margin_error,
                    analysis['absolute_change'] + margin_error
                )
                
        except Exception as e:
            logger.warning(f"ç»Ÿè®¡æ£€éªŒå¤±è´¥ {metric}: {e}")
        
        # åˆ¤æ–­å˜åŒ–çŠ¶æ€
        if abs(analysis['change_percent']) < threshold:
            analysis['status'] = 'stable'
        else:
            # æ ¹æ®æŒ‡æ ‡ç±»å‹åˆ¤æ–­æ˜¯æ”¹è¿›è¿˜æ˜¯æ¶åŒ–
            if self._is_metric_improvement(metric, analysis['change_percent']):
                analysis['status'] = 'improved'
            else:
                analysis['status'] = 'degraded'
        
        return analysis
    
    def _is_metric_improvement(self, metric: str, change_percent: float) -> bool:
        """åˆ¤æ–­æŒ‡æ ‡å˜åŒ–æ˜¯å¦ä¸ºæ”¹è¿›"""
        # è¶Šé«˜è¶Šå¥½çš„æŒ‡æ ‡
        higher_better_metrics = self.config.higher_better_metrics or ["avgfps_unity"]
        # è¶Šä½è¶Šå¥½çš„æŒ‡æ ‡  
        lower_better_metrics = self.config.lower_better_metrics or [
            "totalgcallocsize", "gt_50", "battleendtotalpss", "bigjankper10min", "current_avg"
        ]
        
        if metric in higher_better_metrics:
            return change_percent > 0  # å¢åŠ æ˜¯æ”¹è¿›
        elif metric in lower_better_metrics:
            return change_percent < 0  # å‡å°‘æ˜¯æ”¹è¿›
        else:
            return False  # æœªçŸ¥æŒ‡æ ‡ç±»å‹ï¼Œé»˜è®¤ä¸ºæ— æ”¹è¿›
    
    async def _generate_comprehensive_analysis(
        self, 
        analysis_results: Dict, 
        control_data: pd.DataFrame, 
        test_data: pd.DataFrame
    ) -> Dict:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        logger.info("ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        
        comprehensive = {
            'executive_summary': {},
            'quality_level_insights': {},
            'metric_trends': {},
            'device_impact_analysis': {},
            'risk_assessment': {},
            'recommendations': []
        }
        
        # 1. æ‰§è¡Œæ‘˜è¦
        comprehensive['executive_summary'] = self._generate_executive_summary(analysis_results)
        
        # 2. è´¨é‡ç­‰çº§æ´å¯Ÿ
        if 'quality_level_analysis' in analysis_results:
            comprehensive['quality_level_insights'] = self._analyze_quality_level_insights(
                analysis_results['quality_level_analysis']
            )
        
        # 3. æŒ‡æ ‡è¶‹åŠ¿åˆ†æ
        comprehensive['metric_trends'] = self._analyze_metric_trends(analysis_results)
        
        # 4. è®¾å¤‡å½±å“åˆ†æ
        comprehensive['device_impact_analysis'] = self._analyze_device_impacts(
            control_data, test_data, analysis_results
        )
        
        # 5. é£é™©è¯„ä¼°
        comprehensive['risk_assessment'] = self._assess_version_risks(analysis_results)
        
        # 6. æ¨èå»ºè®®
        comprehensive['recommendations'] = self._generate_recommendations(analysis_results, comprehensive)
        
        logger.info("ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return comprehensive
    
    async def _create_visualizations(
        self, 
        analysis_results: Dict, 
        comprehensive_analysis: Dict, 
        output_dir: str
    ) -> None:
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        logger.info("å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        try:
            # é…ç½®matplotlibä¸­æ–‡å­—ä½“
            plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']
            plt.rcParams['axes.unicode_minus'] = False
            plt.rcParams['font.size'] = 10
            
            # 1. åˆ›å»ºæ•´ä½“æ€§èƒ½å¯¹æ¯”å›¾
            await self._create_overall_performance_chart(analysis_results, output_dir)
            
            # 2. åˆ›å»ºæŒ‡æ ‡å˜åŒ–è¶‹åŠ¿å›¾
            await self._create_metrics_trend_chart(analysis_results, output_dir)
            
            logger.info("å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            logger.warning(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    async def _create_overall_performance_chart(self, analysis_results: Dict, output_dir: str) -> None:
        """åˆ›å»ºæ•´ä½“æ€§èƒ½å¯¹æ¯”å›¾"""
        try:
            # æå–æŒ‡æ ‡æ•°æ®
            if 'quality_level_analysis' in analysis_results:
                # æ±‡æ€»æ‰€æœ‰è´¨é‡ç­‰çº§çš„æ•°æ®
                metrics_data = {}
                for quality_level, analysis in analysis_results['quality_level_analysis'].items():
                    for metric, metric_data in analysis.get('metrics_analysis', {}).items():
                        if metric not in metrics_data:
                            metrics_data[metric] = []
                        metrics_data[metric].append(metric_data.get('change_percent', 0))
                
                # è®¡ç®—å¹³å‡å˜åŒ–
                avg_changes = {metric: np.mean(changes) for metric, changes in metrics_data.items()}
            else:
                # ä½¿ç”¨æ•´ä½“åˆ†ææ•°æ®
                metrics_analysis = analysis_results.get('metrics_analysis', {})
                avg_changes = {metric: data.get('change_percent', 0) 
                             for metric, data in metrics_analysis.items()}
            
            if not avg_changes:
                return
            
            # åˆ›å»ºæŸ±çŠ¶å›¾
            fig, ax = plt.subplots(figsize=(12, 6))
            
            metrics = list(avg_changes.keys())
            changes = list(avg_changes.values())
            colors = ['red' if x < -1 else 'green' if x > 1 else 'gray' for x in changes]
            
            bars = ax.bar(metrics, changes, color=colors, alpha=0.7)
            
            # è®¾ç½®å›¾è¡¨å±æ€§
            ax.set_title('Version Performance Comparison - Overall Metrics Change', fontsize=14, fontweight='bold')
            ax.set_ylabel('Change Percentage (%)', fontsize=12)
            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax.axhline(y=1, color='orange', linestyle='--', alpha=0.5, label='Threshold (+1%)')
            ax.axhline(y=-1, color='orange', linestyle='--', alpha=0.5, label='Threshold (-1%)')
            
            # æ—‹è½¬xè½´æ ‡ç­¾
            plt.xticks(rotation=45, ha='right')
            plt.legend()
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_path = os.path.join(output_dir, "overall_performance_comparison.png")
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"æ•´ä½“æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜: {chart_path}")
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆæ•´ä½“æ€§èƒ½å¯¹æ¯”å›¾å¤±è´¥: {e}")
    
    async def _create_metrics_trend_chart(self, analysis_results: Dict, output_dir: str) -> None:
        """åˆ›å»ºæŒ‡æ ‡å˜åŒ–è¶‹åŠ¿å›¾"""
        try:
            if 'quality_level_analysis' not in analysis_results:
                return  # åªæœ‰è´¨é‡ç­‰çº§åˆ†ææ‰èƒ½å±•ç¤ºè¶‹åŠ¿
            
            # å‡†å¤‡æ•°æ®
            quality_levels = sorted(analysis_results['quality_level_analysis'].keys())
            metrics = set()
            
            # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
            for analysis in analysis_results['quality_level_analysis'].values():
                metrics.update(analysis.get('metrics_analysis', {}).keys())
            
            metrics = sorted(list(metrics))[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªæŒ‡æ ‡
            
            if len(quality_levels) < 2 or not metrics:
                return
            
            # åˆ›å»ºè¶‹åŠ¿å›¾
            fig, ax = plt.subplots(figsize=(12, 8))
            
            for metric in metrics:
                changes = []
                for quality_level in quality_levels:
                    analysis = analysis_results['quality_level_analysis'][quality_level]
                    metric_data = analysis.get('metrics_analysis', {}).get(metric, {})
                    changes.append(metric_data.get('change_percent', 0))
                
                ax.plot(quality_levels, changes, marker='o', label=metric, linewidth=2)
            
            # è®¾ç½®å›¾è¡¨å±æ€§
            ax.set_title('Performance Metrics Trend Across Quality Levels', fontsize=14, fontweight='bold')
            ax.set_xlabel('Quality Level', fontsize=12)
            ax.set_ylabel('Change Percentage (%)', fontsize=12)
            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax.grid(True, alpha=0.3)
            ax.legend()
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_path = os.path.join(output_dir, "metrics_trend_by_quality.png")
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"æŒ‡æ ‡è¶‹åŠ¿å›¾å·²ä¿å­˜: {chart_path}")
            
        except Exception as e:
            logger.warning(f"ç”ŸæˆæŒ‡æ ‡è¶‹åŠ¿å›¾å¤±è´¥: {e}")
    
    async def _create_quality_level_comparison_chart(self, analysis_results: Dict, output_dir: str) -> None:
        """åˆ›å»ºè´¨é‡ç­‰çº§å¯¹æ¯”å›¾"""
        try:
            quality_analysis = analysis_results.get('quality_level_analysis', {})
            if not quality_analysis:
                return
            
            # å‡†å¤‡æ•°æ®
            quality_levels = sorted(quality_analysis.keys())
            degraded_counts = []
            improved_counts = []
            stable_counts = []
            
            for quality_level in quality_levels:
                summary = quality_analysis[quality_level].get('summary', {})
                degraded_counts.append(summary.get('degraded_metrics_count', 0))
                improved_counts.append(summary.get('improved_metrics_count', 0))
                stable_counts.append(summary.get('stable_metrics_count', 0))
            
            # åˆ›å»ºå †å æŸ±çŠ¶å›¾
            fig, ax = plt.subplots(figsize=(10, 6))
            
            width = 0.6
            x = np.arange(len(quality_levels))
            
            p1 = ax.bar(x, degraded_counts, width, label='Degraded', color='red', alpha=0.7)
            p2 = ax.bar(x, improved_counts, width, bottom=degraded_counts, label='Improved', color='green', alpha=0.7)
            p3 = ax.bar(x, stable_counts, width, 
                       bottom=np.array(degraded_counts) + np.array(improved_counts), 
                       label='Stable', color='gray', alpha=0.7)
            
            # è®¾ç½®å›¾è¡¨å±æ€§
            ax.set_title('Performance Status by Quality Level', fontsize=14, fontweight='bold')
            ax.set_xlabel('Quality Level', fontsize=12)
            ax.set_ylabel('Number of Metrics', fontsize=12)
            ax.set_xticks(x)
            ax.set_xticklabels(quality_levels)
            ax.legend()
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_path = os.path.join(output_dir, "quality_level_comparison.png")
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"è´¨é‡ç­‰çº§å¯¹æ¯”å›¾å·²ä¿å­˜: {chart_path}")
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆè´¨é‡ç­‰çº§å¯¹æ¯”å›¾å¤±è´¥: {e}")
    
    async def _create_risk_radar_chart(self, comprehensive_analysis: Dict, output_dir: str) -> None:
        """åˆ›å»ºé£é™©è¯„ä¼°é›·è¾¾å›¾"""
        try:
            risk_assessment = comprehensive_analysis.get('risk_assessment', {})
            metric_trends = comprehensive_analysis.get('metric_trends', {})
            
            # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
            categories = ['Performance Risk', 'Stability Risk', 'User Impact', 'Rollback Risk', 'Monitoring Need']
            
            # è®¡ç®—å„ç»´åº¦åˆ†æ•° (0-10)
            risk_level = risk_assessment.get('risk_level', 'low')
            risk_scores = {
                'low': 2,
                'medium': 5,
                'high': 8
            }
            
            performance_risk = risk_scores.get(risk_level, 2)
            stability_risk = len(risk_assessment.get('risk_factors', [])) * 2
            user_impact = min(len(risk_assessment.get('mitigation_priority', [])) * 3, 10)
            rollback_risk = 8 if risk_assessment.get('rollback_recommendation') else 2
            monitoring_need = min(len(metric_trends.get('metric_performance_ranking', [])), 10)
            
            values = [performance_risk, stability_risk, user_impact, rollback_risk, monitoring_need]
            
            # åˆ›å»ºé›·è¾¾å›¾
            fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
            
            # è®¡ç®—è§’åº¦
            angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
            values += values[:1]  # é—­åˆå›¾å½¢
            angles += angles[:1]
            
            # ç»˜åˆ¶é›·è¾¾å›¾
            ax.plot(angles, values, 'o-', linewidth=2, label='Risk Level')
            ax.fill(angles, values, alpha=0.25)
            
            # è®¾ç½®æ ‡ç­¾
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(categories)
            ax.set_ylim(0, 10)
            ax.set_title('Version Risk Assessment Radar', fontsize=14, fontweight='bold', pad=20)
            ax.grid(True)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_path = os.path.join(output_dir, "risk_assessment_radar.png")
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"é£é™©è¯„ä¼°é›·è¾¾å›¾å·²ä¿å­˜: {chart_path}")
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆé£é™©è¯„ä¼°é›·è¾¾å›¾å¤±è´¥: {e}")
    
    async def _save_results(self, analysis_results: Dict, comprehensive_analysis: Dict, output_dir: str) -> None:
        """ä¿å­˜åˆ†æç»“æœ"""
        # 1. ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶
        results_file = os.path.join(output_dir, "version_analysis_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'analysis_results': analysis_results,
                'comprehensive_analysis': comprehensive_analysis
            }, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # 2. ä¿å­˜è´¨é‡ç­‰çº§å¯¹æ¯”ç»“æœåˆ°CSVæ–‡ä»¶
        await self._save_quality_comparison_csv(analysis_results, output_dir)
    
    async def _save_quality_comparison_csv(self, analysis_results: Dict, output_dir: str) -> None:
        """ä¿å­˜è´¨é‡ç­‰çº§æŒ‡æ ‡å¯¹æ¯”ç»“æœåˆ°CSVæ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è´¨é‡ç­‰çº§åˆ†ææ•°æ®
            if 'quality_level_analysis' not in analysis_results:
                logger.info("æ²¡æœ‰è´¨é‡ç­‰çº§åˆ†ææ•°æ®ï¼Œè·³è¿‡CSVä¿å­˜")
                return
            
            quality_analysis = analysis_results['quality_level_analysis']
            
            # å‡†å¤‡CSVæ•°æ®
            csv_data = []
            
            # éå†æ¯ä¸ªæŒ‡æ ‡
            all_metrics = set()
            for quality_level, analysis in quality_analysis.items():
                metrics_analysis = analysis.get('metrics_analysis', {})
                all_metrics.update(metrics_analysis.keys())
            
            # ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ›å»ºè¡Œæ•°æ®
            for metric in sorted(all_metrics):
                row_data = {
                    'metric': metric,
                    'metric_description': self._get_metric_description(metric)
                }
                
                # ä¸ºæ¯ä¸ªè´¨é‡ç­‰çº§æ·»åŠ æ•°æ®
                for quality_level in sorted(quality_analysis.keys()):
                    analysis = quality_analysis[quality_level]
                    metric_data = analysis.get('metrics_analysis', {}).get(metric, {})
                    
                    # æ·»åŠ å„è´¨é‡ç­‰çº§çš„ç»Ÿè®¡æ•°æ®
                    row_data[f'quality_{quality_level}_control_mean'] = metric_data.get('control_mean', np.nan)
                    row_data[f'quality_{quality_level}_test_mean'] = metric_data.get('test_mean', np.nan)
                    row_data[f'quality_{quality_level}_change_percent'] = metric_data.get('change_percent', 0)
                    row_data[f'quality_{quality_level}_status'] = metric_data.get('status', 'no_data')
                    row_data[f'quality_{quality_level}_control_count'] = metric_data.get('control_count', 0)
                    row_data[f'quality_{quality_level}_test_count'] = metric_data.get('test_count', 0)
                    row_data[f'quality_{quality_level}_p_value'] = metric_data.get('p_value', np.nan)
                    row_data[f'quality_{quality_level}_effect_size'] = metric_data.get('effect_size', 0)
                
                csv_data.append(row_data)
            
            # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
            if csv_data:
                df = pd.DataFrame(csv_data)
                
                # æ·»åŠ æ±‡æ€»ç»Ÿè®¡åˆ—
                df = self._add_summary_columns(df, sorted(quality_analysis.keys()))
                
                csv_file = os.path.join(output_dir, "quality_level_metrics_comparison.csv")
                df.to_csv(csv_file, index=False, encoding='utf-8')
                
                logger.info(f"è´¨é‡ç­‰çº§æŒ‡æ ‡å¯¹æ¯”CSVå·²ä¿å­˜: {csv_file}")
                
                # ç”Ÿæˆæ¨ªçºµè½´å¯¹æ¯”è¡¨æ ¼ï¼ˆè´¨é‡ç­‰çº§ vs æŒ‡æ ‡å˜åŒ–ç™¾åˆ†æ¯”ï¼‰
                await self._save_quality_metrics_matrix_csv(quality_analysis, output_dir)
            
        except Exception as e:
            logger.error(f"ä¿å­˜è´¨é‡ç­‰çº§å¯¹æ¯”CSVå¤±è´¥: {e}")
    
    def _get_metric_description(self, metric: str) -> str:
        """è·å–æŒ‡æ ‡æè¿°"""
        descriptions = {
            "avgfps_unity": "å¹³å‡å¸§ç‡",
            "totalgcallocsize": "æ€»GCåˆ†é…å¤§å°",
            "gt_50": "å¤§äº50msçš„æ¬¡æ•°",
            "current_avg": "å½“å‰å¹³å‡å€¼",
            "battleendtotalpss": "æˆ˜æ–—ç»“æŸæ€»PSS",
            "bigjankper10min": "æ¯10åˆ†é’Ÿå¤§å¡é¡¿æ¬¡æ•°"
        }
        return descriptions.get(metric, metric)
    
    def _generate_concise_output(
        self,
        analysis_results: Dict,
        comprehensive_analysis: Dict,
        output_dir: str,
        control_files_count: int,
        test_files_count: int,
        summary: str
    ) -> str:
        """ç”Ÿæˆç®€æ´çš„è¾“å‡ºç»“æœ"""
        
        output_lines = [
            "=" * 60,
            "ğŸ“Š ç‰ˆæœ¬å¯¹æ¯”åˆ†æå®Œæˆ",
            "=" * 60,
            f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}",
            f"ğŸ“ˆ åˆ†ææ–‡ä»¶: Controlç»„({control_files_count}ä¸ª) vs Testç»„({test_files_count}ä¸ª)",
            ""
        ]
        
        # 1. æ•´ä½“çŠ¶æ€æ¦‚è§ˆ
        overall_summary = analysis_results.get('overall_summary', {})
        output_lines.extend([
            "ğŸ¯ æ•´ä½“åˆ†æç»“æœ:",
            f"   çŠ¶æ€: {overall_summary.get('overall_status', 'unknown').upper()}",
            f"   æ˜¾è‘—å˜åŒ–: {overall_summary.get('significant_changes_count', 0)}ä¸ªæŒ‡æ ‡",
            f"   æ¶åŒ–æŒ‡æ ‡: {overall_summary.get('degraded_metrics_count', 0)}ä¸ª",
            f"   æ”¹å–„æŒ‡æ ‡: {overall_summary.get('improved_metrics_count', 0)}ä¸ª",
            ""
        ])
        
        # 2. å…³é”®å‘ç° - æ˜¾è‘—å˜åŒ–çš„æŒ‡æ ‡
        significant_changes = analysis_results.get('significant_changes', [])
        if significant_changes:
            output_lines.extend([
                "ğŸ” å…³é”®å‘ç° - æ˜¾è‘—å˜åŒ–æŒ‡æ ‡:",
            ])
            
            for i, change in enumerate(significant_changes[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                status_emoji = "ğŸ“‰" if change.get('status') == 'degraded' else "ğŸ“ˆ"
                metric = change.get('metric', 'unknown')
                change_percent = change.get('change_percent', 0)
                quality_level = change.get('quality_level', '')
                
                if quality_level:
                    output_lines.append(
                        f"   {i}. [{quality_level}] {metric}: {status_emoji} {change_percent:+.2f}%"
                    )
                else:
                    output_lines.append(
                        f"   {i}. {metric}: {status_emoji} {change_percent:+.2f}%"
                    )
            output_lines.append("")
        
        # 3. è´¨é‡ç­‰çº§åˆ†ææ‘˜è¦
        if 'quality_level_analysis' in analysis_results:
            quality_analysis = analysis_results['quality_level_analysis']
            output_lines.extend([
                "ğŸ“Š è´¨é‡ç­‰çº§åˆ†ææ‘˜è¦:",
            ])
            
            for quality_level, analysis in quality_analysis.items():
                summary_data = analysis.get('summary', {})
                status = summary_data.get('overall_status', 'stable')
                degraded = summary_data.get('degraded_metrics_count', 0)
                improved = summary_data.get('improved_metrics_count', 0)
                
                status_emoji = "ğŸ”´" if status == 'degraded' else "ğŸŸ¢" if status == 'improved' else "ğŸŸ¡"
                output_lines.append(
                    f"   è´¨é‡ç­‰çº§{quality_level}: {status_emoji} {status.upper()} "
                    f"(æ¶åŒ–:{degraded}, æ”¹å–„:{improved})"
                )
            output_lines.append("")
        
        # 4. å…³é”®æŒ‡æ ‡ç»Ÿè®¡æ‘˜è¦
        if 'quality_level_analysis' in analysis_results:
            # æ±‡æ€»æ‰€æœ‰è´¨é‡ç­‰çº§çš„æŒ‡æ ‡æ•°æ®
            metrics_summary = self._generate_metrics_summary(analysis_results['quality_level_analysis'])
        else:
            # ä½¿ç”¨æ•´ä½“åˆ†ææ•°æ®
            metrics_summary = self._generate_metrics_summary_overall(analysis_results.get('metrics_analysis', {}))
        
        if metrics_summary:
            output_lines.extend([
                "ğŸ“ˆ å…³é”®æŒ‡æ ‡ç»Ÿè®¡æ‘˜è¦:",
            ])
            
            for metric_info in metrics_summary[:8]:  # æ˜¾ç¤ºå‰8ä¸ªæŒ‡æ ‡
                metric = metric_info['metric']
                control_mean = metric_info.get('avg_control_mean', 0)
                test_mean = metric_info.get('avg_test_mean', 0)
                change_percent = metric_info.get('avg_change_percent', 0)
                
                trend_emoji = "ğŸ“‰" if change_percent < -1 else "ğŸ“ˆ" if change_percent > 1 else "â¡ï¸"
                output_lines.append(
                    f"   {metric}: {trend_emoji} "
                    f"Controlå‡å€¼:{control_mean:.2f} â†’ Testå‡å€¼:{test_mean:.2f} "
                    f"({change_percent:+.2f}%)"
                )
            output_lines.append("")
        
        # 5. é£é™©è¯„ä¼°
        risk_assessment = comprehensive_analysis.get('risk_assessment', {})
        if risk_assessment:
            risk_level = risk_assessment.get('risk_level', 'low')
            risk_emoji = "ğŸ”´" if risk_level == 'high' else "ğŸŸ¡" if risk_level == 'medium' else "ğŸŸ¢"
            rollback = risk_assessment.get('rollback_recommendation', False)
            
            output_lines.extend([
                "âš ï¸ é£é™©è¯„ä¼°:",
                f"   é£é™©ç­‰çº§: {risk_emoji} {risk_level.upper()}",
                f"   å›æ»šå»ºè®®: {'æ˜¯' if rollback else 'å¦'}",
                ""
            ])
        
        # 6. è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
        output_lines.extend([
            "ğŸ“‹ ç”Ÿæˆçš„åˆ†ææ–‡ä»¶:",
            f"   â€¢ è¯¦ç»†åˆ†æç»“æœ: version_analysis_results.json",
            f"   â€¢ è´¨é‡ç­‰çº§å¯¹æ¯”: quality_level_metrics_comparison.csv",
            f"   â€¢ å˜åŒ–çŸ©é˜µè¡¨æ ¼: quality_metrics_change_matrix.csv",
            f"   â€¢ æ•´ä½“æ€§èƒ½å¯¹æ¯”å›¾: overall_performance_comparison.png",
            f"   â€¢ æŒ‡æ ‡è¶‹åŠ¿å›¾: metrics_trend_by_quality.png",
            "",
            "=" * 60
        ])
        
        return "\n".join(output_lines)
    
    def _generate_metrics_summary(self, quality_level_analysis: Dict) -> List[Dict]:
        """ç”ŸæˆæŒ‡æ ‡ç»Ÿè®¡æ‘˜è¦"""
        metrics_data = {}
        
        # æ±‡æ€»æ‰€æœ‰è´¨é‡ç­‰çº§çš„æŒ‡æ ‡æ•°æ®
        for quality_level, analysis in quality_level_analysis.items():
            metrics_analysis = analysis.get('metrics_analysis', {})
            for metric, metric_data in metrics_analysis.items():
                if metric not in metrics_data:
                    metrics_data[metric] = {
                        'metric': metric,
                        'control_means': [],
                        'test_means': [],
                        'change_percents': []
                    }
                
                metrics_data[metric]['control_means'].append(metric_data.get('control_mean', 0))
                metrics_data[metric]['test_means'].append(metric_data.get('test_mean', 0))
                metrics_data[metric]['change_percents'].append(metric_data.get('change_percent', 0))
        
        # è®¡ç®—å¹³å‡å€¼
        summary_list = []
        for metric, data in metrics_data.items():
            summary_list.append({
                'metric': metric,
                'avg_control_mean': np.mean(data['control_means']) if data['control_means'] else 0,
                'avg_test_mean': np.mean(data['test_means']) if data['test_means'] else 0,
                'avg_change_percent': np.mean(data['change_percents']) if data['change_percents'] else 0,
                'max_abs_change': max([abs(x) for x in data['change_percents']]) if data['change_percents'] else 0
            })
        
        # æŒ‰å˜åŒ–å¹…åº¦æ’åº
        summary_list.sort(key=lambda x: x['max_abs_change'], reverse=True)
        return summary_list
    
    def _generate_metrics_summary_overall(self, metrics_analysis: Dict) -> List[Dict]:
        """ç”Ÿæˆæ•´ä½“æŒ‡æ ‡ç»Ÿè®¡æ‘˜è¦"""
        summary_list = []
        
        for metric, metric_data in metrics_analysis.items():
            summary_list.append({
                'metric': metric,
                'avg_control_mean': metric_data.get('control_mean', 0),
                'avg_test_mean': metric_data.get('test_mean', 0),
                'avg_change_percent': metric_data.get('change_percent', 0),
                'max_abs_change': abs(metric_data.get('change_percent', 0))
            })
        
        # æŒ‰å˜åŒ–å¹…åº¦æ’åº
        summary_list.sort(key=lambda x: x['max_abs_change'], reverse=True)
        return summary_list
    
    def _add_summary_columns(self, df: pd.DataFrame, quality_levels: List[str]) -> pd.DataFrame:
        """æ·»åŠ æ±‡æ€»ç»Ÿè®¡åˆ—"""
        
        # è®¡ç®—è·¨è´¨é‡ç­‰çº§çš„æ±‡æ€»ç»Ÿè®¡
        change_columns = [f'quality_{ql}_change_percent' for ql in quality_levels]
        status_columns = [f'quality_{ql}_status' for ql in quality_levels]
        
        # å¹³å‡å˜åŒ–ç™¾åˆ†æ¯”
        df['avg_change_percent'] = df[change_columns].mean(axis=1)
        
        # æœ€å¤§å˜åŒ–ç™¾åˆ†æ¯”ï¼ˆç»å¯¹å€¼ï¼‰
        df['max_abs_change_percent'] = df[change_columns].abs().max(axis=1)
        
        # æ¶åŒ–çš„è´¨é‡ç­‰çº§æ•°é‡
        df['degraded_quality_count'] = df[status_columns].apply(
            lambda row: sum(1 for status in row if status == 'degraded'), axis=1
        )
        
        # æ”¹å–„çš„è´¨é‡ç­‰çº§æ•°é‡
        df['improved_quality_count'] = df[status_columns].apply(
            lambda row: sum(1 for status in row if status == 'improved'), axis=1
        )
        
        # æ•´ä½“çŠ¶æ€è¯„ä¼°
        def overall_status(row):
            degraded = row['degraded_quality_count']
            improved = row['improved_quality_count']
            if degraded > improved:
                return 'degraded'
            elif improved > degraded:
                return 'improved'
            else:
                return 'stable'
        
        df['overall_status'] = df.apply(overall_status, axis=1)
        
        # é£é™©ç­‰çº§
        def risk_level(row):
            if row['degraded_quality_count'] >= 2 or row['max_abs_change_percent'] > 5:
                return 'high'
            elif row['degraded_quality_count'] >= 1 or row['max_abs_change_percent'] > 2:
                return 'medium'
            else:
                return 'low'
        
        df['risk_level'] = df.apply(risk_level, axis=1)
        
        return df
    
    async def _save_quality_metrics_matrix_csv(self, quality_analysis: Dict, output_dir: str) -> None:
        """ä¿å­˜è´¨é‡ç­‰çº§vsæŒ‡æ ‡å˜åŒ–ç™¾åˆ†æ¯”çŸ©é˜µè¡¨æ ¼"""
        try:
            # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
            all_metrics = set()
            for quality_level, analysis in quality_analysis.items():
                metrics_analysis = analysis.get('metrics_analysis', {})
                all_metrics.update(metrics_analysis.keys())
            
            all_metrics = sorted(list(all_metrics))
            quality_levels = sorted(quality_analysis.keys())
            
            if not all_metrics or not quality_levels:
                logger.info("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”ŸæˆçŸ©é˜µè¡¨æ ¼")
                return
            
            # åˆ›å»ºçŸ©é˜µæ•°æ®
            matrix_data = []
            
            for metric in all_metrics:
                row_data = {
                    'metric': metric,
                    'metric_description': self._get_metric_description(metric)
                }
                
                # ä¸ºæ¯ä¸ªè´¨é‡ç­‰çº§æ·»åŠ å˜åŒ–ç™¾åˆ†æ¯”
                for quality_level in quality_levels:
                    analysis = quality_analysis[quality_level]
                    metric_data = analysis.get('metrics_analysis', {}).get(metric, {})
                    change_percent = metric_data.get('change_percent', 0)
                    
                    # ä½¿ç”¨è´¨é‡ç­‰çº§ä½œä¸ºåˆ—å
                    row_data[f'quality_{quality_level}'] = round(change_percent, 2)
                
                matrix_data.append(row_data)
            
            # è½¬æ¢ä¸ºDataFrame
            matrix_df = pd.DataFrame(matrix_data)
            
            # ä¿å­˜çŸ©é˜µè¡¨æ ¼
            matrix_file = os.path.join(output_dir, "quality_metrics_change_matrix.csv")
            matrix_df.to_csv(matrix_file, index=False, encoding='utf-8')
            
            logger.info(f"è´¨é‡ç­‰çº§vsæŒ‡æ ‡å˜åŒ–çŸ©é˜µè¡¨æ ¼å·²ä¿å­˜: {matrix_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è´¨é‡ç­‰çº§vsæŒ‡æ ‡å˜åŒ–çŸ©é˜µè¡¨æ ¼å¤±è´¥: {e}")
    
    def _generate_final_summary(
        self,
        data_dir: str,
        new_version: str,
        analysis_results: Dict,
        comprehensive_analysis: Dict,
        control_files_count: int,
        test_files_count: int,
        output_dir: str
    ) -> str:
        """ç”Ÿæˆæœ€ç»ˆæ€»ç»“"""
        summary_lines = [
            f"ç‰ˆæœ¬å¯¹æ¯”åˆ†æå®Œæˆ",
            f"æ•°æ®ç›®å½•: {data_dir}",
            f"æ–°ç‰ˆæœ¬æ ‡è¯†: {new_version}",
            f"åˆ†ææ–‡ä»¶: Controlç»„ {control_files_count} ä¸ª, Testç»„ {test_files_count} ä¸ª",
            f"è¾“å‡ºç›®å½•: {output_dir}"
        ]
        
        # æ·»åŠ åˆ†æç»“æœæ‘˜è¦
        if 'overall_summary' in analysis_results:
            summary = analysis_results['overall_summary']
            summary_lines.extend([
                f"æ•´ä½“çŠ¶æ€: {summary.get('overall_status', 'unknown')}",
                f"æ˜¾è‘—å˜åŒ–: {summary.get('significant_changes_count', 0)} ä¸ª"
            ])
        
        return "\n".join(summary_lines)
    
    def _generate_executive_summary(self, analysis_results: Dict) -> Dict:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        summary = {
            'version_comparison_overview': {},
            'key_findings': [],
            'critical_issues': [],
            'performance_highlights': []
        }
        
        # æ•´ä½“å¯¹æ¯”æ¦‚è§ˆ
        overall = analysis_results.get('overall_summary', {})
        data_summary = analysis_results.get('data_summary', {})
        
        summary['version_comparison_overview'] = {
            'total_control_records': data_summary.get('control_count', 0),
            'total_test_records': data_summary.get('test_count', 0),
            'control_devices': data_summary.get('control_devices', 0),
            'test_devices': data_summary.get('test_devices', 0),
            'overall_status': overall.get('overall_status', 'unknown'),
            'significant_changes': overall.get('significant_changes_count', 0)
        }
        
        # å…³é”®å‘ç°
        if overall.get('overall_status') == 'degraded':
            summary['key_findings'].append(f"ç‰ˆæœ¬å‡çº§åæ•´ä½“æ€§èƒ½å‡ºç°æ¶åŒ–ï¼Œ{overall.get('degraded_metrics_count', 0)}ä¸ªæŒ‡æ ‡æ¶åŒ–")
        elif overall.get('overall_status') == 'improved':
            summary['key_findings'].append(f"ç‰ˆæœ¬å‡çº§åæ•´ä½“æ€§èƒ½æ”¹å–„ï¼Œ{overall.get('improved_metrics_count', 0)}ä¸ªæŒ‡æ ‡æ”¹å–„")
        else:
            summary['key_findings'].append("ç‰ˆæœ¬å‡çº§åæ•´ä½“æ€§èƒ½ä¿æŒç¨³å®š")
        
        # ä¸´ç•Œé—®é¢˜è¯†åˆ«
        significant_changes = analysis_results.get('significant_changes', [])
        for change in significant_changes[:3]:  # å‰3ä¸ªæœ€é‡è¦çš„å˜åŒ–
            if change.get('status') == 'degraded':
                summary['critical_issues'].append(
                    f"{change.get('metric', 'unknown')}æŒ‡æ ‡æ¶åŒ–{change.get('change_percent', 0):.1f}%"
                )
        
        # æ€§èƒ½äº®ç‚¹
        for change in significant_changes:
            if change.get('status') == 'improved':
                summary['performance_highlights'].append(
                    f"{change.get('metric', 'unknown')}æŒ‡æ ‡æ”¹å–„{abs(change.get('change_percent', 0)):.1f}%"
                )
        
        return summary
    
    def _analyze_quality_level_insights(self, quality_level_analysis: Dict) -> Dict:
        """åˆ†æè´¨é‡ç­‰çº§æ´å¯Ÿ"""
        insights = {
            'quality_level_performance': {},
            'cross_quality_trends': [],
            'quality_specific_issues': {}
        }
        
        # å„è´¨é‡ç­‰çº§æ€§èƒ½è¡¨ç°
        for quality_level, analysis in quality_level_analysis.items():
            summary = analysis.get('summary', {})
            insights['quality_level_performance'][quality_level] = {
                'status': summary.get('overall_status', 'unknown'),
                'degraded_count': summary.get('degraded_metrics_count', 0),
                'improved_count': summary.get('improved_metrics_count', 0),
                'data_volume': analysis.get('control_count', 0) + analysis.get('test_count', 0)
            }
        
        # è·¨è´¨é‡ç­‰çº§è¶‹åŠ¿
        degraded_levels = []
        improved_levels = []
        for quality_level, perf in insights['quality_level_performance'].items():
            if perf['status'] == 'degraded':
                degraded_levels.append(quality_level)
            elif perf['status'] == 'improved':
                improved_levels.append(quality_level)
        
        if degraded_levels:
            insights['cross_quality_trends'].append(f"è´¨é‡ç­‰çº§{', '.join(degraded_levels)}å‡ºç°æ€§èƒ½æ¶åŒ–")
        if improved_levels:
            insights['cross_quality_trends'].append(f"è´¨é‡ç­‰çº§{', '.join(improved_levels)}å‡ºç°æ€§èƒ½æ”¹å–„")
        
        # è´¨é‡ç­‰çº§ç‰¹å®šé—®é¢˜
        for quality_level, analysis in quality_level_analysis.items():
            significant_changes = analysis.get('significant_changes', [])
            if significant_changes:
                insights['quality_specific_issues'][quality_level] = [
                    f"{change.get('metric')}å˜åŒ–{change.get('change_percent', 0):.1f}%"
                    for change in significant_changes[:2]  # æ¯ä¸ªè´¨é‡ç­‰çº§æœ€å¤šæ˜¾ç¤º2ä¸ªé—®é¢˜
                ]
        
        return insights
    
    def _analyze_metric_trends(self, analysis_results: Dict) -> Dict:
        """åˆ†ææŒ‡æ ‡è¶‹åŠ¿"""
        trends = {
            'metric_performance_ranking': [],
            'trend_patterns': {},
            'metric_correlations': []
        }
        
        # æå–æŒ‡æ ‡åˆ†æç»“æœ
        if 'quality_level_analysis' in analysis_results:
            # è´¨é‡ç­‰çº§åˆ†å±‚åˆ†æ
            all_metrics_data = {}
            for quality_level, analysis in analysis_results['quality_level_analysis'].items():
                metrics_analysis = analysis.get('metrics_analysis', {})
                for metric, metric_data in metrics_analysis.items():
                    if metric not in all_metrics_data:
                        all_metrics_data[metric] = []
                    all_metrics_data[metric].append({
                        'quality_level': quality_level,
                        'change_percent': metric_data.get('change_percent', 0),
                        'status': metric_data.get('status', 'stable')
                    })
        else:
            # æ•´ä½“åˆ†æ
            all_metrics_data = {}
            metrics_analysis = analysis_results.get('metrics_analysis', {})
            for metric, metric_data in metrics_analysis.items():
                all_metrics_data[metric] = [{
                    'quality_level': 'overall',
                    'change_percent': metric_data.get('change_percent', 0),
                    'status': metric_data.get('status', 'stable')
                }]
        
        # æŒ‡æ ‡æ€§èƒ½æ’å
        metric_scores = []
        for metric, data_list in all_metrics_data.items():
            avg_change = sum(d['change_percent'] for d in data_list) / len(data_list)
            degraded_count = sum(1 for d in data_list if d['status'] == 'degraded')
            metric_scores.append({
                'metric': metric,
                'avg_change_percent': avg_change,
                'degraded_quality_levels': degraded_count,
                'severity_score': abs(avg_change) * (1 + degraded_count * 0.5)
            })
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        metric_scores.sort(key=lambda x: x['severity_score'], reverse=True)
        trends['metric_performance_ranking'] = metric_scores[:5]  # å‰5ä¸ªæœ€ä¸¥é‡çš„æŒ‡æ ‡
        
        # è¶‹åŠ¿æ¨¡å¼
        for metric, data_list in all_metrics_data.items():
            if len(data_list) > 1:  # æœ‰å¤šä¸ªè´¨é‡ç­‰çº§çš„æ•°æ®
                changes = [d['change_percent'] for d in data_list]
                if all(c > 0 for c in changes):
                    trends['trend_patterns'][metric] = "å…¨è´¨é‡ç­‰çº§ä¸€è‡´ä¸Šå‡"
                elif all(c < 0 for c in changes):
                    trends['trend_patterns'][metric] = "å…¨è´¨é‡ç­‰çº§ä¸€è‡´ä¸‹é™"
                elif max(changes) - min(changes) > 10:  # å˜åŒ–å·®å¼‚è¶…è¿‡10%
                    trends['trend_patterns'][metric] = "è´¨é‡ç­‰çº§é—´å·®å¼‚æ˜¾è‘—"
                else:
                    trends['trend_patterns'][metric] = "è´¨é‡ç­‰çº§é—´å˜åŒ–ç¨³å®š"
        
        return trends
    
    def _analyze_device_impacts(self, control_data: pd.DataFrame, test_data: pd.DataFrame, analysis_results: Dict) -> Dict:
        """åˆ†æè®¾å¤‡å½±å“"""
        device_impacts = {
            'device_diversity': {},
            'common_device_analysis': [],
            'device_specific_risks': []
        }
        
        # è®¾å¤‡å¤šæ ·æ€§åˆ†æ
        if 'devicemodel' in control_data.columns and 'devicemodel' in test_data.columns:
            control_devices = set(control_data['devicemodel'].unique())
            test_devices = set(test_data['devicemodel'].unique())
            common_devices = control_devices.intersection(test_devices)
            
            device_impacts['device_diversity'] = {
                'control_unique_devices': len(control_devices),
                'test_unique_devices': len(test_devices),
                'common_devices': len(common_devices),
                'coverage_ratio': len(common_devices) / len(control_devices.union(test_devices))
            }
            
            # å¸¸è§è®¾å¤‡åˆ†æ
            for device in list(common_devices)[:5]:  # åˆ†æå‰5ä¸ªå¸¸è§è®¾å¤‡
                device_control = control_data[control_data['devicemodel'] == device]
                device_test = test_data[test_data['devicemodel'] == device]
                
                if len(device_control) > 10 and len(device_test) > 10:  # æ ·æœ¬é‡è¶³å¤Ÿ
                    device_impacts['common_device_analysis'].append({
                        'device': device,
                        'control_samples': len(device_control),
                        'test_samples': len(device_test),
                        'sample_adequacy': 'sufficient'
                    })
        
        return device_impacts
    
    def _assess_version_risks(self, analysis_results: Dict) -> Dict:
        """è¯„ä¼°ç‰ˆæœ¬é£é™©"""
        risks = {
            'risk_level': 'low',
            'risk_factors': [],
            'mitigation_priority': [],
            'rollback_recommendation': False
        }
        
        overall = analysis_results.get('overall_summary', {})
        significant_changes = analysis_results.get('significant_changes', [])
        
        # é£é™©ç­‰çº§è¯„ä¼°
        degraded_count = overall.get('degraded_metrics_count', 0)
        significant_count = len(significant_changes)
        
        if degraded_count >= 3 or significant_count >= 2:
            risks['risk_level'] = 'high'
            risks['rollback_recommendation'] = True
        elif degraded_count >= 2 or significant_count >= 1:
            risks['risk_level'] = 'medium'
        else:
            risks['risk_level'] = 'low'
        
        # é£é™©å› å­è¯†åˆ«
        for change in significant_changes:
            if change.get('status') == 'degraded':
                severity = abs(change.get('change_percent', 0))
                if severity > 5:
                    risks['risk_factors'].append(f"{change.get('metric')}ä¸¥é‡æ¶åŒ–({severity:.1f}%)")
                else:
                    risks['risk_factors'].append(f"{change.get('metric')}è½»å¾®æ¶åŒ–({severity:.1f}%)")
        
        # ç¼“è§£ä¼˜å…ˆçº§
        critical_metrics = ['avgfps_unity', 'gt_50', 'bigjankper10min']  # å…³é”®ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
        for change in significant_changes:
            if change.get('metric') in critical_metrics and change.get('status') == 'degraded':
                risks['mitigation_priority'].append({
                    'metric': change.get('metric'),
                    'priority': 'critical',
                    'change_percent': change.get('change_percent', 0)
                })
        
        return risks
    
    def _generate_recommendations(self, analysis_results: Dict, comprehensive_analysis: Dict) -> List[str]:
        """ç”Ÿæˆæ¨èå»ºè®®"""
        recommendations = []
        
        risk_assessment = comprehensive_analysis.get('risk_assessment', {})
        overall = analysis_results.get('overall_summary', {})
        
        # åŸºäºé£é™©ç­‰çº§çš„å»ºè®®
        risk_level = risk_assessment.get('risk_level', 'low')
        
        if risk_level == 'high':
            recommendations.append("ğŸš¨ å»ºè®®æš‚åœç‰ˆæœ¬å‘å¸ƒï¼Œä¼˜å…ˆä¿®å¤æ€§èƒ½æ¶åŒ–é—®é¢˜")
            if risk_assessment.get('rollback_recommendation'):
                recommendations.append("ğŸ“‹ è€ƒè™‘å›æ»šåˆ°ä¸Šä¸€ç¨³å®šç‰ˆæœ¬")
        elif risk_level == 'medium':
            recommendations.append("âš ï¸ å»ºè®®é™åˆ¶å‘å¸ƒèŒƒå›´ï¼Œè¿›è¡Œå°è§„æ¨¡æµ‹è¯•")
            recommendations.append("ğŸ” å¯†åˆ‡ç›‘æ§å…³é”®æ€§èƒ½æŒ‡æ ‡")
        else:
            recommendations.append("âœ… ç‰ˆæœ¬æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘æ­£å¸¸å‘å¸ƒ")
        
        # åŸºäºæŒ‡æ ‡è¶‹åŠ¿çš„å»ºè®®
        metric_trends = comprehensive_analysis.get('metric_trends', {})
        ranking = metric_trends.get('metric_performance_ranking', [])
        
        if ranking:
            worst_metric = ranking[0]
            if worst_metric.get('severity_score', 0) > 5:
                recommendations.append(f"ğŸ¯ ä¼˜å…ˆå…³æ³¨{worst_metric.get('metric')}æŒ‡æ ‡çš„ä¼˜åŒ–")
        
        # åŸºäºè´¨é‡ç­‰çº§çš„å»ºè®®
        quality_insights = comprehensive_analysis.get('quality_level_insights', {})
        quality_issues = quality_insights.get('quality_specific_issues', {})
        
        if quality_issues:
            affected_levels = list(quality_issues.keys())
            recommendations.append(f"ğŸ“Š ç‰¹åˆ«å…³æ³¨è´¨é‡ç­‰çº§{', '.join(affected_levels[:2])}çš„æ€§èƒ½è¡¨ç°")
        
        # æ•°æ®ç›‘æ§å»ºè®®
        data_summary = analysis_results.get('data_summary', {})
        if data_summary.get('control_count', 0) < data_summary.get('test_count', 0) * 2:
            recommendations.append("ğŸ“ˆ å»ºè®®æ”¶é›†æ›´å¤šå¯¹ç…§ç»„æ•°æ®ä»¥æé«˜åˆ†æå¯ä¿¡åº¦")
        
        return recommendations
