import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Optional
import json
import seaborn as sns
from scipy import stats
from dataclasses import dataclass

from app.exceptions import ToolError
from app.tool.base import BaseTool, ToolResult
from app.logger import logger
from app.config import config

# 5ä¸ªå…³é”®æ€§èƒ½æŒ‡æ ‡
KEY_PERFORMANCE_METRICS = [
        "avgfps_unity",        # å¹³å‡å¸§ç‡
        "totalgcallocsize",    # æ€»GCåˆ†é…å¤§å°
        "gt_50",               # å¤§äº50msçš„æ¬¡æ•°
        "current_avg",         # å½“å‰å¹³å‡å€¼
        "battleendtotalpss",   # æˆ˜æ–—ç»“æŸæ€»PSS
        "bigjankper10min"      # æ¯10åˆ†é’Ÿå¤§å¡é¡¿æ¬¡æ•°
    ]

# å…¬å…±åˆ†æç»´åº¦
COMMON_COLUMNS = [
        "unityversion",  # å®¢æˆ·ç«¯unityä¸»ç‰ˆæœ¬
        "unityversion_inner", # å®¢æˆ·ç«¯unityå†…éƒ¨ç‰ˆæœ¬ï¼Œæ›´è¯¦ç»†çš„ç‰ˆæœ¬å·
        "devicetotalmemory", # è®¾å¤‡æ€»å†…å­˜
        "cpumodel", # cpuå‹å·ï¼Œæ˜¯ä¸€ä¸ªstring
        "refreshrate",
        "ischarged", # 1 æˆ– 0 ä»£è¡¨æ˜¯å¦å……ç”µ
        "systemmemorysize",
        "logymd",
        "zoneid",
        # æ–°å¢é¢„å¤„ç†ç‰¹å¾
        "time_flag",  # æ—¶é—´æ ‡è¯† (åŸºäºlogymd >= 2025-07-15)
        "memory_category",  # å†…å­˜åˆ†çº§ (0G-2G, 2G-6G, 6G)
        "unity_digital",  # Unityç‰ˆæœ¬ä½æ•° (32bit, 64bit)
        "zone_group",  # åŒºåŸŸåˆ†ç»„ (zoneid % 2)
    ]

@dataclass
class AnalysisConfig:
    """å…³é”®æŒ‡æ ‡åˆ†æé…ç½®å‚æ•°ç±»

    ä½¿ç”¨ç¤ºä¾‹:
        # åˆ›å»ºè‡ªå®šä¹‰é…ç½®
        custom_config = AnalysisConfig(
            degradation_threshold=2.0,  # æé«˜æ¶åŒ–é˜ˆå€¼åˆ°2%
            numeric_significance_threshold=3.0,  # é™ä½æ•°å€¼æ˜¾è‘—æ€§é˜ˆå€¼åˆ°3%
            max_ranking_devices=20,  # æ˜¾ç¤ºæ›´å¤šè®¾å¤‡æ’å
            confidence_max_score=5.0  # é™ä½æœ€å¤§ç½®ä¿¡åº¦è¯„åˆ†
        )

        # ä½¿ç”¨é…ç½®
        analyzer = KeyMetricAnalysisTool(config=custom_config)
        results = await analyzer.execute(csv_file_path, output_dir)
    """

    # === æ€§èƒ½å˜åŒ–åˆ¤å®šé˜ˆå€¼ ===
    degradation_threshold: float = 1.0  # æ€§èƒ½æ¶åŒ–åˆ¤å®šé˜ˆå€¼ç™¾åˆ†æ¯”

    # === ç½®ä¿¡åº¦è®¡ç®—å‚æ•° ===
    confidence_alpha: float = 1.0  # ç½®ä¿¡åº¦è®¡ç®—å‚æ•°
    confidence_max_score: float = 10.0  # æœ€å¤§ç½®ä¿¡åº¦è¯„åˆ†

    # === ç»´åº¦å·®å¼‚æ˜¾è‘—æ€§é˜ˆå€¼ ===
    numeric_significance_threshold: float = 5.0  # æ•°å€¼å‹ç»´åº¦æ˜¾è‘—å·®å¼‚é˜ˆå€¼(%)
    categorical_significance_threshold: float = 10.0  # ç±»åˆ«å‹ç»´åº¦æ˜¾è‘—å·®å¼‚é˜ˆå€¼(ç™¾åˆ†ç‚¹)

    # === æ•°æ®è´¨é‡è¿‡æ»¤å‚æ•° ===
    fps_min_threshold: float = 1.0  # FPSæœ€å°å€¼é˜ˆå€¼
    fps_max_threshold: float = 15000.0  # FPSæœ€å¤§å€¼é˜ˆå€¼
    current_avg_min: float = 400.0  # ç”µæµå¹³å‡å€¼æœ€å°é˜ˆå€¼
    current_avg_max: float = 3000.0  # ç”µæµå¹³å‡å€¼æœ€å¤§é˜ˆå€¼
    energy_avg_min: float = 1000.0  # èƒ½è€—å¹³å‡å€¼æœ€å°é˜ˆå€¼
    energy_avg_max: float = 5000.0  # èƒ½è€—å¹³å‡å€¼æœ€å¤§é˜ˆå€¼

    # === æ—¶é—´åˆ‡åˆ†å‚æ•° ===
    time_split_date: str = "2025-07-15"  # æ—¶é—´æ ‡è¯†åˆ†å‰²æ—¥æœŸ

    # === è®¾å¤‡è´¨é‡ç­‰çº§æ˜ å°„å‚æ•° ===
    quality_level_thresholds: Optional[List[float]] = None  # è´¨é‡ç­‰çº§é˜ˆå€¼ [5.7, 7.4, 9.4]

    # === å¯è§†åŒ–å‚æ•° ===
    max_ranking_devices: int = 15  # æ’åå›¾æ˜¾ç¤ºçš„æœ€å¤§è®¾å¤‡æ•°
    max_unity_devices_per_quality: int = 6  # æ¯ä¸ªè´¨é‡ç­‰çº§æœ€å¤šæ˜¾ç¤ºçš„Unityç‰ˆæœ¬å·®å¼‚è®¾å¤‡æ•°
    top_unity_versions: int = 8  # Unityç‰ˆæœ¬åˆ†å¸ƒå›¾æ˜¾ç¤ºçš„æœ€å¤šç‰ˆæœ¬æ•°

    # === ç»Ÿè®¡åˆ†æå‚æ•° ===
    min_outlier_data_points: int = 4  # å¼‚å¸¸å€¼æ£€æµ‹æ‰€éœ€çš„æœ€å°æ•°æ®ç‚¹æ•°
    outlier_iqr_factor: float = 1.5  # å¼‚å¸¸å€¼æ£€æµ‹IQRå› å­

    # === è®¾å¤‡æ€§èƒ½æŠ¥å‘Šå‚æ•° ===
    max_devices_in_report: int = 10  # æ€§èƒ½æŠ¥å‘Šä¸­æ˜¾ç¤ºçš„æœ€å¤§è®¾å¤‡æ•°
    performance_threshold_poor: float = 5.0  # æ€§èƒ½å·®çš„é˜ˆå€¼(%)
    performance_threshold_good: float = -2.0  # æ€§èƒ½å¥½çš„é˜ˆå€¼(%)

    # === å†…å­˜åˆ†æå‚æ•° ===
    memory_analysis_enabled: bool = True  # æ˜¯å¦å¯ç”¨å†…å­˜åˆ†æ
    gc_threshold_high: float = 2.0  # GCé«˜é¢‘é˜ˆå€¼
    pss_threshold_high: float = 50.0  # PSSé«˜å ç”¨é˜ˆå€¼

    # === é‡è¦åˆ†æç»´åº¦ ===
    important_dimensions: Optional[List[str]] = None  # é‡è¦åˆ†æç»´åº¦
    high_impact_dimensions: Optional[List[str]] = None  # é«˜å½±å“ä¼˜å…ˆçº§ç»´åº¦

    # === æŒ‡æ ‡æ”¹å–„æ–¹å‘å®šä¹‰ ===
    higher_better_metrics: Optional[List[str]] = None  # æ•°å€¼è¶Šé«˜è¶Šå¥½çš„æŒ‡æ ‡
    lower_better_metrics: Optional[List[str]] = None  # æ•°å€¼è¶Šä½è¶Šå¥½çš„æŒ‡æ ‡

    def __post_init__(self):
        """åˆå§‹åŒ–é»˜è®¤å€¼"""
        if self.quality_level_thresholds is None:
            self.quality_level_thresholds = [5.7, 7.4, 9.4]

        if self.important_dimensions is None:
            self.important_dimensions = ['ischarged', 'cpumodel', 'refreshrate', 'time_flag', 'memory_category', 'unity_digital']

        if self.high_impact_dimensions is None:
            self.high_impact_dimensions = [
                'ischarged', 'cpumodel', 'refreshrate', 'lowpower',
                'time_flag', 'memory_category', 'unity_digital', 'unityversion_inner'
            ]

        if self.higher_better_metrics is None:
            self.higher_better_metrics = ["avgfps_unity"]

        if self.lower_better_metrics is None:
            self.lower_better_metrics = ["totalgcallocsize", "gt_50", "battleendtotalpss", "bigjankper10min", "current_avg"]

    def get_quality_level_conditions(self):
        """è·å–è´¨é‡ç­‰çº§æ˜ å°„æ¡ä»¶"""
        thresholds = self.quality_level_thresholds or [5.7, 7.4, 9.4]
        return [
            (0, thresholds[0]),      # 0 < score < 5.7 -> level 1
            (thresholds[0], thresholds[1]),  # 5.7 <= score < 7.4 -> level 2
            (thresholds[1], thresholds[2]),  # 7.4 <= score < 9.4 -> level 3
            (thresholds[2], float('inf'))  # score >= 9.4 -> level 4
        ]

class KeyMetricAnalysisTool(BaseTool):

    name: str = "key_metric_analysis_tool"
    description: str = (
        "å…³é”®æŒ‡æ ‡æ·±åº¦åˆ†æå·¥å…·ã€‚æŒ‰ç…§è®¾å¤‡è´¨é‡ç­‰çº§(realpicturequality)å’Œè®¾å¤‡å‹å·(devicemodel)è¿›è¡Œå±‚çº§åˆ†æï¼Œ"
        "å¯¹5ä¸ªå…³é”®æ€§èƒ½æŒ‡æ ‡åœ¨ä¸åŒå…¬å…±ç»´åº¦ä¸‹çš„control vs testç»„å˜åŒ–è¿›è¡Œæ·±å…¥æŒ–æ˜ï¼Œ"
        "åŒ…æ‹¬å……ç”µçŠ¶æ€ã€CPUå‹å·ã€åˆ·æ–°ç‡ç­‰ç»´åº¦å¯¹æ€§èƒ½çš„å½±å“åˆ†æã€‚"
    )

    parameters: dict = {
        "type": "object",
        "properties": {
            "csv_file_path": {
                "type": "string",
                "description": "åŒ…å«å…³é”®æŒ‡æ ‡æ•°æ®çš„CSVæ–‡ä»¶è·¯å¾„",
                "default": os.path.join(config.workspace_root, "key_metric.csv")
            },
            "group_column": {
                "type": "string",
                "description": "æ ‡è¯†control/testç»„çš„åˆ—å",
                "default": "param_value"
            },
            "control_value": {
                "type": "string",
                "description": "controlç»„çš„æ ‡è¯†å€¼",
                "default": "control"
            },
            "test_value": {
                "type": "string",
                "description": "testç»„çš„æ ‡è¯†å€¼",
                "default": "test"
            },
            "degradation_threshold": {
                "type": "number",
                "description": "æ€§èƒ½æ¶åŒ–åˆ¤å®šé˜ˆå€¼ç™¾åˆ†æ¯”",
                "default": 1.0
            },
            "confidence_alpha": {
                "type": "number",
                "description": "ç½®ä¿¡åº¦è®¡ç®—å‚æ•°",
                "default": 1.0
            },
            "config": {
                "type": "object",
                "description": "åˆ†æé…ç½®å‚æ•°å¯¹è±¡ï¼Œå¯é€‰ï¼Œç”¨äºè¦†ç›–é»˜è®¤é…ç½®"
            }
        },
        "required": ["csv_file_path"]
    }

    def __init__(self, config: Optional[AnalysisConfig] = None, **kwargs):
        super().__init__(**kwargs)
        # ä½¿ç”¨ object.__setattr__ ç»•è¿‡ Pydantic çš„å­—æ®µæ£€æŸ¥
        object.__setattr__(self, '_config', config or AnalysisConfig())

    @property
    def config(self) -> AnalysisConfig:
        """è·å–é…ç½®å¯¹è±¡"""
        return getattr(self, '_config', AnalysisConfig())

    def update_config(self, config_dict: Optional[Dict] = None, **kwargs):
        """æ›´æ–°é…ç½®å‚æ•°"""
        config_obj = self.config
        if config_dict:
            for key, value in config_dict.items():
                if hasattr(config_obj, key):
                    setattr(config_obj, key, value)

        for key, value in kwargs.items():
            if hasattr(config_obj, key):
                setattr(config_obj, key, value)

    def _get_available_key_metrics(self):
        """è·å–å¯ç”¨çš„å…³é”®æŒ‡æ ‡"""
        return getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)

    def _get_available_common_columns(self):
        """è·å–å¯ç”¨çš„å…¬å…±åˆ—"""
        return getattr(self, 'available_common_columns', COMMON_COLUMNS)

    async def execute(
        self,
        csv_file_path: str,
        group_column: str = "param_value",
        control_value: str = "control",
        test_value: str = "test",
        degradation_threshold: float = 1.0,
        confidence_alpha: float = 1.0,
        output_dir = None,
        config: Optional[Dict] = None
    ) -> ToolResult:
        """
        æ‰§è¡Œå…³é”®æŒ‡æ ‡å±‚çº§åˆ†ææµç¨‹

        Args:
            csv_file_path: CSVæ•°æ®æ–‡ä»¶è·¯å¾„
            group_column: ç»„åˆ«æ ‡è¯†åˆ—å
            control_value: å¯¹ç…§ç»„å€¼
            test_value: æµ‹è¯•ç»„å€¼
            degradation_threshold: æ¶åŒ–é˜ˆå€¼
            confidence_alpha: ç½®ä¿¡åº¦å‚æ•°
            output_dir: è¾“å‡ºç›®å½•
            config: é…ç½®å‚æ•°å­—å…¸ï¼Œç”¨äºè¦†ç›–é»˜è®¤é…ç½®

        Returns:
            ToolResult: åˆ†æç»“æœ
        """
        try:
            # æ›´æ–°é…ç½®å‚æ•°
            if config:
                self.update_config(config)

            # ä½¿ç”¨é…ç½®å‚æ•°è¦†ç›–ä¼ å…¥çš„å‚æ•°
            degradation_threshold = self.config.degradation_threshold
            confidence_alpha = self.config.confidence_alpha

            # 1. æ•°æ®åŠ è½½å’ŒéªŒè¯
            df, available_common_columns, available_key_metrics = await self._load_and_validate_data(csv_file_path, group_column)

            # åŠ¨æ€è®¾ç½®å®ä¾‹å±æ€§ç”¨äºå…¶ä»–æ–¹æ³•è®¿é—®
            object.__setattr__(self, 'available_common_columns', available_common_columns)
            object.__setattr__(self, 'available_key_metrics', available_key_metrics)
            object.__setattr__(self, 'group_column', group_column)

            # 2. è®¾ç½®è¾“å‡ºç›®å½•
            if output_dir is None:
                csv_dir = os.path.dirname(csv_file_path)
                output_dir = os.path.join(csv_dir, "key_metric_analysis")
            os.makedirs(output_dir, exist_ok=True)

            # 3. æŒ‰realpicturequalityæ‹†åˆ†æ•°æ®
            quality_splits = await self._split_by_quality(df, group_column, control_value, test_value)

            # 4. å¯¹æ¯ä¸ªè´¨é‡ç­‰çº§è¿›è¡Œè®¾å¤‡çº§åˆ†æ
            all_results = {}

            for quality_level, quality_data in quality_splits.items():
                logger.info(f"å¼€å§‹åˆ†æè´¨é‡ç­‰çº§: {quality_level}")

                # æŒ‰devicemodelè¿›ä¸€æ­¥æ‹†åˆ†
                device_splits = await self._split_by_device_model(
                    quality_data, quality_level, group_column, control_value, test_value
                )

                # åˆ†ææ¯ä¸ªè®¾å¤‡å‹å·
                quality_results = {}
                for device_model, device_data in device_splits.items():
                    logger.info(f"åˆ†æè®¾å¤‡: {device_model} (è´¨é‡ç­‰çº§: {quality_level})")

                    device_results = await self._analyze_device_metrics(
                        device_data, device_model, quality_level,
                        degradation_threshold, confidence_alpha
                    )

                    quality_results[device_model] = device_results

                all_results[quality_level] = quality_results

            # 5. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
            comprehensive_analysis = await self._generate_comprehensive_analysis(all_results)

            # 6. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
            await self._create_comprehensive_visualizations(all_results, output_dir)

            # 7. ä¿å­˜åˆ†ææ•°æ®
            await self._save_analysis_results(all_results, comprehensive_analysis, output_dir)

            # 8. æŒ‰Unityç‰ˆæœ¬æ‹†åˆ†æ•°æ®
            # split_path_dir = os.path.join(os.path.dirname(output_dir), "unity_version_splits")
            await self._split_data_by_unity_version(all_results, output_dir)

            # 9. ç”Ÿæˆè®¾å¤‡æ€§èƒ½æŠ¥å‘Š
            device_performance_report = self._generate_device_performance_report(all_results, output_dir)

            # 10. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            summary = self._generate_final_summary(
                csv_file_path, all_results, comprehensive_analysis,
                quality_splits, output_dir, degradation_threshold, confidence_alpha
            )

            # å°†è®¾å¤‡æ€§èƒ½æŠ¥å‘Šæ·»åŠ åˆ°æ‘˜è¦ä¸­
            summary += "\n\n" + "="*60 + "\n"
            summary += "ğŸ“Š è¯¦ç»†è®¾å¤‡æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: device_performance_report.md\n"
            summary += "="*60 + "\n"

            return ToolResult(output=summary)

        except ToolError as e:
            return ToolResult(error=str(e))
        except Exception as e:
            error_msg = f"å…³é”®æŒ‡æ ‡åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(error_msg)
            return ToolResult(error=error_msg)

    async def _load_and_validate_data(self, csv_file_path: str, group_column: str) -> tuple[pd.DataFrame, list, list]:
        """åŠ è½½å’ŒéªŒè¯æ•°æ®ï¼Œè¿”å›æ•°æ®æ¡†å’Œå¯ç”¨çš„åˆ—ä¿¡æ¯"""

        if not os.path.exists(csv_file_path):
            raise ToolError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")

        try:
            df = pd.read_csv(csv_file_path)
            logger.info(f"æˆåŠŸåŠ è½½CSVæ–‡ä»¶ï¼Œæ•°æ®å½¢çŠ¶: {df.shape}")
        except Exception as e:
            raise ToolError(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")

        # åº”ç”¨SQLé€»è¾‘è¿›è¡Œæ•°æ®é¢„å¤„ç†
        df = await self._apply_sql_preprocessing(df)

        # éªŒè¯åŸºç¡€å¿…éœ€åˆ—
        base_required_columns = [group_column, "devicemodel", "realpicturequality"]
        missing_base_columns = [col for col in base_required_columns if col not in df.columns]

        if missing_base_columns:
            raise ToolError(f"ç¼ºå°‘åŸºç¡€å¿…éœ€åˆ—: {missing_base_columns}")

        # æ£€æŸ¥å…³é”®æŒ‡æ ‡åˆ—çš„å¯ç”¨æ€§
        available_key_metrics = [col for col in KEY_PERFORMANCE_METRICS if col in df.columns]
        missing_key_metrics = [col for col in KEY_PERFORMANCE_METRICS if col not in df.columns]

        if missing_key_metrics:
            logger.warning(f"ç¼ºå°‘éƒ¨åˆ†å…³é”®æŒ‡æ ‡åˆ—: {missing_key_metrics}")

        if not available_key_metrics:
            raise ToolError(f"æ‰€æœ‰å…³é”®æŒ‡æ ‡åˆ—éƒ½ç¼ºå¤±ï¼Œæ— æ³•è¿›è¡Œåˆ†æ: {KEY_PERFORMANCE_METRICS}")

        logger.info(f"å¯ç”¨å…³é”®æŒ‡æ ‡: {available_key_metrics} ({len(available_key_metrics)}/{len(KEY_PERFORMANCE_METRICS)})")

        # éªŒè¯å…¬å…±åˆ—
        available_common_columns = [col for col in COMMON_COLUMNS if col in df.columns]
        missing_common_columns = [col for col in COMMON_COLUMNS if col not in df.columns]

        if missing_common_columns:
            logger.warning(f"ç¼ºå°‘éƒ¨åˆ†å…¬å…±åˆ—: {missing_common_columns}")

        logger.info(f"å¯ç”¨å…¬å…±åˆ†æç»´åº¦: {available_common_columns}")

        # æ›´æ–°å¯ç”¨çš„åˆ—ï¼Œå¹¶ç¡®ä¿åŒ…å«group_columnç”¨äºåŒºåˆ†control/test
        available_common_columns = available_common_columns
        available_key_metrics = available_key_metrics  # ä¿å­˜å¯ç”¨çš„å…³é”®æŒ‡æ ‡
        group_column = group_column  # ä¿å­˜group_columnä»¥ä¾¿åç»­ä½¿ç”¨

        return df, available_common_columns, available_key_metrics

    async def _apply_sql_preprocessing(self, df: pd.DataFrame) -> pd.DataFrame:
        """åº”ç”¨SQLé€»è¾‘è¿›è¡Œæ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹"""

        logger.info("å¼€å§‹åº”ç”¨SQLé¢„å¤„ç†é€»è¾‘...")

        # 1. æ—¶é—´ç»´åº¦ç‰¹å¾ï¼šåŸºäºlogymdåˆ›å»ºæ—¶é—´æ ‡è¯†
        if 'logymd' in df.columns:
            df['logymd'] = pd.to_datetime(df['logymd'], format='%Y-%m-%d', errors='coerce')
            # åˆ›å»ºæ—¶é—´æ ‡è¯†ï¼šä½¿ç”¨é…ç½®çš„æ—¶é—´åˆ†å‰²æ—¥æœŸ
            split_date = pd.to_datetime(self.config.time_split_date)
            df['time_flag'] = (df['logymd'] >= split_date).astype(int)
            logger.info(f"åˆ›å»ºæ—¶é—´æ ‡è¯†ç‰¹å¾ time_flag (åˆ†å‰²æ—¥æœŸ: {self.config.time_split_date})")

        # 2. å†…å­˜åˆ†çº§ç‰¹å¾ï¼šåŸºäºsystemmemorysize
        if 'systemmemorysize' in df.columns:
            def categorize_memory(memory_size):
                if pd.isna(memory_size):
                    return 'unknown'
                memory_gb = memory_size / 1024.0
                if memory_gb <= 2:
                    return '0G-2G'
                elif memory_gb < 6:
                    return '2G-6G'
                elif memory_gb >= 6:
                    return '6G'
                else:
                    return 'else'

            df['memory_category'] = df['systemmemorysize'].apply(categorize_memory)
            logger.info("åˆ›å»ºå†…å­˜åˆ†çº§ç‰¹å¾ memory_category")

        # 3. Unityç‰ˆæœ¬ä½æ•°ç‰¹å¾ï¼šåŸºäºunityversion
        if 'unityversion' in df.columns:
            def extract_digital_version(unity_version):
                if pd.isna(unity_version):
                    return 'unknown'
                if '64' in str(unity_version):
                    return '64bit'
                else:
                    return '32bit'

            df['unity_digital'] = df['unityversion'].apply(extract_digital_version)
            logger.info("åˆ›å»ºUnityç‰ˆæœ¬ä½æ•°ç‰¹å¾ unity_digital")

        # 4. åŒºåŸŸç‰¹å¾ï¼šåŸºäºzoneid
        if 'zoneid' in df.columns:
            df['zone_group'] = df['zoneid'] % 2
            logger.info("åˆ›å»ºåŒºåŸŸåˆ†ç»„ç‰¹å¾ zone_group")

        # 5. æ•°æ®è´¨é‡è¿‡æ»¤ï¼ˆåŸºäºSQLçš„WHEREæ¡ä»¶ï¼‰
        initial_count = len(df)

        # è¿‡æ»¤å¼‚å¸¸FPSæ•°æ®
        if 'avgfps_unity' in df.columns:
            df = df[(df['avgfps_unity'] >= self.config.fps_min_threshold) &
                   (df['avgfps_unity'] <= self.config.fps_max_threshold)]

        # è¿‡æ»¤å¼‚å¸¸çš„æ€§èƒ½æŒ‡æ ‡
        performance_filters = {
            'avgfps': lambda x: x > 0,
            'jankper10min': lambda x: x >= 0,
            'bigjankper10min': lambda x: x >= 0,
            'current_avg': lambda x: (x >= self.config.current_avg_min) & (x <= self.config.current_avg_max) if 'current_avg' in df.columns else True,
        }

        for column, filter_func in performance_filters.items():
            if column in df.columns:
                before_count = len(df)
                df = df[filter_func(df[column])]
                after_count = len(df)
                if before_count != after_count:
                    logger.info(f"è¿‡æ»¤ {column} å¼‚å¸¸å€¼ï¼š{before_count} -> {after_count}")

        # 6. è®¾å¤‡è´¨é‡ç­‰çº§æ˜ å°„ï¼ˆåŸºäºfixed_scoreï¼‰
        if 'fixed_score' in df.columns and 'nowpicturequality' in df.columns:
            # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            df['fixed_score'] = pd.to_numeric(df['fixed_score'], errors='coerce')
            df['nowpicturequality'] = pd.to_numeric(df['nowpicturequality'], errors='coerce')

            # åŸºäºé…ç½®çš„é˜ˆå€¼ç¡®å®šæœŸæœ›çš„ç”»è´¨ç­‰çº§
            thresholds = self.config.quality_level_thresholds or [5.7, 7.4, 9.4]
            conditions = [
                (df['fixed_score'] > 0) & (df['fixed_score'] < thresholds[0]),
                (df['fixed_score'] >= thresholds[0]) & (df['fixed_score'] < thresholds[1]),
                (df['fixed_score'] >= thresholds[1]) & (df['fixed_score'] < thresholds[2]),
                (df['fixed_score'] >= thresholds[2])
            ]
            choices = [1, 2, 3, 4]

            df['expected_quality'] = np.select(conditions, choices, default=0)

            # åªä¿ç•™ç”»è´¨ç­‰çº§ä¸è®¾å¤‡èƒ½åŠ›åŒ¹é…çš„æ•°æ®
            df = df[df['nowpicturequality'] == df['expected_quality']]

            # ä½¿ç”¨æœŸæœ›çš„è´¨é‡ç­‰çº§ä½œä¸ºrealpicturequality
            df['realpicturequality'] = df['expected_quality']
            logger.info(f"åº”ç”¨è®¾å¤‡è´¨é‡ç­‰çº§æ˜ å°„ï¼Œä¿ç•™ {len(df)} æ¡åŒ¹é…è®°å½•")

        # 7. åˆ›å»ºè®¾å¤‡æ¨¡å‹æ ‡è¯†ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if 'devicemodel' not in df.columns:
            # åŸºäºå†…å­˜å’ŒUnityç‰ˆæœ¬åˆ›å»ºå¤åˆè®¾å¤‡æ¨¡å‹
            device_features = []
            if 'memory_category' in df.columns:
                device_features.append('memory_category')
            if 'unity_digital' in df.columns:
                device_features.append('unity_digital')
            if 'cpumodel' in df.columns:
                device_features.append('cpumodel')

            if device_features:
                df['devicemodel'] = df[device_features].apply(
                    lambda x: '_'.join([str(v) for v in x if pd.notna(v)]), axis=1
                )
                logger.info(f"åŸºäº {device_features} åˆ›å»ºå¤åˆè®¾å¤‡æ¨¡å‹æ ‡è¯†")
            else:
                df['devicemodel'] = 'unknown_device'
                logger.warning("æ— æ³•åˆ›å»ºè®¾å¤‡æ¨¡å‹æ ‡è¯†ï¼Œä½¿ç”¨é»˜è®¤å€¼")

        # 8. è§„èŒƒåŒ–æŒ‡æ ‡åç§°ï¼ˆç¡®ä¿ä¸KEY_PERFORMANCE_METRICSä¸€è‡´ï¼‰
        metric_mapping = {
            'avgfps_unity': 'avgfps_unity',
            'battleendtotalpss': 'battleendtotalpss',
            'current_avg': 'current_avg',
            'gt_50': 'gt_50',  # GCæ¬¡æ•°
            'totalgcallocsize': 'totalgcallocsize',  # GCåˆ†é…
            'bigjankper10min': 'bigjankper10min'
        }

        # é‡å‘½ååˆ—ä»¥åŒ¹é…é¢„æœŸçš„æŒ‡æ ‡åç§°
        for old_name, new_name in metric_mapping.items():
            if old_name in df.columns and old_name != new_name:
                df = df.rename(columns={old_name: new_name})
                logger.info(f"é‡å‘½åæŒ‡æ ‡åˆ—ï¼š{old_name} -> {new_name}")

        final_count = len(df)
        logger.info(f"SQLé¢„å¤„ç†å®Œæˆï¼š{initial_count} -> {final_count} æ¡è®°å½•")

        # 9. æ·»åŠ é¢„å¤„ç†åçš„ç»Ÿè®¡ä¿¡æ¯
        if final_count > 0:
            logger.info("é¢„å¤„ç†åçš„æ•°æ®ç»Ÿè®¡ï¼š")
            if 'memory_category' in df.columns:
                logger.info(f"å†…å­˜åˆ†å¸ƒï¼š\n{df['memory_category'].value_counts()}")
            if 'unity_digital' in df.columns:
                logger.info(f"Unityç‰ˆæœ¬åˆ†å¸ƒï¼š\n{df['unity_digital'].value_counts()}")
            if 'realpicturequality' in df.columns:
                logger.info(f"è´¨é‡ç­‰çº§åˆ†å¸ƒï¼š\n{df['realpicturequality'].value_counts()}")

        return df

    async def _split_by_quality(
        self,
        df: pd.DataFrame,
        group_column: str,
        control_value: str,
        test_value: str
    ) -> Dict[str, Dict]:
        """æŒ‰realpicturequalityæ‹†åˆ†æ•°æ®"""

        quality_levels = sorted(df['realpicturequality'].unique())
        logger.info(f"å‘ç° {len(quality_levels)} ä¸ªè´¨é‡ç­‰çº§: {quality_levels}")

        quality_splits = {}

        for quality_level in quality_levels:
            quality_df = df[df['realpicturequality'] == quality_level].copy()

            control_data = quality_df[quality_df[group_column] == control_value].copy()
            test_data = quality_df[quality_df[group_column] == test_value].copy()

            if control_data.empty or test_data.empty:
                logger.warning(f"è´¨é‡ç­‰çº§ {quality_level} ç¼ºå°‘controlæˆ–testæ•°æ®ï¼Œè·³è¿‡")
                continue

            quality_splits[quality_level] = {
                'control': control_data,
                'test': test_data,
                'total_records': len(quality_df),
                'control_records': len(control_data),
                'test_records': len(test_data)
            }

            logger.info(f"è´¨é‡ç­‰çº§ {quality_level}: Control={len(control_data)}, Test={len(test_data)}")

        return quality_splits

    async def _split_by_device_model(
        self,
        quality_data: Dict,
        quality_level: str,
        group_column: str,
        control_value: str,
        test_value: str
    ) -> Dict[str, Dict]:
        """åœ¨è´¨é‡ç­‰çº§å†…æŒ‰devicemodelæ‹†åˆ†"""

        control_data = quality_data['control']
        test_data = quality_data['test']

        # è·å–æ‰€æœ‰è®¾å¤‡å‹å·
        control_devices = set(control_data['devicemodel'].unique())
        test_devices = set(test_data['devicemodel'].unique())
        all_devices = control_devices.union(test_devices)

        logger.info(f"è´¨é‡ç­‰çº§ {quality_level} åŒ…å« {len(all_devices)} ä¸ªè®¾å¤‡å‹å·")

        device_splits = {}

        for device_model in all_devices:
            device_control = control_data[control_data['devicemodel'] == device_model].copy()
            device_test = test_data[test_data['devicemodel'] == device_model].copy()

            if device_control.empty or device_test.empty:
                logger.warning(f"è®¾å¤‡ {device_model} (è´¨é‡{quality_level}) ç¼ºå°‘controlæˆ–testæ•°æ®ï¼Œè·³è¿‡")
                continue

            # ä¿ç•™å¯ç”¨çš„å…³é”®æŒ‡æ ‡ã€å…¬å…±åˆ—å’Œgroup_column
            available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)
            available_common_columns = getattr(self, 'available_common_columns', COMMON_COLUMNS)
            group_column = getattr(self, 'group_column', group_column)

            final_columns = available_key_metrics + available_common_columns + [group_column]
            # å»é‡å¹¶ç¡®ä¿åˆ—å­˜åœ¨
            final_columns = list(set(final_columns))
            final_columns = [col for col in final_columns if col in device_control.columns]

            device_splits[device_model] = {
                'control': device_control[final_columns].copy(),
                'test': device_test[final_columns].copy(),
                'control_count': len(device_control),
                'test_count': len(device_test),
                'quality_level': quality_level
            }

            logger.info(f"è®¾å¤‡ {device_model} (è´¨é‡{quality_level}): Control={len(device_control)}, Test={len(device_test)}")

        return device_splits

    async def _analyze_device_metrics(
        self,
        device_data: Dict,
        device_model: str,
        quality_level: str,
        degradation_threshold: float,
        confidence_alpha: float
    ) -> Dict:
        """åˆ†æå•ä¸ªè®¾å¤‡çš„å…³é”®æŒ‡æ ‡"""

        control_data = device_data['control']
        test_data = device_data['test']

        analysis_result = {
            'device_model': device_model,
            'quality_level': quality_level,
            'control_count': device_data['control_count'],
            'test_count': device_data['test_count'],
            'confidence_score': 0.0,
            'key_metrics_analysis': {},
            'overall_status': 'stable',
            'degradation_score': 0.0,
            'performance_change_explanations': {},  # æ”¹ä¸ºæ€§èƒ½å˜åŒ–è§£é‡Š
            'insights': [],
            # ä¿å­˜åŸå§‹æ•°æ®ä¾›åç»­å›¾è¡¨åˆ†æä½¿ç”¨
            'control_data': control_data.copy(),
            'test_data': test_data.copy()
        }

        # è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        nc, nt = device_data['control_count'], device_data['test_count']
        if nc > 0 and nt > 0:
            balance_ratio = min(nc, nt) / max(nc, nt)
            sample_size_factor = min(1.0, np.log10((nc + nt) * 10) / 2.0)
            analysis_result['confidence_score'] = self.config.confidence_max_score * (balance_ratio ** confidence_alpha) * sample_size_factor

        # åˆ†æå¯ç”¨çš„å…³é”®æŒ‡æ ‡
        degradation_count = 0
        improvement_count = 0
        total_degradation = 0.0
        significant_changed_metrics = []  # è®°å½•æœ‰æ˜¾è‘—å˜åŒ–çš„æŒ‡æ ‡

        available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)

        for metric in available_key_metrics:
            if metric in control_data.columns and metric in test_data.columns:
                metric_analysis = await self._analyze_single_metric(
                    control_data, test_data, metric, degradation_threshold
                )
                analysis_result['key_metrics_analysis'][metric] = metric_analysis

                if metric_analysis['status'] == 'degraded':
                    degradation_count += 1
                    total_degradation += abs(metric_analysis['change_percent'])
                    significant_changed_metrics.append(metric)
                elif metric_analysis['status'] == 'improved':
                    improvement_count += 1
                    significant_changed_metrics.append(metric)

        # è®¡ç®—æ•´ä½“çŠ¶æ€
        if degradation_count > improvement_count:
            analysis_result['overall_status'] = 'degraded'
        elif improvement_count > degradation_count:
            analysis_result['overall_status'] = 'improved'

        analysis_result['degradation_score'] = total_degradation / max(len(available_key_metrics), 1)

        # æ³¨é‡Šæ‰åŸå› åˆ†æ - åªæœ‰å½“å…³é”®æŒ‡æ ‡æœ‰æ˜¾è‘—å˜åŒ–æ—¶ï¼Œæ‰åˆ†æcommon_columnsæ¥å¯»æ‰¾åŸå› 
        # if significant_changed_metrics:
        #     logger.info(f"è®¾å¤‡ {device_model} æœ‰æ˜¾è‘—æ€§èƒ½å˜åŒ–ï¼Œåˆ†æå¯èƒ½åŸå› ...")
        #     analysis_result['performance_change_explanations'] = await self._explain_performance_changes(
        #         control_data, test_data, significant_changed_metrics
        #     )

        # ç”Ÿæˆæ´å¯Ÿ
        analysis_result['insights'] = await self._generate_device_insights(analysis_result)

        return analysis_result

    async def _analyze_single_metric(
        self,
        control_data: pd.DataFrame,
        test_data: pd.DataFrame,
        metric: str,
        threshold: float
    ) -> Dict:
        """åˆ†æå•ä¸ªå…³é”®æŒ‡æ ‡"""

        # æå–æ•°å€¼æ•°æ®
        control_values = pd.to_numeric(control_data[metric], errors='coerce').dropna()
        test_values = pd.to_numeric(test_data[metric], errors='coerce').dropna()

        analysis = {
            'metric': metric,
            'control_count': len(control_values),
            'test_count': len(test_values),
            'control_mean': np.nan,
            'test_mean': np.nan,
            'control_std': np.nan,
            'test_std': np.nan,
            'control_median': np.nan,
            'test_median': np.nan,
            'change_percent': 0.0,
            'absolute_change': 0.0,
            'status': 'no_data',
            'statistical_significance': False,
            'effect_size': 0.0
        }

        if len(control_values) > 0:
            analysis['control_mean'] = control_values.mean()
            analysis['control_std'] = control_values.std()
            analysis['control_median'] = control_values.median()

        if len(test_values) > 0:
            analysis['test_mean'] = test_values.mean()
            analysis['test_std'] = test_values.std()
            analysis['test_median'] = test_values.median()

        # è®¡ç®—å˜åŒ–
        if not np.isnan(analysis['control_mean']) and not np.isnan(analysis['test_mean']):
            if analysis['control_mean'] != 0:
                change_percent = ((analysis['test_mean'] - analysis['control_mean']) /
                                analysis['control_mean']) * 100
                analysis['change_percent'] = change_percent
                analysis['absolute_change'] = analysis['test_mean'] - analysis['control_mean']

                # åˆ¤æ–­æ˜¯å¦ä¸ºæ”¹å–„ (é’ˆå¯¹ä¸åŒæŒ‡æ ‡)
                is_improvement = self._is_metric_improvement(metric, change_percent)

                if abs(change_percent) >= threshold:
                    analysis['status'] = 'improved' if is_improvement else 'degraded'
                else:
                    analysis['status'] = 'stable'

                # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ (æš‚æ—¶ç®€åŒ–)
                analysis['statistical_significance'] = False
                analysis['effect_size'] = 0.0
            else:
                analysis['status'] = 'stable'

        return analysis

    def _is_metric_improvement(self, metric: str, change_percent: float) -> bool:
        """åˆ¤æ–­æŒ‡æ ‡å˜åŒ–æ˜¯å¦ä¸ºæ”¹å–„"""

        # å¯¹äºä¸åŒæŒ‡æ ‡å®šä¹‰æ”¹å–„æ–¹å‘
        higher_better_metrics = ["avgfps_unity", "current_avg"]  # è¶Šé«˜è¶Šå¥½
        lower_better_metrics = ["totalgcallocsize", "gt_50", "battleendtotalpss", "bigjankper10min"]  # è¶Šä½è¶Šå¥½

        if metric in higher_better_metrics:
            return change_percent > 0  # å¢åŠ æ˜¯æ”¹å–„
        elif metric in lower_better_metrics:
            return change_percent < 0  # å‡å°‘æ˜¯æ”¹å–„
        else:
            return False  # æœªçŸ¥æŒ‡æ ‡ï¼Œä¿å®ˆå¤„ç†

    async def _explain_performance_changes(
        self,
        control_data: pd.DataFrame,
        test_data: pd.DataFrame,
        changed_metrics: List[str]
    ) -> Dict:
        """åˆ†ææ€§èƒ½å˜åŒ–çš„å¯èƒ½åŸå› """

        explanations = {
            'potential_causes': [],
            'dimension_differences': {},
            'key_findings': []
        }

        logger.info(f"åˆ†æç»´åº¦å·®å¼‚æ¥è§£é‡ŠæŒ‡æ ‡å˜åŒ–: {changed_metrics}")

        available_common_columns = getattr(self, 'available_common_columns', COMMON_COLUMNS)

        # åˆ†ææ¯ä¸ªcommon_columnï¼Œå¯»æ‰¾å¯èƒ½å¯¼è‡´æ€§èƒ½å˜åŒ–çš„å› ç´ 
        for column in available_common_columns:
            if column in control_data.columns and column in test_data.columns:
                dimension_analysis = await self._analyze_dimension_for_explanation(
                    control_data, test_data, column
                )

                if dimension_analysis['has_significant_difference']:
                    explanations['dimension_differences'][column] = dimension_analysis
                    explanations['potential_causes'].extend(dimension_analysis['explanations'])

        # ç”Ÿæˆå…³é”®å‘ç°
        explanations['key_findings'] = await self._generate_causal_insights(
            explanations['dimension_differences'], changed_metrics
        )

        return explanations

    async def _analyze_dimension_for_explanation(
        self,
        control_data: pd.DataFrame,
        test_data: pd.DataFrame,
        column: str
    ) -> Dict:
        """åˆ†æå•ä¸ªç»´åº¦æ˜¯å¦èƒ½è§£é‡Šæ€§èƒ½å˜åŒ–"""

        analysis = {
            'dimension': column,
            'has_significant_difference': False,
            'explanations': [],
            'difference_details': {}
        }

        # è·å–éç©ºå€¼
        control_values = control_data[column].dropna()
        test_values = test_data[column].dropna()

        if len(control_values) == 0 or len(test_values) == 0:
            return analysis

        # æ˜ç¡®å®šä¹‰å“ªäº›åˆ—åº”è¯¥æŒ‰ç±»åˆ«å‹å¤„ç†
        categorical_columns = [
            'unityversion_inner',  # ç‰ˆæœ¬å·å­—ç¬¦ä¸² (å¦‚: 2019-2.01.0050.10-6dca93b4...)
            'unityversion',        # ç‰ˆæœ¬åç§° (å¦‚: UI20_64_2019)
            'cpumodel',           # CPUå‹å·
            'ischarged',          # å……ç”µçŠ¶æ€ï¼ˆ0/1ï¼‰
            # æ–°å¢é¢„å¤„ç†ç‰¹å¾
            'time_flag',          # æ—¶é—´æ ‡è¯† (0/1)
            'memory_category',    # å†…å­˜åˆ†çº§ (0G-2G, 2G-6G, 6G)
            'unity_digital',      # Unityç‰ˆæœ¬ä½æ•° (32bit, 64bit)
            'zone_group',         # åŒºåŸŸåˆ†ç»„ (0/1)
            'logymd',            # æ—¥æœŸ
            'zoneid'             # åŒºåŸŸID
        ]

        # æ˜ç¡®å®šä¹‰å“ªäº›åˆ—åº”è¯¥æŒ‰æ•°å€¼å‹å¤„ç†
        numeric_columns = [
            'devicetotalmemory',  # è®¾å¤‡å†…å­˜
            'refreshrate',        # åˆ·æ–°ç‡
            'systemmemorysize'    # ç³»ç»Ÿå†…å­˜å¤§å°
        ]

        if column in categorical_columns:
            # ç±»åˆ«å‹æ•°æ®å¤„ç†
            logger.debug(f"å°† {column} æŒ‰ç±»åˆ«å‹æ•°æ®å¤„ç†")
            analysis = await self._analyze_categorical_explanation(
                control_values, test_values, analysis
            )
        elif column in numeric_columns:
            # æ•°å€¼å‹æ•°æ®å¤„ç†
            logger.debug(f"å°† {column} æŒ‰æ•°å€¼å‹æ•°æ®å¤„ç†")
            analysis = await self._analyze_numeric_explanation(
                control_values, test_values, analysis
            )
        else:
            # è‡ªåŠ¨åˆ¤æ–­ï¼šå°è¯•è½¬æ¢ä¸ºæ•°å€¼ï¼Œå¦‚æœå¤±è´¥åˆ™æŒ‰ç±»åˆ«å¤„ç†
            try:
                pd.to_numeric(control_values.iloc[:5])  # æµ‹è¯•å‰5ä¸ªå€¼
                logger.debug(f"è‡ªåŠ¨æ£€æµ‹ {column} ä¸ºæ•°å€¼å‹æ•°æ®")
                analysis = await self._analyze_numeric_explanation(
                    control_values, test_values, analysis
                )
            except (ValueError, TypeError):
                logger.debug(f"è‡ªåŠ¨æ£€æµ‹ {column} ä¸ºç±»åˆ«å‹æ•°æ®")
                analysis = await self._analyze_categorical_explanation(
                    control_values, test_values, analysis
                )

        return analysis

    async def _analyze_numeric_explanation(
        self,
        control_values: pd.Series,
        test_values: pd.Series,
        analysis: Dict
    ) -> Dict:
        """åˆ†ææ•°å€¼å‹ç»´åº¦çš„è§£é‡Šæ€§å·®å¼‚"""

        control_mean = control_values.mean()
        test_mean = test_values.mean()

        if control_mean != 0:
            diff_percent = ((test_mean - control_mean) / control_mean) * 100
        else:
            diff_percent = 0

        analysis['difference_details'] = {
            'control_mean': control_mean,
            'test_mean': test_mean,
            'difference_percent': diff_percent,
            'absolute_difference': test_mean - control_mean
        }

        # è®¾å®šæ˜¾è‘—å·®å¼‚é˜ˆå€¼ï¼šæ•°å€¼å·®å¼‚>5%ï¼ˆé™ä½é˜ˆå€¼ä»¥æ£€æµ‹æ›´å¤šå·®å¼‚ï¼‰
        if abs(diff_percent) > 5:
            analysis['has_significant_difference'] = True

            direction = "æ›´é«˜" if diff_percent > 0 else "æ›´ä½"
            explanation = f"Testç»„åœ¨{analysis['dimension']}ç»´åº¦çš„å¹³å‡å€¼{direction} {abs(diff_percent):.1f}%"

            # ç‰¹å®šç»´åº¦çš„æ€§èƒ½å½±å“åˆ†æ
            if analysis['dimension'] == 'refreshrate':
                if diff_percent > 0:
                    explanation += "ï¼Œæ›´é«˜çš„åˆ·æ–°ç‡å¯èƒ½å¯¼è‡´æ›´å¤§çš„æ€§èƒ½è´Ÿè½½"
                else:
                    explanation += "ï¼Œæ›´ä½çš„åˆ·æ–°ç‡å¯èƒ½å‡å°‘äº†æ€§èƒ½å‹åŠ›"
            elif analysis['dimension'] == 'devicetotalmemory':
                if diff_percent > 0:
                    explanation += "ï¼Œæ›´å¤§çš„å†…å­˜å¯èƒ½å½±å“å†…å­˜ç®¡ç†ç­–ç•¥"
                else:
                    explanation += "ï¼Œæ›´å°çš„å†…å­˜å¯èƒ½é€ æˆå†…å­˜å‹åŠ›"

            analysis['explanations'].append(explanation)

        return analysis

    async def _analyze_categorical_explanation(
        self,
        control_values: pd.Series,
        test_values: pd.Series,
        analysis: Dict
    ) -> Dict:
        """åˆ†æç±»åˆ«å‹ç»´åº¦çš„è§£é‡Šæ€§å·®å¼‚"""

        # è®¡ç®—åˆ†å¸ƒ
        control_dist = control_values.value_counts(normalize=True).to_dict()
        test_dist = test_values.value_counts(normalize=True).to_dict()

        analysis['difference_details'] = {
            'control_distribution': control_dist,
            'test_distribution': test_dist
        }

        # æ‰¾å‡ºæ˜¾è‘—å·®å¼‚çš„ç±»åˆ«
        all_categories = set(control_dist.keys()).union(set(test_dist.keys()))
        significant_differences = []

        for category in all_categories:
            control_pct = control_dist.get(category, 0) * 100
            test_pct = test_dist.get(category, 0) * 100
            diff = test_pct - control_pct

            # è®¾å®šæ˜¾è‘—å·®å¼‚é˜ˆå€¼ï¼šåˆ†å¸ƒå·®å¼‚>10ä¸ªç™¾åˆ†ç‚¹ï¼ˆé™ä½é˜ˆå€¼ä»¥æ£€æµ‹æ›´å¤šå·®å¼‚ï¼‰
            if abs(diff) > 10:
                significant_differences.append({
                    'category': category,
                    'control_percent': control_pct,
                    'test_percent': test_pct,
                    'difference': diff
                })

        if significant_differences:
            analysis['has_significant_difference'] = True

            for diff in significant_differences:
                explanation = f"Testç»„ä¸­{analysis['dimension']}='{diff['category']}'çš„æ¯”ä¾‹" + \
                             f"{'æ›´é«˜' if diff['difference'] > 0 else 'æ›´ä½'}" + \
                             f" {abs(diff['difference']):.1f}ä¸ªç™¾åˆ†ç‚¹"

                # ç‰¹å®šç»´åº¦çš„æ€§èƒ½å½±å“åˆ†æ
                if analysis['dimension'] == 'ischarged':
                    if diff['category'] == 1 and diff['difference'] > 0:
                        explanation += " (å……ç”µçŠ¶æ€å¯èƒ½å½±å“CPUé¢‘ç‡å’Œå‘çƒ­ï¼Œè¿›è€Œå½±å“æ€§èƒ½)"
                    elif diff['category'] == 0 and diff['difference'] > 0:
                        explanation += " (éå……ç”µçŠ¶æ€å¯èƒ½æœ‰æ›´ç¨³å®šçš„æ€§èƒ½è¡¨ç°)"
                elif analysis['dimension'] == 'cpumodel':
                    explanation += f" (ä¸åŒCPUå‹å·çš„æ€§èƒ½ç‰¹å¾å·®å¼‚å¯èƒ½å½±å“æµ‹è¯•ç»“æœ)"
                elif analysis['dimension'] == 'lowpower':
                    if diff['category'] == 1 and diff['difference'] > 0:
                        explanation += " (æ›´å¤šè®¾å¤‡å¤„äºä½åŠŸè€—æ¨¡å¼ï¼Œå¯èƒ½é™åˆ¶äº†æ€§èƒ½)"
                # æ–°å¢é¢„å¤„ç†ç‰¹å¾çš„è§£é‡Š
                elif analysis['dimension'] == 'time_flag':
                    if diff['category'] == 1 and diff['difference'] > 0:
                        explanation += " (7æœˆ15æ—¥åçš„æ•°æ®æ›´å¤šï¼Œå¯èƒ½åæ˜ äº†ç‰ˆæœ¬æ›´æ–°æˆ–ç¯å¢ƒå˜åŒ–)"
                    elif diff['category'] == 0 and diff['difference'] > 0:
                        explanation += " (7æœˆ15æ—¥å‰çš„æ•°æ®æ›´å¤šï¼Œå¯èƒ½æ˜¯åŸºçº¿æ€§èƒ½è¡¨ç°)"
                elif analysis['dimension'] == 'memory_category':
                    if 'G' in str(diff['category']):
                        explanation += f" (ä¸åŒå†…å­˜è§„æ ¼çš„è®¾å¤‡æ€§èƒ½ç‰¹å¾ä¸åŒ)"
                elif analysis['dimension'] == 'unity_digital':
                    if diff['category'] == '64bit' and diff['difference'] > 0:
                        explanation += " (64ä½Unityå¼•æ“ç‰ˆæœ¬æ›´å¤šï¼Œå¯èƒ½æœ‰ä¸åŒçš„æ€§èƒ½ç‰¹å¾)"
                    elif diff['category'] == '32bit' and diff['difference'] > 0:
                        explanation += " (32ä½Unityå¼•æ“ç‰ˆæœ¬æ›´å¤šï¼Œå¯èƒ½å½±å“å†…å­˜å’Œæ€§èƒ½è¡¨ç°)"
                elif analysis['dimension'] == 'zone_group':
                    explanation += f" (ä¸åŒæœåŠ¡å™¨åŒºåŸŸçš„ç½‘ç»œå’Œè´Ÿè½½æ¡ä»¶å¯èƒ½ä¸åŒ)"

                analysis['explanations'].append(explanation)

        return analysis

    async def _generate_causal_insights(
        self,
        dimension_differences: Dict,
        changed_metrics: List[str]
    ) -> List[str]:
        """ç”Ÿæˆå› æœå…³ç³»æ´å¯Ÿ"""

        insights = []

        # ä¼˜å…ˆçº§ç»´åº¦ï¼šå¯¹æ€§èƒ½å½±å“æœ€å¤§çš„ç»´åº¦
        high_impact_dimensions = [
            'ischarged', 'cpumodel', 'refreshrate', 'lowpower',
            # æ–°å¢å…³é”®é¢„å¤„ç†ç‰¹å¾
            'time_flag', 'memory_category', 'unity_digital', 'unityversion_inner'
        ]

        # æŒ‰ä¼˜å…ˆçº§ç”Ÿæˆæ´å¯Ÿ
        for dimension in high_impact_dimensions:
            if dimension in dimension_differences:
                diff_analysis = dimension_differences[dimension]

                # ç”Ÿæˆé’ˆå¯¹æ€§çš„æ´å¯Ÿ
                if dimension == 'ischarged' and diff_analysis['has_significant_difference']:
                    insights.append(f"ğŸ”‹ å……ç”µçŠ¶æ€å·®å¼‚å¯èƒ½æ˜¯å½±å“ {', '.join(changed_metrics)} æ€§èƒ½çš„å…³é”®å› ç´ ")

                elif dimension == 'cpumodel' and diff_analysis['has_significant_difference']:
                    insights.append(f"ğŸ’» CPUå‹å·åˆ†å¸ƒå·®å¼‚å¯èƒ½å¯¼è‡´äº† {', '.join(changed_metrics)} çš„æ€§èƒ½å˜åŒ–")

                elif dimension == 'refreshrate' and diff_analysis['has_significant_difference']:
                    insights.append(f"ğŸ“± åˆ·æ–°ç‡å·®å¼‚å¯èƒ½å½±å“äº†å¸§ç‡å’Œå¡é¡¿ç›¸å…³æŒ‡æ ‡çš„è¡¨ç°")

                elif dimension == 'lowpower' and diff_analysis['has_significant_difference']:
                    insights.append(f"âš¡ åŠŸè€—æ¨¡å¼å·®å¼‚å¯èƒ½æ˜¯æ€§èƒ½å˜åŒ–çš„é‡è¦åŸå› ")

                # æ–°å¢é¢„å¤„ç†ç‰¹å¾çš„æ´å¯Ÿ
                elif dimension == 'time_flag' and diff_analysis['has_significant_difference']:
                    insights.append(f"ğŸ“… æµ‹è¯•æ—¶é—´åˆ†å¸ƒå·®å¼‚(7æœˆ15æ—¥å‰å)å¯èƒ½å½±å“äº† {', '.join(changed_metrics)} çš„è¡¨ç°")

                elif dimension == 'memory_category' and diff_analysis['has_significant_difference']:
                    insights.append(f"ğŸ’¾ è®¾å¤‡å†…å­˜è§„æ ¼åˆ†å¸ƒå·®å¼‚å¯èƒ½æ˜¯ {', '.join(changed_metrics)} å˜åŒ–çš„åŸå› ")

                elif dimension == 'unity_digital' and diff_analysis['has_significant_difference']:
                    insights.append(f"ğŸ® Unityå¼•æ“ä½æ•°(32/64ä½)åˆ†å¸ƒå·®å¼‚å¯èƒ½å½±å“äº†æ€§èƒ½è¡¨ç°")

                elif dimension == 'unityversion_inner' and diff_analysis['has_significant_difference']:
                    insights.append(f"ğŸ”§ Unityè¯¦ç»†ç‰ˆæœ¬åˆ†å¸ƒå·®å¼‚å¯èƒ½æ˜¯æ€§èƒ½å˜åŒ–çš„å…³é”®å› ç´ ")

        # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„é«˜ä¼˜å…ˆçº§åŸå› ï¼Œæ£€æŸ¥å…¶ä»–ç»´åº¦
        if not insights:
            for dimension, diff_analysis in dimension_differences.items():
                if diff_analysis['has_significant_difference']:
                    insights.append(f"ğŸ“Š {dimension}ç»´åº¦çš„å·®å¼‚å¯èƒ½ä¸æ€§èƒ½å˜åŒ–ç›¸å…³")

        # å¦‚æœä»ç„¶æ²¡æœ‰æ˜æ˜¾åŸå› 
        if not insights:
            insights.append("ğŸ“ æœªå‘ç°æ˜æ˜¾çš„ç¯å¢ƒå› ç´ å·®å¼‚ï¼Œæ€§èƒ½å˜åŒ–å¯èƒ½ç”±ä»£ç é€»è¾‘æˆ–å…¶ä»–æœªç›‘æ§å› ç´ å¼•èµ·")

        return insights

    async def _generate_device_insights(self, analysis_result: Dict) -> List[str]:
        """ä¸ºè®¾å¤‡åˆ†æç»“æœç”Ÿæˆæ´å¯Ÿ"""

        insights = []
        device_model = analysis_result['device_model']
        quality_level = analysis_result['quality_level']

        # æ•´ä½“çŠ¶æ€æ´å¯Ÿ
        if analysis_result['overall_status'] == 'degraded':
            insights.append(f"ğŸ”´ è®¾å¤‡ {device_model} (è´¨é‡{quality_level}) æ•´ä½“æ€§èƒ½æ¶åŒ–")
        elif analysis_result['overall_status'] == 'improved':
            insights.append(f"ğŸŸ¢ è®¾å¤‡ {device_model} (è´¨é‡{quality_level}) æ•´ä½“æ€§èƒ½æ”¹å–„")

        # å…³é”®æŒ‡æ ‡æ´å¯Ÿ
        degraded_metrics = []
        improved_metrics = []

        for metric, metric_analysis in analysis_result['key_metrics_analysis'].items():
            if metric_analysis['status'] == 'degraded':
                degraded_metrics.append(f"{metric} ({metric_analysis['change_percent']:.1f}%)")
            elif metric_analysis['status'] == 'improved':
                improved_metrics.append(f"{metric} ({metric_analysis['change_percent']:.1f}%)")

        if degraded_metrics:
            insights.append(f"ğŸ“‰ æ¶åŒ–æŒ‡æ ‡: {', '.join(degraded_metrics)}")

        if improved_metrics:
            insights.append(f"ğŸ“ˆ æ”¹å–„æŒ‡æ ‡: {', '.join(improved_metrics)}")

        # æ³¨é‡Šæ‰æ€§èƒ½å˜åŒ–åŸå› åˆ†æ
        # if 'performance_change_explanations' in analysis_result:
        #     explanations = analysis_result['performance_change_explanations']
        #
        #     if explanations['key_findings']:
        #         insights.append("ğŸ” å¯èƒ½çš„å˜åŒ–åŸå› :")
        #         insights.extend([f"   â€¢ {finding}" for finding in explanations['key_findings']])
        #
        #     if explanations['potential_causes']:
        #         insights.append("ğŸ“‹ ç¯å¢ƒå› ç´ å·®å¼‚:")
        #         insights.extend([f"   â€¢ {cause}" for cause in explanations['potential_causes'][:3]])  # åªæ˜¾ç¤ºå‰3ä¸ª

        return insights

    def _configure_chart_fonts(self) -> str:
        """é…ç½®å›¾è¡¨å­—ä½“å¹¶è¿”å›é€‰ä¸­çš„å­—ä½“åç§°"""
        import matplotlib.font_manager as fm

        # è·å–ç³»ç»Ÿå¯ç”¨å­—ä½“åˆ—è¡¨
        available_fonts = [f.name for f in fm.fontManager.ttflist]

        # å®šä¹‰ä¼˜å…ˆä½¿ç”¨çš„è‹±æ–‡å­—ä½“åˆ—è¡¨
        preferred_fonts = [
            'DejaVu Sans', 'Arial', 'Helvetica',
            'Verdana', 'Tahoma', 'Calibri', 'Segoe UI', 'Ubuntu',
            'Droid Sans', 'Noto Sans', 'sans-serif'
        ]

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„å­—ä½“
        selected_font = 'sans-serif'  # é»˜è®¤å­—ä½“
        for font in preferred_fonts:
            if font in available_fonts:
                selected_font = font
                break

        # è®¾ç½®å­—ä½“é…ç½®
        plt.rcParams['font.family'] = [selected_font]
        plt.rcParams['axes.unicode_minus'] = False
        plt.rcParams['font.size'] = 10

        return selected_font

    async def _generate_comprehensive_analysis(self, all_results: Dict) -> Dict:
        """ç”Ÿæˆç»¼åˆåˆ†æç»“æœ"""

        comprehensive = {
            'quality_level_summary': {},
            'device_ranking': [],
            'metric_impact_analysis': {},
            'dimension_impact_analysis': {},
            'overall_insights': []
        }

        available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)
        available_common_columns = getattr(self, 'available_common_columns', COMMON_COLUMNS)

        # è´¨é‡ç­‰çº§æ±‡æ€»
        for quality_level, quality_results in all_results.items():
            # ç¡®ä¿quality_levelæ˜¯åŸç”ŸPythonç±»å‹
            quality_level_key = int(quality_level) if hasattr(quality_level, 'item') else quality_level

            quality_summary = {
                'total_devices': len(quality_results),
                'degraded_devices': 0,
                'improved_devices': 0,
                'stable_devices': 0,
                'avg_confidence': 0.0
            }

            total_confidence = 0.0
            for device_model, device_analysis in quality_results.items():
                status = device_analysis['overall_status']
                if status == 'degraded':
                    quality_summary['degraded_devices'] += 1
                elif status == 'improved':
                    quality_summary['improved_devices'] += 1
                else:
                    quality_summary['stable_devices'] += 1

                total_confidence += device_analysis['confidence_score']

            quality_summary['avg_confidence'] = total_confidence / len(quality_results) if quality_results else 0
            comprehensive['quality_level_summary'][quality_level_key] = quality_summary

        # è®¾å¤‡æ’å
        all_devices = []
        for quality_level, quality_results in all_results.items():
            for device_model, device_analysis in quality_results.items():
                # ç¡®ä¿æ‰€æœ‰æ•°å€¼ç±»å‹éƒ½æ˜¯PythonåŸç”Ÿç±»å‹
                all_devices.append({
                    'device_model': str(device_model),
                    'quality_level': int(quality_level) if hasattr(quality_level, 'item') else quality_level,
                    'overall_status': str(device_analysis['overall_status']),
                    'degradation_score': float(device_analysis['degradation_score']),
                    'confidence_score': float(device_analysis['confidence_score'])
                })

        # æŒ‰æ¶åŒ–ç¨‹åº¦å’Œç½®ä¿¡åº¦æ’åº
        degraded_devices = [d for d in all_devices if d['overall_status'] == 'degraded']
        degraded_devices.sort(key=lambda x: (x['degradation_score'], x['confidence_score']), reverse=True)
        comprehensive['device_ranking'] = degraded_devices[:10]  # Top 10 æ¶åŒ–è®¾å¤‡

        # 3. Generate metric impact analysis
        comprehensive['metric_impact_analysis'] = self._analyze_metric_impacts(all_results)

        # 4. Generate dimension impact analysis
        comprehensive['dimension_impact_analysis'] = self._analyze_dimension_impacts(
            all_results, available_common_columns)

        # 5. Generate overall insights
        comprehensive['overall_insights'] = self._generate_overall_insights(
            comprehensive, available_key_metrics, available_common_columns)

        return comprehensive

    def _analyze_metric_impacts(self, all_results: Dict) -> Dict:
        """åˆ†ææŒ‡æ ‡å½±å“ç¨‹åº¦"""
        metric_impacts = {}
        available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)

        for metric in available_key_metrics:
            metric_data = {
                'total_devices_analyzed': 0,
                'devices_with_degradation': 0,
                'avg_degradation_magnitude': 0,
                'max_degradation': 0,
                'affected_quality_levels': [],
                'top_affected_devices': []
            }

            all_degradations = []
            affected_devices = []

            for quality_level, quality_results in all_results.items():
                quality_has_degradation = False

                for device_model, device_analysis in quality_results.items():
                    metric_data['total_devices_analyzed'] += 1

                    # æ£€æŸ¥è¯¥æŒ‡æ ‡çš„å˜åŒ–
                    if 'key_metrics_analysis' in device_analysis:
                        metric_analysis = device_analysis['key_metrics_analysis'].get(metric, {})

                        if metric_analysis.get('change_percent', 0) > 0:  # æ¶åŒ–
                            metric_data['devices_with_degradation'] += 1
                            quality_has_degradation = True

                            degradation = metric_analysis['change_percent']
                            all_degradations.append(degradation)

                            affected_devices.append({
                                'device_model': device_model,
                                'quality_level': quality_level,
                                'degradation_percentage': float(degradation)
                            })

                if quality_has_degradation:
                    metric_data['affected_quality_levels'].append(quality_level)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            if all_degradations:
                metric_data['avg_degradation_magnitude'] = float(np.mean(all_degradations))
                metric_data['max_degradation'] = float(np.max(all_degradations))

                # è·å–æœ€å—å½±å“çš„è®¾å¤‡ï¼ˆå‰5ä¸ªï¼‰
                affected_devices.sort(key=lambda x: x['degradation_percentage'], reverse=True)
                metric_data['top_affected_devices'] = affected_devices[:5]

            metric_impacts[metric] = metric_data

        return metric_impacts

    def _analyze_dimension_impacts(self, all_results: Dict, available_common_columns: List[str]) -> Dict:
        """åˆ†æç»´åº¦å½±å“ç¨‹åº¦"""
        dimension_impacts = {}

        for dimension in available_common_columns:
            dimension_data = {
                'total_analysis_count': 0,
                'significant_differences_found': 0,
                'avg_impact_magnitude': 0,
                'top_impacted_scenarios': [],
                'quality_level_distribution': {}
            }

            all_impacts = []
            impacted_scenarios = []

            for quality_level, quality_results in all_results.items():
                quality_impacts = 0

                for device_model, device_analysis in quality_results.items():
                    # æ³¨é‡Šæ‰æ£€æŸ¥ç»´åº¦åˆ†æç»“æœ
                    # explanations = device_analysis.get('performance_change_explanations', {})
                    # dimension_differences = explanations.get('dimension_differences', {})
                    #
                    # if dimension in dimension_differences:
                    #     dimension_analysis = dimension_differences[dimension]
                        dimension_data['total_analysis_count'] += 1

                        # æ³¨é‡Šæ‰ç»´åº¦å·®å¼‚åˆ†æ
                        # if dimension_analysis.get('has_significant_difference', False):
                        #     dimension_data['significant_differences_found'] += 1
                        #     quality_impacts += 1
                        #
                        #     # è®¡ç®—å½±å“ç¨‹åº¦ï¼ˆåŸºäºå·®å¼‚çš„æ˜¾è‘—æ€§ï¼‰
                        #     difference_details = dimension_analysis.get('difference_details', {})
                        #     impact_magnitude = 1.0 if dimension_analysis.get('has_significant_difference') else 0.0
                        #     all_impacts.append(impact_magnitude)
                        #
                        #     impacted_scenarios.append({
                        #         'device_model': device_model,
                        #         'quality_level': quality_level,
                        #         'dimension_value': dimension,
                        #         'difference_summary': dimension_analysis.get('summary', ''),
                        #         'impact_magnitude': float(impact_magnitude)
                        #     })

                dimension_data['quality_level_distribution'][str(quality_level)] = quality_impacts

            # è®¡ç®—å¹³å‡å½±å“ç¨‹åº¦
            if all_impacts:
                dimension_data['avg_impact_magnitude'] = float(np.mean(all_impacts))

                # è·å–æœ€å—å½±å“çš„åœºæ™¯ï¼ˆå‰5ä¸ªï¼‰
                impacted_scenarios.sort(key=lambda x: x['impact_magnitude'], reverse=True)
                dimension_data['top_impacted_scenarios'] = impacted_scenarios[:5]

            dimension_impacts[dimension] = dimension_data

        return dimension_impacts

    def _generate_overall_insights(self, comprehensive: Dict, available_key_metrics: List[str],
                                 available_common_columns: List[str]) -> List[str]:
        """ç”Ÿæˆæ•´ä½“æ´å¯Ÿ"""
        insights = []

        # 1. è®¾å¤‡æ’åæ´å¯Ÿ
        if comprehensive['device_ranking']:
            top_degraded = comprehensive['device_ranking'][0]
            insights.append(f"ğŸš¨ æœ€éœ€è¦å…³æ³¨çš„è®¾å¤‡: {top_degraded['device_model']} "
                          f"(è´¨é‡ç­‰çº§{top_degraded['quality_level']})ï¼Œæ¶åŒ–è¯„åˆ†: "
                          f"{top_degraded['degradation_score']:.2f}")

            total_degraded = len(comprehensive['device_ranking'])
            insights.append(f"ğŸ“Š å…±å‘ç° {total_degraded} ä¸ªæ€§èƒ½æ¶åŒ–çš„è®¾å¤‡å‹å·ç»„åˆ")

        # 2. æŒ‡æ ‡å½±å“æ´å¯Ÿ
        metric_impacts = comprehensive.get('metric_impact_analysis', {})
        if metric_impacts:
            # æ‰¾åˆ°å½±å“æœ€ä¸¥é‡çš„æŒ‡æ ‡
            most_impacted_metric = None
            max_affected_devices = 0

            for metric, data in metric_impacts.items():
                if data['devices_with_degradation'] > max_affected_devices:
                    max_affected_devices = data['devices_with_degradation']
                    most_impacted_metric = metric

            if most_impacted_metric:
                metric_data = metric_impacts[most_impacted_metric]
                insights.append(f"âš ï¸ æœ€å—å½±å“çš„æ€§èƒ½æŒ‡æ ‡: {most_impacted_metric}ï¼Œ"
                              f"å½±å“äº† {metric_data['devices_with_degradation']} ä¸ªè®¾å¤‡ï¼Œ"
                              f"å¹³å‡æ¶åŒ–ç¨‹åº¦: {metric_data['avg_degradation_magnitude']:.2f}%")

        # 3. ç»´åº¦å½±å“æ´å¯Ÿ
        dimension_impacts = comprehensive.get('dimension_impact_analysis', {})
        if dimension_impacts:
            # æ‰¾åˆ°å½±å“æœ€æ˜¾è‘—çš„ç»´åº¦
            most_significant_dimension = None
            max_significance_rate = 0

            for dimension, data in dimension_impacts.items():
                if data['total_analysis_count'] > 0:
                    significance_rate = data['significant_differences_found'] / data['total_analysis_count']
                    if significance_rate > max_significance_rate:
                        max_significance_rate = significance_rate
                        most_significant_dimension = dimension

            if most_significant_dimension:
                dimension_data = dimension_impacts[most_significant_dimension]
                insights.append(f"ğŸ” æœ€å…³é”®çš„ç¯å¢ƒç»´åº¦: {most_significant_dimension}ï¼Œ"
                              f"åœ¨ {dimension_data['significant_differences_found']} ä¸ªåœºæ™¯ä¸­å‘ç°æ˜¾è‘—å·®å¼‚ï¼Œ"
                              f"æ˜¾è‘—æ€§æ¯”ä¾‹: {max_significance_rate:.1%}")

        # 4. è´¨é‡ç­‰çº§æ´å¯Ÿ
        quality_summary = comprehensive.get('quality_level_summary', {})
        if quality_summary:
            total_quality_levels = len(quality_summary)
            affected_levels = len([level for level, data in quality_summary.items()
                                 if data['degraded_devices'] > 0])

            insights.append(f"ğŸ“ˆ è´¨é‡ç­‰çº§åˆ†å¸ƒ: å…±åˆ†æ {total_quality_levels} ä¸ªè´¨é‡ç­‰çº§ï¼Œ"
                          f"å…¶ä¸­ {affected_levels} ä¸ªç­‰çº§å­˜åœ¨æ€§èƒ½æ¶åŒ–è®¾å¤‡")

        # 5. æ•°æ®å®Œæ•´æ€§æ´å¯Ÿ
        available_metrics_count = len(available_key_metrics)
        available_dimensions_count = len(available_common_columns)

        insights.append(f"ğŸ“‹ æ•°æ®å®Œæ•´æ€§: å¯åˆ†æ {available_metrics_count}/{len(KEY_PERFORMANCE_METRICS)} ä¸ªå…³é”®æŒ‡æ ‡ï¼Œ"
                       f"{available_dimensions_count}/{len(COMMON_COLUMNS)} ä¸ªç¯å¢ƒç»´åº¦")

        # 6. æ¨èå»ºè®®
        if comprehensive['device_ranking']:
            insights.append("ğŸ’¡ å»ºè®®ä¼˜å…ˆä¼˜åŒ–æ’åå‰3çš„æ¶åŒ–è®¾å¤‡ï¼Œé‡ç‚¹å…³æ³¨æœ€å—å½±å“çš„æ€§èƒ½æŒ‡æ ‡")

        if dimension_impacts:
            insights.append("ğŸ”§ å»ºè®®æ·±å…¥åˆ†æå…³é”®ç¯å¢ƒç»´åº¦çš„é…ç½®å·®å¼‚ï¼Œè¯†åˆ«æ½œåœ¨çš„ç¯å¢ƒå› ç´ å½±å“")

        return insights

    async def _create_comprehensive_visualizations(self, all_results: Dict, output_dir: str) -> None:
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨"""

        # æ£€æµ‹å¹¶è®¾ç½®å¯ç”¨çš„è‹±æ–‡å­—ä½“
        selected_font = self._configure_chart_fonts()

        logger.info(f"Using font: {selected_font} for chart generation")

        logger.info("Generating comprehensive visualization charts...")

        # 1. æŒ‰è´¨é‡ç­‰çº§åˆ†åˆ«ç”Ÿæˆæ¦‚è§ˆå›¾
        await self._create_quality_level_overview_separate(all_results, output_dir)

        # 2. æŒ‰è´¨é‡ç­‰çº§åˆ†åˆ«ç”Ÿæˆè®¾å¤‡æŒ‡æ ‡çƒ­åŠ›å›¾
        await self._create_metrics_heatmap_by_quality(all_results, output_dir)

        # 3. ç”Ÿæˆå…³é”®æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ
        await self._create_metrics_correlation_analysis(all_results, output_dir)

        # 4. æš‚æ—¶è·³è¿‡å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆåŸå§‹æ•°æ®æœªä¿å­˜åœ¨åˆ†æç»“æœä¸­ï¼‰
        logger.info("Skipping outlier detection - raw data not preserved in analysis results")
        # await self._create_outlier_detection_charts(all_results, output_dir)

        # 5. å…¬å…±ç»´åº¦å½±å“åˆ†æ
        await self._create_dimension_impact_charts(all_results, output_dir)

        # 6. è®¾å¤‡æ€§èƒ½æ’åå›¾
        await self._create_device_ranking_chart(all_results, output_dir)

        # 7. Unityç‰ˆæœ¬åˆ†å¸ƒå¯è§†åŒ–
        await self._create_unity_version_distribution_charts(all_results, output_dir)

    async def _create_quality_level_overview(self, all_results: Dict, output_dir: str) -> None:
        """åˆ›å»ºè´¨é‡ç­‰çº§æ¦‚è§ˆå›¾"""

        available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)

        quality_levels = list(all_results.keys())
        metrics_data = {metric: [] for metric in available_key_metrics}

        for quality_level in quality_levels:
            quality_results = all_results[quality_level]

            for metric in available_key_metrics:
                # è®¡ç®—è¯¥è´¨é‡ç­‰çº§ä¸‹è¯¥æŒ‡æ ‡çš„å¹³å‡å˜åŒ–
                metric_changes = []
                for device_model, device_analysis in quality_results.items():
                    if metric in device_analysis['key_metrics_analysis']:
                        change = device_analysis['key_metrics_analysis'][metric]['change_percent']
                        if not np.isnan(change):
                            metric_changes.append(change)

                avg_change = np.mean(metric_changes) if metric_changes else 0
                metrics_data[metric].append(avg_change)

        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(12, 8))
        heatmap_data = [metrics_data[metric] for metric in available_key_metrics]

        im = plt.imshow(heatmap_data, cmap='RdYlBu_r', aspect='auto')

        plt.title('Key Metrics Performance by Quality Level', fontsize=16, fontweight='bold')
        plt.xlabel('Quality Levels', fontsize=12)
        plt.ylabel('Key Metrics', fontsize=12)

        plt.xticks(range(len(quality_levels)), quality_levels)
        plt.yticks(range(len(available_key_metrics)), available_key_metrics)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i in range(len(available_key_metrics)):
            for j in range(len(quality_levels)):
                text = plt.text(j, i, f'{heatmap_data[i][j]:.1f}%',
                              ha="center", va="center", color="black" if abs(heatmap_data[i][j]) < 2 else "white")

        plt.colorbar(im, label='Performance Change (%)')
        plt.tight_layout()

        chart_path = os.path.join(output_dir, 'quality_level_performance_overview.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved quality level performance overview: {chart_path}")

    async def _create_quality_level_overview_separate(self, all_results: Dict, output_dir: str) -> None:
        """Create overview charts by quality level with pagination (50 devices max per chart)"""

        # Configure fonts for proper device name handling
        selected_font = self._configure_chart_fonts()

        available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)

        for quality_level, quality_results in all_results.items():
            if not quality_results:
                continue

            # Collect device data for this quality level
            device_names = []
            for device_model in quality_results.keys():
                # Clean device name to ensure proper display
                clean_device_name = str(device_model).encode('ascii', 'ignore').decode('ascii')
                if not clean_device_name.strip():
                    clean_device_name = f"Device_{len(device_names) + 1}"
                device_names.append(clean_device_name)

            metrics_data = {metric: [] for metric in available_key_metrics}

            for device_model, device_analysis in quality_results.items():
                for metric in available_key_metrics:
                    if metric in device_analysis['key_metrics_analysis']:
                        change = device_analysis['key_metrics_analysis'][metric]['change_percent']
                        metrics_data[metric].append(change if not np.isnan(change) else 0)
                    else:
                        metrics_data[metric].append(0)

            # Split into chunks of 50 devices maximum
            chunk_size = 50
            total_devices = len(device_names)

            for chunk_idx in range(0, total_devices, chunk_size):
                end_idx = min(chunk_idx + chunk_size, total_devices)

                chunk_device_names = device_names[chunk_idx:end_idx]
                chunk_metrics_data = {metric: values[chunk_idx:end_idx] for metric, values in metrics_data.items()}

                # Create heatmap for this chunk
                plt.figure(figsize=(max(10, len(chunk_device_names) * 0.8), max(6, len(available_key_metrics) * 0.6)))
                heatmap_data = [chunk_metrics_data[metric] for metric in available_key_metrics]

                im = plt.imshow(heatmap_data, cmap='RdYlBu_r', aspect='auto')

                # Determine chart title with page info if multiple chunks
                if total_devices > chunk_size:
                    page_num = (chunk_idx // chunk_size) + 1
                    total_pages = (total_devices + chunk_size - 1) // chunk_size
                    title = f'Quality Level {quality_level} - Key Metrics Performance (Page {page_num}/{total_pages})'
                else:
                    title = f'Quality Level {quality_level} - Key Metrics Performance'

                plt.title(title, fontsize=16, fontweight='bold')
                plt.xlabel('Device Models', fontsize=12)
                plt.ylabel('Key Metrics', fontsize=12)

                plt.xticks(range(len(chunk_device_names)), chunk_device_names, rotation=45, ha='right')
                plt.yticks(range(len(available_key_metrics)), available_key_metrics)

                # Add value labels
                for i in range(len(available_key_metrics)):
                    for j in range(len(chunk_device_names)):
                        if j < len(heatmap_data[i]):
                            text = plt.text(j, i, f'{heatmap_data[i][j]:.1f}%',
                                          ha="center", va="center",
                                          color="black" if abs(heatmap_data[i][j]) < 2 else "white",
                                          fontsize=8)

                plt.colorbar(im, label='Performance Change (%)')
                plt.tight_layout()

                # Generate filename with page number if multiple chunks
                if total_devices > chunk_size:
                    page_num = (chunk_idx // chunk_size) + 1
                    chart_path = os.path.join(output_dir, f'quality_level_{quality_level}_performance_overview_page_{page_num}.png')
                else:
                    chart_path = os.path.join(output_dir, f'quality_level_{quality_level}_performance_overview.png')

                plt.savefig(chart_path, dpi=300, bbox_inches='tight')
                plt.close()

                logger.info(f"Saved quality level {quality_level} performance overview: {chart_path}")

    async def _create_metrics_heatmap_by_quality(self, all_results: Dict, output_dir: str) -> None:
        """Create device metrics heatmaps by quality level with pagination (50 devices max per chart)"""

        # Configure fonts for proper device name handling
        selected_font = self._configure_chart_fonts()

        available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)

        for quality_level, quality_results in all_results.items():
            if not quality_results:
                continue

            # Collect device data for this quality level
            device_metric_data = []
            device_labels = []

            for device_model, device_analysis in quality_results.items():
                # Clean device name to ensure proper display
                clean_device_name = str(device_model).encode('ascii', 'ignore').decode('ascii')
                if not clean_device_name.strip():
                    clean_device_name = f"Device_{len(device_labels) + 1}"

                device_labels.append(clean_device_name)

                metric_row = []
                for metric in available_key_metrics:
                    if metric in device_analysis['key_metrics_analysis']:
                        change = device_analysis['key_metrics_analysis'][metric]['change_percent']
                        metric_row.append(change if not np.isnan(change) else 0)
                    else:
                        metric_row.append(0)

                device_metric_data.append(metric_row)

            if not device_metric_data:
                continue

            # Split into chunks of 50 devices maximum
            chunk_size = 50
            total_devices = len(device_labels)

            for chunk_idx in range(0, total_devices, chunk_size):
                end_idx = min(chunk_idx + chunk_size, total_devices)

                chunk_device_labels = device_labels[chunk_idx:end_idx]
                chunk_device_data = device_metric_data[chunk_idx:end_idx]

                # Create heatmap for this chunk
                plt.figure(figsize=(max(10, len(available_key_metrics) * 1.2), max(8, len(chunk_device_labels) * 0.5)))

                im = plt.imshow(chunk_device_data, cmap='RdYlBu_r', aspect='auto')

                # Determine chart title with page info if multiple chunks
                if total_devices > chunk_size:
                    page_num = (chunk_idx // chunk_size) + 1
                    total_pages = (total_devices + chunk_size - 1) // chunk_size
                    title = f'Quality Level {quality_level} - Device Performance Heatmap (Page {page_num}/{total_pages})'
                else:
                    title = f'Quality Level {quality_level} - Device Performance Heatmap'

                plt.title(title, fontsize=16, fontweight='bold')
                plt.xlabel('Key Performance Metrics', fontsize=12)
                plt.ylabel('Device Models', fontsize=12)

                plt.xticks(range(len(available_key_metrics)), available_key_metrics, rotation=45, ha='right')
                plt.yticks(range(len(chunk_device_labels)), chunk_device_labels, fontsize=10)

                plt.colorbar(im, label='Performance Change (%)')
                plt.tight_layout()

                # Generate filename with page number if multiple chunks
                if total_devices > chunk_size:
                    page_num = (chunk_idx // chunk_size) + 1
                    chart_path = os.path.join(output_dir, f'quality_level_{quality_level}_device_metrics_heatmap_page_{page_num}.png')
                else:
                    chart_path = os.path.join(output_dir, f'quality_level_{quality_level}_device_metrics_heatmap.png')

                plt.savefig(chart_path, dpi=300, bbox_inches='tight')
                plt.close()

                logger.info(f"Saved quality level {quality_level} device metrics heatmap: {chart_path}")

    async def _create_metrics_correlation_analysis(self, all_results: Dict, output_dir: str) -> None:
        """åˆ›å»ºå…³é”®æŒ‡æ ‡ä¹‹é—´çš„å…³è”åˆ†æ"""

        available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)

        for quality_level, quality_results in all_results.items():
            if not quality_results:
                continue

            # æ”¶é›†è¯¥è´¨é‡ç­‰çº§æ‰€æœ‰è®¾å¤‡çš„æŒ‡æ ‡æ•°æ®
            metrics_data = {metric: [] for metric in available_key_metrics}

            for device_model, device_analysis in quality_results.items():
                for metric in available_key_metrics:
                    if metric in device_analysis['key_metrics_analysis']:
                        change = device_analysis['key_metrics_analysis'][metric]['change_percent']
                        metrics_data[metric].append(change if not np.isnan(change) else 0)
                    else:
                        metrics_data[metric].append(0)

            # åˆ›å»ºæ•°æ®æ¡†è®¡ç®—ç›¸å…³æ€§
            if all(len(values) > 1 for values in metrics_data.values()):
                df = pd.DataFrame(metrics_data)
                correlation_matrix = df.corr()

                # åˆ›å»ºç›¸å…³æ€§çƒ­åŠ›å›¾
                plt.figure(figsize=(10, 8))
                mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

                im = plt.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')

                plt.title(f'Quality Level {quality_level} - Metrics Correlation Analysis', fontsize=16, fontweight='bold')
                plt.xlabel('Key Performance Metrics', fontsize=12)
                plt.ylabel('Key Performance Metrics', fontsize=12)

                plt.xticks(range(len(available_key_metrics)), available_key_metrics, rotation=45, ha='right')
                plt.yticks(range(len(available_key_metrics)), available_key_metrics)

                # æ·»åŠ ç›¸å…³ç³»æ•°æ ‡ç­¾
                for i in range(len(available_key_metrics)):
                    for j in range(len(available_key_metrics)):
                        corr_value = correlation_matrix.iloc[i, j]
                        if not pd.isna(corr_value):
                            try:
                                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å†è½¬å›æµ®ç‚¹æ•°
                                corr_str = str(corr_value)
                                corr_val = float(corr_str)
                                text = plt.text(j, i, f'{corr_val:.2f}',
                                              ha="center", va="center",
                                              color="black" if abs(corr_val) < 0.5 else "white",
                                              fontsize=8)
                            except (ValueError, TypeError):
                                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œåªæ˜¾ç¤ºæ–‡æœ¬
                                text = plt.text(j, i, "N/A",
                                              ha="center", va="center",
                                              color="gray",
                                              fontsize=8)

                plt.colorbar(im, label='Correlation Coefficient')
                plt.tight_layout()

                chart_path = os.path.join(output_dir, f'quality_level_{quality_level}_metrics_correlation.png')
                plt.savefig(chart_path, dpi=300, bbox_inches='tight')
                plt.close()

                logger.info(f"Saved quality level {quality_level} metrics correlation: {chart_path}")

    async def _create_outlier_detection_charts(self, all_results: Dict, output_dir: str) -> None:
        """å¯¹åŒä¸€è´¨é‡ç­‰çº§å’Œè®¾å¤‡å‹å·ä¸­çš„controlå’Œtestç»„è¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹"""

        available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)

        for quality_level, quality_results in all_results.items():
            if not quality_results:
                continue

            for device_model, device_analysis in quality_results.items():
                control_data = device_analysis['control_data']
                test_data = device_analysis['test_data']

                # ä¸ºæ¯ä¸ªæŒ‡æ ‡æ£€æµ‹å¼‚å¸¸å€¼
                fig, axes = plt.subplots(2, 2, figsize=(16, 12))
                fig.suptitle(f'Quality {quality_level} - {device_model} - Outlier Detection', fontsize=16, fontweight='bold')

                axes = axes.flatten()

                for idx, metric in enumerate(available_key_metrics[:4]):  # æœ€å¤šæ˜¾ç¤º4ä¸ªæŒ‡æ ‡
                    if idx >= len(axes):
                        break

                    # è·å–controlå’Œtestç»„çš„æŒ‡æ ‡æ•°æ®
                    control_values = []
                    test_values = []

                    # ä»DataFrameè¡Œä¸­æå–æ•°æ®
                    for _, row in control_data.iterrows():
                        if metric in row and pd.notna(row[metric]):
                            value = row[metric]
                            if not pd.isna(value):
                                control_values.append(float(value))

                    for _, row in test_data.iterrows():
                        if metric in row and pd.notna(row[metric]):
                            value = row[metric]
                            if not pd.isna(value):
                                test_values.append(float(value))

                    # è®¡ç®—å¼‚å¸¸å€¼
                    all_values = control_values + test_values
                    if len(all_values) > self.config.min_outlier_data_points:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®ç‚¹
                        Q1 = np.percentile(all_values, 25)
                        Q3 = np.percentile(all_values, 75)
                        IQR = Q3 - Q1
                        lower_bound = Q1 - self.config.outlier_iqr_factor * IQR
                        upper_bound = Q3 + self.config.outlier_iqr_factor * IQR

                        # è¯†åˆ«å¼‚å¸¸å€¼
                        control_outliers = [x for x in control_values if x < lower_bound or x > upper_bound]
                        test_outliers = [x for x in test_values if x < lower_bound or x > upper_bound]

                        # ç»˜åˆ¶ç®±çº¿å›¾
                        ax = axes[idx]
                        box_data = [control_values, test_values]
                        box_labels = ['Control', 'Test']

                        bp = ax.boxplot(box_data, labels=box_labels, patch_artist=True)
                        bp['boxes'][0].set_facecolor('lightblue')
                        bp['boxes'][1].set_facecolor('lightcoral')

                        ax.set_title(f'{metric}\nControl outliers: {len(control_outliers)}, Test outliers: {len(test_outliers)}')
                        ax.set_ylabel('Value')
                        ax.grid(True, alpha=0.3)

                        # æ ‡è®°å¼‚å¸¸å€¼èŒƒå›´
                        ax.axhline(y=upper_bound, color='red', linestyle='--', alpha=0.7, label='Upper threshold')
                        ax.axhline(y=lower_bound, color='red', linestyle='--', alpha=0.7, label='Lower threshold')

                # éšè—å¤šä½™çš„å­å›¾
                for idx in range(len(available_key_metrics), len(axes)):
                    axes[idx].set_visible(False)

                plt.tight_layout()
                chart_path = os.path.join(output_dir, f'quality_{quality_level}_{device_model}_outlier_detection.png')
                plt.savefig(chart_path, dpi=300, bbox_inches='tight')
                plt.close()

                logger.info(f"Saved outlier detection for quality {quality_level}, device {device_model}: {chart_path}")

    async def _create_metrics_heatmap(self, all_results: Dict, output_dir: str) -> None:
        """åˆ›å»ºæŒ‡æ ‡çƒ­åŠ›å›¾"""

        available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)

        # æ”¶é›†æ‰€æœ‰è®¾å¤‡çš„æŒ‡æ ‡æ•°æ®
        device_metric_data = []
        device_labels = []

        for quality_level, quality_results in all_results.items():
            for device_model, device_analysis in quality_results.items():
                device_label = f"{device_model}\n(Q{quality_level})"
                device_labels.append(device_label)

                metric_row = []
                for metric in available_key_metrics:
                    if metric in device_analysis['key_metrics_analysis']:
                        change = device_analysis['key_metrics_analysis'][metric]['change_percent']
                        metric_row.append(change if not np.isnan(change) else 0)
                    else:
                        metric_row.append(0)

                device_metric_data.append(metric_row)

        if not device_metric_data:
            return

        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(15, max(8, len(device_labels) * 0.3)))

        im = plt.imshow(device_metric_data, cmap='RdYlBu_r', aspect='auto')

        plt.title('Device Performance Heatmap - Key Metrics Change (%)', fontsize=16, fontweight='bold')
        plt.xlabel('Key Performance Metrics', fontsize=12)
        plt.ylabel('Devices (Quality Level)', fontsize=12)

        plt.xticks(range(len(available_key_metrics)), available_key_metrics, rotation=45, ha='right')
        plt.yticks(range(len(device_labels)), device_labels, fontsize=10)

        plt.colorbar(im, label='Performance Change (%)')
        plt.tight_layout()

        chart_path = os.path.join(output_dir, 'device_metrics_heatmap.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved device metrics heatmap: {chart_path}")

    async def _create_dimension_impact_charts(self, all_results: Dict, output_dir: str) -> None:
        """åˆ›å»ºå…¬å…±ç»´åº¦å½±å“åˆ†æå›¾"""

        available_common_columns = getattr(self, 'available_common_columns', COMMON_COLUMNS)
        logger.info(f"å¯ç”¨çš„å…¬å…±ç»´åº¦åˆ—: {available_common_columns}")

        # ä¸ºæ¯ä¸ªé‡è¦ç»´åº¦åˆ›å»ºåˆ†æå›¾ï¼ŒåŒ…æ‹¬æ–°å¢çš„é¢„å¤„ç†ç‰¹å¾
        important_dimensions = ['ischarged', 'cpumodel', 'refreshrate', 'time_flag', 'memory_category', 'unity_digital']

        for dimension in important_dimensions:
            logger.info(f"æ£€æŸ¥ç»´åº¦: {dimension}")

            if dimension in available_common_columns:
                logger.info(f"Dimension {dimension} exists in data, starting chart generation...")
                await self._create_single_dimension_chart(all_results, dimension, output_dir)
            else:
                logger.warning(f"ç»´åº¦ {dimension} ä¸å­˜åœ¨äºå¯ç”¨åˆ—ä¸­ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
                logger.info(f"å½“å‰å¯ç”¨ç»´åº¦: {available_common_columns}")

    async def _create_single_dimension_chart(self, all_results: Dict, dimension: str, output_dir: str) -> None:
        """Create single dimension impact analysis chart"""

        logger.info(f"Starting to collect analysis data for dimension {dimension}...")

        # Collect analysis data for this dimension
        dimension_insights = []

        for quality_level, quality_results in all_results.items():
            for device_model, device_analysis in quality_results.items():
                # Comment out dimension analysis related visualization data collection
                # explanations = device_analysis.get('performance_change_explanations', {})
                # dimension_differences = explanations.get('dimension_differences', {})
                #
                # if dimension in dimension_differences:
                #     dim_analysis = dimension_differences[dimension]
                #     logger.info(f"è®¾å¤‡ {device_model} åœ¨ç»´åº¦ {dimension} æœ‰åˆ†ææ•°æ®")
                #
                #     for insight in dim_analysis.get('explanations', []):
                #         dimension_insights.append({
                #             'device': f"{device_model} (Q{quality_level})",
                #             'insight': insight,
                #             'quality_level': quality_level
                #         })
                # else:
                #     logger.debug(f"è®¾å¤‡ {device_model} åœ¨ç»´åº¦ {dimension} æ²¡æœ‰æ˜¾è‘—å·®å¼‚")
                pass

        logger.info(f"Dimension {dimension} collected {len(dimension_insights)} insights")

        if not dimension_insights:
            logger.warning(f"Dimension {dimension} has no analysis data, skipping chart generation")
            logger.info(f"Possible reasons: 1) This dimension has no significant differences across all devices; 2) No devices have performance changes; 3) Data quality issues")
            return

        # Create text chart
        plt.figure(figsize=(14, max(6, len(dimension_insights) * 0.4)))

        y_positions = list(range(len(dimension_insights)))
        colors = ['red' if 'higher' in insight['insight'] or 'more' in insight['insight'] else 'blue'
                 for insight in dimension_insights]

        plt.barh(y_positions, [1]*len(dimension_insights), color=colors, alpha=0.3)

        plt.title(f'{dimension.capitalize()} Impact Analysis', fontsize=16, fontweight='bold')
        plt.xlabel('Impact Direction', fontsize=12)
        plt.ylabel('Devices', fontsize=12)

        # Add insight text
        for i, insight_data in enumerate(dimension_insights):
            plt.text(0.5, i, insight_data['insight'], ha='center', va='center',
                    fontsize=10, wrap=True)

        plt.yticks(y_positions, [insight['device'] for insight in dimension_insights])
        plt.xlim(0, 1)
        plt.tight_layout()

        chart_path = os.path.join(output_dir, f'{dimension}_impact_analysis.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Saved {dimension} dimension impact analysis chart: {chart_path}")

    async def _create_device_ranking_chart(self, all_results: Dict, output_dir: str) -> None:
        """Create device performance ranking charts with pagination (50 devices max per chart)"""

        # Configure fonts for proper device name handling
        selected_font = self._configure_chart_fonts()

        # Collect all degraded devices
        degraded_devices = []

        for quality_level, quality_results in all_results.items():
            for device_model, device_analysis in quality_results.items():
                if device_analysis['overall_status'] == 'degraded':
                    # Clean device name to ensure proper display
                    clean_device_name = str(device_model).encode('ascii', 'ignore').decode('ascii')
                    if not clean_device_name.strip():
                        clean_device_name = f"Device_{len(degraded_devices) + 1}"

                    degraded_devices.append({
                        'device': f"{clean_device_name} (Q{quality_level})",
                        'degradation_score': device_analysis['degradation_score'],
                        'confidence_score': device_analysis['confidence_score'],
                        'quality_level': quality_level
                    })

        if not degraded_devices:
            logger.info("No degraded devices found, skipping ranking chart generation")
            return

        # Sort by degradation level
        degraded_devices.sort(key=lambda x: x['degradation_score'], reverse=True)

        # Split into chunks of 50 devices maximum
        chunk_size = 50
        total_devices = len(degraded_devices)

        for chunk_idx in range(0, total_devices, chunk_size):
            end_idx = min(chunk_idx + chunk_size, total_devices)

            chunk_devices = degraded_devices[chunk_idx:end_idx]

            devices = [d['device'] for d in chunk_devices]
            scores = [d['degradation_score'] for d in chunk_devices]
            confidences = [d['confidence_score'] for d in chunk_devices]

            plt.figure(figsize=(12, max(8, len(devices) * 0.5)))

            # Set colors based on confidence
            colors = ['darkred' if conf > 7.5 else 'red' if conf > 5.0 else 'orange'
                     for conf in confidences]

            bars = plt.barh(range(len(devices)), scores, color=colors, alpha=0.7)

            # Determine chart title with page info if multiple chunks
            if total_devices > chunk_size:
                page_num = (chunk_idx // chunk_size) + 1
                total_pages = (total_devices + chunk_size - 1) // chunk_size
                title = f'Top Degraded Devices Ranking (Page {page_num}/{total_pages})'
            else:
                title = 'Top Degraded Devices Ranking'

            plt.title(title, fontsize=16, fontweight='bold')
            plt.xlabel('Average Degradation Score (%)', fontsize=12)
            plt.ylabel('Devices (Quality Level)', fontsize=12)

            plt.yticks(range(len(devices)), devices)

            # Add confidence labels
            for i, (bar, score, confidence) in enumerate(zip(bars, scores, confidences)):
                plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,
                        f'{score:.1f}% (C:{confidence:.1f})',
                        va='center', fontsize=10)

            plt.grid(axis='x', alpha=0.3)
            plt.tight_layout()

            # Generate filename with page number if multiple chunks
            if total_devices > chunk_size:
                page_num = (chunk_idx // chunk_size) + 1
                chart_path = os.path.join(output_dir, f'degraded_devices_ranking_page_{page_num}.png')
            else:
                chart_path = os.path.join(output_dir, 'degraded_devices_ranking.png')

            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()

            logger.info(f"Saved degraded devices ranking chart: {chart_path}")

    async def _create_unity_version_distribution_charts(self, all_results: Dict, output_dir: str) -> None:
        """Create Unity version distribution charts with pagination (50 devices max per chart)"""

        # Configure fonts for proper device name handling
        selected_font = self._configure_chart_fonts()

        available_common_columns = getattr(self, 'available_common_columns', COMMON_COLUMNS)

        # Check if unityversion_inner column exists
        if 'unityversion_inner' not in available_common_columns:
            logger.warning("unityversion_inner column does not exist, skipping Unity version distribution chart generation")
            return

        for quality_level, quality_results in all_results.items():
            if not quality_results:
                continue

            # Include all devices for Unity version distribution analysis
            devices_with_unity_data = []

            for device_model, device_analysis in quality_results.items():
                # Check if device has Unity version data
                control_data = device_analysis.get('control_data', pd.DataFrame())
                test_data = device_analysis.get('test_data', pd.DataFrame())

                if (not control_data.empty and not test_data.empty and
                    'unityversion_inner' in control_data.columns and
                    'unityversion_inner' in test_data.columns):
                    devices_with_unity_data.append((device_model, device_analysis))

            # If no devices have Unity version data, skip this quality level
            if not devices_with_unity_data:
                logger.info(f"Quality level {quality_level} has no devices with Unity version data, skipping chart generation")
                continue

            # Apply pagination: maximum 50 devices, split into multiple charts
            chunk_size = 50
            total_devices = len(devices_with_unity_data)

            for chunk_idx in range(0, total_devices, chunk_size):
                end_idx = min(chunk_idx + chunk_size, total_devices)
                chunk_devices = devices_with_unity_data[chunk_idx:end_idx]

                # Create charts for this chunk of devices
                device_count = len(chunk_devices)
                cols = min(3, device_count)  # Maximum 3 devices per row
                rows = (device_count + cols - 1) // cols

                fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 5))

                # Determine chart title with page info if multiple chunks
                if total_devices > chunk_size:
                    page_num = (chunk_idx // chunk_size) + 1
                    total_pages = (total_devices + chunk_size - 1) // chunk_size
                    title = f'Quality Level {quality_level} - Unity Version Distribution by Device (Page {page_num}/{total_pages})\n(Control vs Test Group Comparison)'
                else:
                    title = f'Quality Level {quality_level} - Unity Version Distribution by Device\n(Control vs Test Group Comparison)'

                fig.suptitle(title, fontsize=14, fontweight='bold')

                # Ensure axes is a 2D array
                if rows == 1 and cols == 1:
                    axes = [[axes]]
                elif rows == 1:
                    axes = [axes]
                elif cols == 1:
                    axes = [[ax] for ax in axes]

                device_idx = 0
                for device_model, device_analysis in chunk_devices:
                    if device_idx >= rows * cols:
                        break

                    row = device_idx // cols
                    col = device_idx % cols
                    ax = axes[row][col]

                    # Clean device name to ensure proper display
                    clean_device_name = str(device_model).encode('ascii', 'ignore').decode('ascii')
                    if not clean_device_name.strip():
                        clean_device_name = f"Device_{device_idx + 1}"

                    # Get Unity version distribution data from control and test groups
                    control_data = device_analysis.get('control_data', pd.DataFrame())
                    test_data = device_analysis.get('test_data', pd.DataFrame())

                    if not control_data.empty and not test_data.empty and 'unityversion_inner' in control_data.columns and 'unityversion_inner' in test_data.columns:
                        # Function to simplify unity version strings
                        def simplify_unity_version(version_str):
                            """Extract version number from unity_inner string"""
                            version_str = str(version_str)
                            # Split by '-' and take the first 4 parts to get version number
                            parts = version_str.split('-')
                            return '-'.join(parts[:2])  # e.g., "2019-2.01.0052.03"

                        # Simplify unity versions for simplboth control and test data
                        control_simplified = control_data['unityversion_inner'].apply(simplify_unity_version)
                        test_simplified = test_data['unityversion_inner'].apply(simplify_unity_version)

                        # Count simplified unity_inner values for control and test separately
                        control_version_counts = control_simplified.value_counts().to_dict()
                        test_version_counts = test_simplified.value_counts().to_dict()

                        # Get all versions from both groups
                        all_versions = set(control_version_counts.keys()).union(set(test_version_counts.keys()))

                        # Get top 6 most common versions by total count
                        version_total_counts = {}
                        for version in all_versions:
                            version_total_counts[version] = control_version_counts.get(version, 0) + test_version_counts.get(version, 0)

                        top_versions = sorted(version_total_counts.items(), key=lambda x: x[1], reverse=True)[:6]
                        top_version_names = [v[0] for v in top_versions]

                        # Prepare data for bar chart - control vs test comparison
                        control_values = [control_version_counts.get(v, 0) for v in top_version_names]
                        test_values = [test_version_counts.get(v, 0) for v in top_version_names]

                        # Calculate total samples for each group
                        control_total = len(control_data)
                        test_total = len(test_data)

                        # Create bar chart for version distribution comparison
                        x = range(len(top_version_names))
                        width = 0.35

                        ax.bar([i - width/2 for i in x], control_values, width, label=f'Control (n={control_total})',
                               alpha=0.8, color='lightblue')
                        ax.bar([i + width/2 for i in x], test_values, width, label=f'Test (n={test_total})',
                               alpha=0.8, color='lightcoral')

                        ax.set_title(f'{clean_device_name}', fontsize=11, fontweight='bold')
                        ax.set_ylabel('Sample Count', fontsize=9)
                        ax.set_xlabel('Unity Version', fontsize=9)
                        ax.set_xticks(x)
                        ax.set_xticklabels(top_version_names, rotation=45, ha='right', fontsize=8)
                        ax.legend(fontsize=8)
                        ax.grid(True, alpha=0.3)

                        # Add count labels on top of bars
                        max_value = max(control_values + test_values) if (control_values + test_values) else 1
                        for i, (control_count, test_count) in enumerate(zip(control_values, test_values)):
                            if control_count > 0:
                                ax.text(i - width/2, control_count + max_value * 0.01,
                                       str(control_count), ha='center', va='bottom', fontsize=8, fontweight='bold')
                            if test_count > 0:
                                ax.text(i + width/2, test_count + max_value * 0.01,
                                       str(test_count), ha='center', va='bottom', fontsize=8, fontweight='bold')

                        # Mark significant differences based on absolute count differences
                        for i, version in enumerate(top_version_names):
                            control_count = control_version_counts.get(version, 0)
                            test_count = test_version_counts.get(version, 0)
                            diff = abs(test_count - control_count)
                            total_samples = control_count + test_count
                            # Consider significant if difference is more than 20% of total or more than 5 samples
                            if diff > max(5, total_samples * 0.2):
                                ax.text(i, max(control_count, test_count) + max_value * 0.1,
                                       f'Î”{diff}', ha='center', va='bottom', fontsize=7, color='red', fontweight='bold')
                    else:
                        # If no Unity version data available, show message
                        ax.text(0.5, 0.5, f"Device: {clean_device_name}\n(No Unity version data)",
                               ha='center', va='center', transform=ax.transAxes,
                               fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))
                        ax.set_title(f'{clean_device_name}', fontsize=11, fontweight='bold')
                        ax.set_xticks([])
                        ax.set_yticks([])

                    device_idx += 1

                # Hide extra subplots
                for idx in range(device_idx, rows * cols):
                    row = idx // cols
                    col = idx % cols
                    axes[row][col].set_visible(False)

                plt.tight_layout()

                # Generate filename with page number if multiple chunks
                if total_devices > chunk_size:
                    page_num = (chunk_idx // chunk_size) + 1
                    chart_path = os.path.join(output_dir, f'unity_version_distribution_quality_{quality_level}_page_{page_num}.png')
                else:
                    chart_path = os.path.join(output_dir, f'unity_version_distribution_quality_{quality_level}.png')

                plt.savefig(chart_path, dpi=300, bbox_inches='tight')
                plt.close()

                logger.info(f"Saved Unity version distribution chart for quality level {quality_level}: {chart_path}")

    async def _save_analysis_results(self, all_results: Dict, comprehensive_analysis: Dict, output_dir: str) -> None:
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""

        # 1. ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
        detailed_results = []

        for quality_level, quality_results in all_results.items():
            for device_model, device_analysis in quality_results.items():

                # åŸºæœ¬ä¿¡æ¯
                row = {
                    'quality_level': quality_level,
                    'device_model': device_model,
                    'control_count': device_analysis['control_count'],
                    'test_count': device_analysis['test_count'],
                    'confidence_score': device_analysis['confidence_score'],
                    'overall_status': device_analysis['overall_status'],
                    'degradation_score': device_analysis['degradation_score']
                }

                # å…³é”®æŒ‡æ ‡åˆ†æ - åªä¿å­˜å¯ç”¨çš„æŒ‡æ ‡
                available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)
                for metric in available_key_metrics:
                    if metric in device_analysis['key_metrics_analysis']:
                        metric_data = device_analysis['key_metrics_analysis'][metric]
                        row[f'{metric}_change_percent'] = metric_data['change_percent']
                        row[f'{metric}_status'] = metric_data['status']
                        row[f'{metric}_control_mean'] = metric_data['control_mean']
                        row[f'{metric}_test_mean'] = metric_data['test_mean']

                # æ´å¯Ÿ
                row['insights'] = '; '.join(device_analysis['insights'])

                detailed_results.append(row)

        detailed_df = pd.DataFrame(detailed_results)
        detailed_path = os.path.join(output_dir, 'detailed_analysis_results.csv')
        detailed_df.to_csv(detailed_path, index=False, encoding='utf-8')
        logger.info(f"Saved detailed analysis results: {detailed_path}")

        # 2. æ³¨é‡Šæ‰ä¿å­˜å…¬å…±ç»´åº¦åˆ†æ
        # dimension_results = []
        #
        # for quality_level, quality_results in all_results.items():
        #     for device_model, device_analysis in quality_results.items():
        #         if 'performance_change_explanations' in device_analysis:
        #             explanations = device_analysis['performance_change_explanations']
        #             if 'dimension_differences' in explanations:
        #                 for dimension, dim_analysis in explanations['dimension_differences'].items():
        #                     dim_row = {
        #                         'quality_level': quality_level,
        #                         'device_model': device_model,
        #                         'dimension': dimension,
        #                         'has_significant_difference': dim_analysis.get('has_significant_difference', False),
        #                         'explanations': '; '.join(dim_analysis.get('explanations', []))
        #                     }
        #
        #                     # æ·»åŠ å·®å¼‚è¯¦æƒ…
        #                     if 'difference_details' in dim_analysis:
        #                         details = dim_analysis['difference_details']
        #                         if isinstance(details, dict) and 'control_mean' in details:
        #                             dim_row.update({
        #                                 'control_mean': details.get('control_mean'),
        #                                 'test_mean': details.get('test_mean'),
        #                                 'difference_percent': details.get('difference_percent')
        #                             })
        #
        #                     dimension_results.append(dim_row)
        #
        # if dimension_results:
        #     dimension_df = pd.DataFrame(dimension_results)
        #     dimension_path = os.path.join(output_dir, 'dimension_analysis_results.csv')
        #     dimension_df.to_csv(dimension_path, index=False, encoding='utf-8')
        #     logger.info(f"ä¿å­˜ç»´åº¦åˆ†æç»“æœ: {dimension_path}")

        # 3. ä¿å­˜ç»¼åˆåˆ†æ
        # è½¬æ¢numpyç±»å‹ä»¥é¿å…JSONåºåˆ—åŒ–é”™è¯¯
        def convert_numpy_types(obj):
            """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
            if isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif hasattr(obj, 'item') and hasattr(obj, 'dtype'):  # numpyæ ‡é‡
                return obj.item()
            else:
                return obj

        comprehensive_analysis_cleaned = convert_numpy_types(comprehensive_analysis)

        with open(os.path.join(output_dir, 'comprehensive_analysis.json'), 'w', encoding='utf-8') as f:
            json.dump(comprehensive_analysis_cleaned, f, indent=2, ensure_ascii=False)

    async def _split_data_by_unity_version(self, all_results: Dict, output_dir: str) -> None:
        """Split data by unity_inner version and save to separate CSV files"""

        logger.info("Starting to split data by Unity version...")

        def simplify_unity_version(version_str):
            """Extract version number from unity_inner string"""
            version_str = str(version_str)
            # Split by '-' and take the first 2 parts to get simplified version
            parts = version_str.split('-')
            return '-'.join(parts[:2])  # e.g., "2019-2"

        # Create unity_versions directory
        unity_versions_dir = os.path.join(output_dir, 'unity_versions')
        os.makedirs(unity_versions_dir, exist_ok=True)

        # Dictionary to store data by simplified version
        version_data = {}
        version_counts = {}

        # Collect all data and group by simplified unity version
        for quality_level, quality_results in all_results.items():
            for device_model, device_analysis in quality_results.items():
                control_data = device_analysis.get('control_data', pd.DataFrame())
                test_data = device_analysis.get('test_data', pd.DataFrame())

                # Process control data
                if not control_data.empty and 'unityversion_inner' in control_data.columns:
                    control_copy = control_data.copy()
                    control_copy['group_type'] = 'control'
                    control_copy['quality_level'] = quality_level
                    control_copy['device_model'] = device_model
                    control_copy['simplified_unity_version'] = control_copy['unityversion_inner'].apply(simplify_unity_version)

                    for _, row in control_copy.iterrows():
                        simplified_version = row['simplified_unity_version']
                        if simplified_version not in version_data:
                            version_data[simplified_version] = []
                            version_counts[simplified_version] = {'control': 0, 'test': 0, 'total': 0}

                        version_data[simplified_version].append(row.to_dict())
                        version_counts[simplified_version]['control'] += 1
                        version_counts[simplified_version]['total'] += 1

                # Process test data
                if not test_data.empty and 'unityversion_inner' in test_data.columns:
                    test_copy = test_data.copy()
                    test_copy['group_type'] = 'test'
                    test_copy['quality_level'] = quality_level
                    test_copy['device_model'] = device_model
                    test_copy['simplified_unity_version'] = test_copy['unityversion_inner'].apply(simplify_unity_version)

                    for _, row in test_copy.iterrows():
                        simplified_version = row['simplified_unity_version']
                        if simplified_version not in version_data:
                            version_data[simplified_version] = []
                            version_counts[simplified_version] = {'control': 0, 'test': 0, 'total': 0}

                        version_data[simplified_version].append(row.to_dict())
                        version_counts[simplified_version]['test'] += 1
                        version_counts[simplified_version]['total'] += 1

        # Save each version's data to separate CSV files
        for simplified_version, rows in version_data.items():
            if rows:  # Only save if there's data
                version_df = pd.DataFrame(rows)

                # Clean version name for filename (replace invalid characters)
                safe_version_name = simplified_version.replace('/', '_').replace('\\', '_').replace(':', '_')
                csv_filename = f"unity_version_{safe_version_name}.csv"
                csv_path = os.path.join(unity_versions_dir, csv_filename)

                # Sort by quality_level, device_model, group_type for better organization
                version_df = version_df.sort_values(['quality_level', 'device_model', 'group_type'])

                # Save to CSV
                version_df.to_csv(csv_path, index=False, encoding='utf-8')

                count_info = version_counts[simplified_version]
                logger.info(f"Saved Unity version {simplified_version}: {csv_path}")
                logger.info(f"  - Total records: {count_info['total']}")
                logger.info(f"  - Control: {count_info['control']}, Test: {count_info['test']}")

        # Create a summary file with version statistics
        summary_data = []
        for version, counts in version_counts.items():
            summary_data.append({
                'unity_version': version,
                'control_records': counts['control'],
                'test_records': counts['test'],
                'total_records': counts['total']
            })

        summary_df = pd.DataFrame(summary_data)
        summary_df = summary_df.sort_values('total_records', ascending=False)
        summary_path = os.path.join(unity_versions_dir, 'unity_versions_summary.csv')
        #summary_df.to_csv(summary_path, index=False, encoding='utf-8')

        logger.info(f"Created Unity versions summary: {summary_path}")
        logger.info(f"Total Unity versions found: {len(version_counts)}")
        logger.info(f"Unity versions data saved in directory: {unity_versions_dir}")

    def _generate_device_performance_report(
        self,
        all_results: Dict,
        output_dir: str
    ) -> str:
        """ç”Ÿæˆç±»ä¼¼ç”¨æˆ·æè¿°çš„è®¾å¤‡æ€§èƒ½åˆ†ææŠ¥å‘Š"""

        logger.info("ç”Ÿæˆè®¾å¤‡æ€§èƒ½åˆ†ææŠ¥å‘Š...")

        report_lines = []
        report_lines.append("# è®¾å¤‡æ€§èƒ½åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 60)
        report_lines.append("")

        # ï¼ˆä¸€ï¼‰å¡é¡¿ã€å¸§ç‡ã€åŠŸè€—æ•°æ®åˆ†æ
        report_lines.append("## ï¼ˆä¸€ï¼‰å¡é¡¿ã€å¸§ç‡ã€åŠŸè€—æ•°æ®")
        report_lines.append("")

        # æ”¶é›†æ‰€æœ‰è®¾å¤‡çš„æ€§èƒ½æ•°æ®
        device_performance_data = []
        device_counter = 1

        for quality_level, quality_results in all_results.items():
            for device_model, device_analysis in quality_results.items():
                if device_counter > self.config.max_devices_in_report:
                    break

                metrics = device_analysis.get('key_metrics_analysis', {})

                # æå–å…³é”®æ€§èƒ½æŒ‡æ ‡
                perf_data = {
                    'device_id': device_counter,
                    'device_model': device_model,
                    'quality_level': quality_level,
                    'bigrate': None,  # å¡é¡¿ç‡ - ä½¿ç”¨bigjankper10min
                    'fps': None,      # å¸§ç‡ - ä½¿ç”¨avgfps_unity
                    'current_avg': None,  # ç”µæµå¹³å‡å€¼
                    'gc_count': None, # GCæ¬¡æ•° - ä½¿ç”¨gt_50
                    'gc_alloc': None, # GCåˆ†é… - ä½¿ç”¨totalgcallocsize
                    'overall_status': device_analysis.get('overall_status', 'stable'),
                    'confidence': device_analysis.get('confidence_score', 0)
                }

                # å¡«å……æ•°æ®
                if 'bigjankper10min' in metrics:
                    perf_data['bigrate'] = metrics['bigjankper10min'].get('test_mean', 0)

                if 'avgfps_unity' in metrics:
                    perf_data['fps'] = metrics['avgfps_unity'].get('test_mean', 0)

                if 'current_avg' in metrics:
                    perf_data['current_avg'] = metrics['current_avg'].get('test_mean', 0)

                if 'gt_50' in metrics:
                    perf_data['gc_count'] = metrics['gt_50'].get('test_mean', 0)

                if 'totalgcallocsize' in metrics:
                    perf_data['gc_alloc'] = metrics['totalgcallocsize'].get('test_mean', 0)

                device_performance_data.append(perf_data)
                device_counter += 1

        # ç”Ÿæˆè®¾å¤‡æ€§èƒ½è¡¨æ ¼
        if device_performance_data:
            report_lines.append("è®°å½•äº†{}å°è®¾å¤‡çš„æ€§èƒ½æŒ‡æ ‡ï¼š".format(len(device_performance_data)))
            report_lines.append("")

            for data in device_performance_data:
                device_desc = f"**è®¾å¤‡{data['device_id']}** ({data['device_model']}, è´¨é‡ç­‰çº§{data['quality_level']})ï¼š"
                report_lines.append(device_desc)

                # å¡é¡¿ç‡åˆ†æ
                if data['bigrate'] is not None:
                    bigrate_pct = data['bigrate']
                    bigrate_status = "é«˜" if bigrate_pct > self.config.performance_threshold_poor else "æ­£å¸¸"
                    report_lines.append(f"- **å¡é¡¿ç‡**ï¼š{bigrate_pct:.2f}%ï¼Œå¡é¡¿{bigrate_status}")

                # å¸§ç‡åˆ†æ
                if data['fps'] is not None:
                    fps_val = data['fps']
                    fps_status = "ä½ï¼Œç”»é¢æ˜“ä¸æµç•…" if fps_val < 30 else "æ­£å¸¸"
                    report_lines.append(f"- **å¸§ç‡**ï¼š{fps_val:.2f} FPSï¼Œ{fps_status}")

                # ç”µæµåˆ†æ
                if data['current_avg'] is not None:
                    current_val = data['current_avg']
                    current_status = "åŠŸè€—è¡¨ç°è¾ƒå·®" if abs(current_val) > 1000 else "åŠŸè€—æ­£å¸¸"
                    report_lines.append(f"- **ç”µæµå¹³å‡å€¼**ï¼š{current_val:.2f}ï¼Œ{current_status}")

                # GCåˆ†æ
                if data['gc_count'] is not None:
                    gc_count_val = data['gc_count']
                    report_lines.append(f"- **GCæ¬¡æ•°**ï¼š{gc_count_val:.2f}")

                if data['gc_alloc'] is not None:
                    gc_alloc_val = data['gc_alloc']
                    gc_status = "è¾ƒé«˜" if gc_alloc_val > self.config.gc_threshold_high else "æ­£å¸¸"
                    report_lines.append(f"- **GCåˆ†é…**ï¼š{gc_alloc_val:.2f}ï¼Œ{gc_status}")

                # æ•´ä½“çŠ¶æ€
                status_emoji = {"degraded": "ğŸ”´", "improved": "ğŸŸ¢", "stable": "ğŸ”µ"}
                status_desc = {"degraded": "æ€§èƒ½æ¶åŒ–", "improved": "æ€§èƒ½æ”¹å–„", "stable": "æ€§èƒ½ç¨³å®š"}
                emoji = status_emoji.get(data['overall_status'], "âšª")
                desc = status_desc.get(data['overall_status'], "æœªçŸ¥")
                report_lines.append(f"- **æ•´ä½“çŠ¶æ€**ï¼š{emoji} {desc} (ç½®ä¿¡åº¦: {data['confidence']:.2f})")

                report_lines.append("")

        # ï¼ˆäºŒï¼‰å†…å­˜æ•°æ®åˆ†æ
        report_lines.append("## ï¼ˆäºŒï¼‰å†…å­˜æ•°æ®åˆ†æ")
        report_lines.append("")

        if self.config.memory_analysis_enabled:
            memory_analysis = self._analyze_memory_by_categories(all_results)

            report_lines.append("æŒ‰å†…å­˜åŒºé—´å’ŒUnityç‰ˆæœ¬ä½æ•°åˆ†ætotalpsså†…å­˜å ç”¨ï¼š")
            report_lines.append("")

            for mem_category, mem_data in memory_analysis.items():
                report_lines.append(f"**{mem_category} å†…å­˜åŒºé—´**ï¼š")

                for unity_digital, pss_data in mem_data.items():
                    avg_pss = pss_data.get('avg_pss', 0)
                    device_count = pss_data.get('device_count', 0)
                    pss_status = "å†…å­˜å ç”¨è¾ƒé«˜" if avg_pss > self.config.pss_threshold_high else "å†…å­˜å ç”¨æ­£å¸¸"

                    report_lines.append(f"- Unity{unity_digital}ä½ç‰ˆæœ¬ï¼štotalpsså¹³å‡ {avg_pss:.2f}ï¼Œ"
                                      f"{pss_status} (åŸºäº{device_count}ä¸ªè®¾å¤‡)")

                report_lines.append("")

        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        report_lines.append("## æ€§èƒ½ä¼˜åŒ–å»ºè®®")
        report_lines.append("")

        # åˆ†ææœ€éœ€è¦å…³æ³¨çš„é—®é¢˜
        high_priority_issues = []
        optimization_suggestions = []

        for data in device_performance_data:
            if data['overall_status'] == 'degraded':
                issues = []
                if data['bigrate'] and data['bigrate'] > self.config.performance_threshold_poor:
                    issues.append("å¡é¡¿ç‡é«˜")
                if data['fps'] and data['fps'] < 30:
                    issues.append("å¸§ç‡ä½")
                if data['current_avg'] and abs(data['current_avg']) > 1000:
                    issues.append("åŠŸè€—å¼‚å¸¸")

                if issues:
                    high_priority_issues.append(f"è®¾å¤‡{data['device_id']}ï¼š{', '.join(issues)}")

        if high_priority_issues:
            report_lines.append("ğŸš¨ **éœ€è¦ä¼˜å…ˆå¤„ç†çš„é—®é¢˜**ï¼š")
            for issue in high_priority_issues[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                report_lines.append(f"- {issue}")
            report_lines.append("")

        # é€šç”¨ä¼˜åŒ–å»ºè®®
        report_lines.append("ğŸ’¡ **é€šç”¨ä¼˜åŒ–å»ºè®®**ï¼š")
        report_lines.append("- é’ˆå¯¹å¡é¡¿ç‡é«˜çš„è®¾å¤‡ï¼Œæ£€æŸ¥æ¸²æŸ“ç®¡çº¿å’ŒGCè§¦å‘é¢‘ç‡")
        report_lines.append("- é’ˆå¯¹å¸§ç‡ä½çš„è®¾å¤‡ï¼Œè€ƒè™‘é™ä½æ¸²æŸ“è´¨é‡æˆ–ä¼˜åŒ–GPUä½¿ç”¨")
        report_lines.append("- é’ˆå¯¹åŠŸè€—å¼‚å¸¸çš„è®¾å¤‡ï¼Œåˆ†æCPUä½¿ç”¨æ¨¡å¼å’Œå……ç”µçŠ¶æ€å½±å“")
        report_lines.append("- å®šæœŸç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œç‰¹åˆ«å…³æ³¨GCåˆ†é…é¢‘ç‡è¾ƒé«˜çš„è®¾å¤‡")
        report_lines.append("")

        # æ•°æ®è´¨é‡è¯´æ˜
        report_lines.append("## æ•°æ®è¯´æ˜")
        report_lines.append("")
        report_lines.append("æœ¬æŠ¥å‘ŠåŸºäºA/Bæµ‹è¯•æ•°æ®åˆ†æç”Ÿæˆï¼Œç”¨äºè¯„ä¼°è®¾å¤‡è¿è¡Œæ€§èƒ½è¡¨ç°ã€")
        report_lines.append("å†…å­˜åˆ©ç”¨æ•ˆç‡ç­‰ï¼Œå¯è¾…åŠ©ä¼˜åŒ–è®¾å¤‡ä½“éªŒã€æ’æŸ¥æ€§èƒ½é—®é¢˜ã€‚")
        report_lines.append("")
        report_lines.append(f"- åˆ†æè®¾å¤‡æ•°ï¼š{len(device_performance_data)}")
        report_lines.append(f"- æ€§èƒ½æ¶åŒ–è®¾å¤‡ï¼š{len([d for d in device_performance_data if d['overall_status'] == 'degraded'])}")
        report_lines.append(f"- æ€§èƒ½æ”¹å–„è®¾å¤‡ï¼š{len([d for d in device_performance_data if d['overall_status'] == 'improved'])}")
        report_lines.append(f"- æ€§èƒ½ç¨³å®šè®¾å¤‡ï¼š{len([d for d in device_performance_data if d['overall_status'] == 'stable'])}")

        # ä¿å­˜æŠ¥å‘Š
        report_content = "\n".join(report_lines)
        report_path = os.path.join(output_dir, 'device_performance_report.md')

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)

        logger.info(f"è®¾å¤‡æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return report_content

    def _analyze_memory_by_categories(self, all_results: Dict) -> Dict:
        """æŒ‰å†…å­˜ç±»åˆ«å’ŒUnityç‰ˆæœ¬åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µ"""

        memory_analysis = {}

        for quality_level, quality_results in all_results.items():
            for device_model, device_analysis in quality_results.items():

                # æ³¨é‡Šæ‰è·å–ç»´åº¦åˆ†æä¸­çš„å†…å­˜å’ŒUnityç‰ˆæœ¬ä¿¡æ¯
                # explanations = device_analysis.get('performance_change_explanations', {})
                # dimension_diffs = explanations.get('dimension_differences', {})

                # æå–å†…å­˜ç±»åˆ«ä¿¡æ¯
                memory_category = "æœªçŸ¥"
                unity_digital = "æœªçŸ¥"

                # æ³¨é‡Šæ‰ä»ç»´åº¦åˆ†æä¸­æå–å†…å­˜å’ŒUnityç‰ˆæœ¬ä¿¡æ¯
                # if 'memory_category' in dimension_diffs:
                #     mem_details = dimension_diffs['memory_category'].get('difference_details', {})
                #     test_dist = mem_details.get('test_distribution', {})
                #     if test_dist:
                #         # å–å æ¯”æœ€é«˜çš„å†…å­˜ç±»åˆ«
                #         memory_category = max(test_dist.items(), key=lambda x: x[1])[0]
                #
                # if 'unity_digital' in dimension_diffs:
                #     unity_details = dimension_diffs['unity_digital'].get('difference_details', {})
                #     test_dist = unity_details.get('test_distribution', {})
                #     if test_dist:
                #         # å–å æ¯”æœ€é«˜çš„Unityç‰ˆæœ¬ä½æ•°
                #         unity_digital = max(test_dist.items(), key=lambda x: x[1])[0]

                # ç›´æ¥ä»è®¾å¤‡æ¨¡å‹åç§°ä¸­å°è¯•æå–å†…å­˜å’ŒUnityç‰ˆæœ¬ä¿¡æ¯
                device_model_str = str(device_model)
                if '0G-2G' in device_model_str:
                    memory_category = '0G-2G'
                elif '2G-6G' in device_model_str:
                    memory_category = '2G-6G'
                elif '6G' in device_model_str:
                    memory_category = '6G'

                if '32bit' in device_model_str:
                    unity_digital = '32bit'
                elif '64bit' in device_model_str:
                    unity_digital = '64bit'

                # è·å–totalpssæ•°æ®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼Œä½¿ç”¨battleendtotalpssä½œä¸ºè¿‘ä¼¼ï¼‰
                metrics = device_analysis.get('key_metrics_analysis', {})
                totalpss = 0
                if 'battleendtotalpss' in metrics:
                    totalpss = metrics['battleendtotalpss'].get('test_mean', 0)

                # ç»„ç»‡æ•°æ®ç»“æ„
                if memory_category not in memory_analysis:
                    memory_analysis[memory_category] = {}

                if unity_digital not in memory_analysis[memory_category]:
                    memory_analysis[memory_category][unity_digital] = {
                        'total_pss': 0,
                        'device_count': 0
                    }

                memory_analysis[memory_category][unity_digital]['total_pss'] += totalpss
                memory_analysis[memory_category][unity_digital]['device_count'] += 1

        # è®¡ç®—å¹³å‡å€¼
        for mem_category in memory_analysis:
            for unity_digital in memory_analysis[mem_category]:
                data = memory_analysis[mem_category][unity_digital]
                if data['device_count'] > 0:
                    data['avg_pss'] = data['total_pss'] / data['device_count']
                else:
                    data['avg_pss'] = 0

        return memory_analysis

    def _generate_final_summary(
        self,
        csv_file_path: str,
        all_results: Dict,
        comprehensive_analysis: Dict,
        quality_splits: Dict,
        output_dir: str,
        degradation_threshold: float,
        confidence_alpha: float
    ) -> str:
        """ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Š"""

        summary = "å…³é”®æŒ‡æ ‡å±‚çº§åˆ†ææŠ¥å‘Š\n"
        summary += "=" * 60 + "\n\n"

        available_common_columns = getattr(self, 'available_common_columns', COMMON_COLUMNS)
        available_key_metrics = getattr(self, 'available_key_metrics', KEY_PERFORMANCE_METRICS)

        summary += f"**è¾“å…¥æ•°æ®ä¿¡æ¯:**\n"
        summary += f"- æ•°æ®æ–‡ä»¶: {csv_file_path}\n"
        summary += f"- åˆ†æç»´åº¦: {len(available_common_columns)} ä¸ªå…¬å…±ç»´åº¦\n"
        summary += f"- å…³é”®æŒ‡æ ‡: {len(available_key_metrics)}/{len(KEY_PERFORMANCE_METRICS)} ä¸ªæ ¸å¿ƒæŒ‡æ ‡ (å¯ç”¨/æ€»è®¡)\n"
        summary += f"- å¯ç”¨æŒ‡æ ‡: {', '.join(available_key_metrics)}\n"
        if len(available_key_metrics) < len(KEY_PERFORMANCE_METRICS):
            missing_metrics = [m for m in KEY_PERFORMANCE_METRICS if m not in available_key_metrics]
            summary += f"- ç¼ºå¤±æŒ‡æ ‡: {', '.join(missing_metrics)}\n"
        summary += f"- æ¶åŒ–é˜ˆå€¼: {degradation_threshold}%\n"
        summary += f"- ç½®ä¿¡åº¦å‚æ•°: {confidence_alpha}\n\n"

        summary += f"**æ•°æ®åˆ†å¸ƒæ¦‚è§ˆ:**\n"
        total_quality_levels = len(quality_splits)
        total_devices = sum(len(quality_results) for quality_results in all_results.values())

        summary += f"- è´¨é‡ç­‰çº§æ•°é‡: {total_quality_levels} ä¸ª\n"
        summary += f"- åˆ†æè®¾å¤‡å‹å·æ€»æ•°: {total_devices} ä¸ª\n"

        for quality_level, quality_data in quality_splits.items():
            device_count = len(all_results.get(quality_level, {}))
            summary += f"  * è´¨é‡ç­‰çº§ {quality_level}: {device_count} ä¸ªè®¾å¤‡å‹å· "
            summary += f"(Control: {quality_data['control_records']}, Test: {quality_data['test_records']})\n"

        summary += "\n**è´¨é‡ç­‰çº§åˆ†æç»“æœ:**\n"
        for quality_level, quality_summary in comprehensive_analysis['quality_level_summary'].items():
            summary += f"ğŸ“Š **è´¨é‡ç­‰çº§ {quality_level}**:\n"
            summary += f"   - æ€»è®¾å¤‡æ•°: {quality_summary['total_devices']}\n"
            summary += f"   - ğŸ”´ æ¶åŒ–è®¾å¤‡: {quality_summary['degraded_devices']}\n"
            summary += f"   - ğŸŸ¢ æ”¹å–„è®¾å¤‡: {quality_summary['improved_devices']}\n"
            summary += f"   - ğŸ”µ ç¨³å®šè®¾å¤‡: {quality_summary['stable_devices']}\n"
            summary += f"   - å¹³å‡ç½®ä¿¡åº¦: {quality_summary['avg_confidence']:.1f}\n\n"

        summary += f"**æ ¸å¿ƒå‘ç°:**\n"

        # Top æ¶åŒ–è®¾å¤‡
        if comprehensive_analysis['device_ranking']:
            summary += f"ğŸ”´ **æ€§èƒ½æ¶åŒ–æœ€ä¸¥é‡çš„è®¾å¤‡:**\n"
            for i, device in enumerate(comprehensive_analysis['device_ranking'][:5], 1):
                summary += f"{i}. {device['device_model']} (è´¨é‡{device['quality_level']}) - "
                summary += f"æ¶åŒ–ç¨‹åº¦: {device['degradation_score']:.1f}%, ç½®ä¿¡åº¦: {device['confidence_score']:.1f}\n"
            summary += "\n"

        # å…³é”®æŒ‡æ ‡æ•´ä½“è¡¨ç° - åªåˆ†æå¯ç”¨çš„æŒ‡æ ‡
        summary += f"ğŸ“ˆ **å…³é”®æŒ‡æ ‡æ•´ä½“è¡¨ç°:**\n"

        for metric in available_key_metrics:
            metric_changes = []

            for quality_level, quality_results in all_results.items():
                for device_model, device_analysis in quality_results.items():
                    if metric in device_analysis['key_metrics_analysis']:
                        change = device_analysis['key_metrics_analysis'][metric]['change_percent']
                        if not np.isnan(change):
                            metric_changes.append(change)

            if metric_changes:
                avg_change = np.mean(metric_changes)
                degraded_count = sum(1 for change in metric_changes if abs(change) >= degradation_threshold and
                                   not self._is_metric_improvement(metric, change))

                summary += f"- {metric}: å¹³å‡å˜åŒ– {avg_change:.1f}%, {degraded_count} ä¸ªè®¾å¤‡æ¶åŒ–\n"

        summary += f"\n**ç»´åº¦æ´å¯Ÿ:**\n"

        # ç»Ÿè®¡å„ç»´åº¦çš„å…³é”®å‘ç°
        dimension_insights_count = {}
        for quality_level, quality_results in all_results.items():
            for device_model, device_analysis in quality_results.items():
                if 'performance_change_explanations' in device_analysis:
                    explanations = device_analysis['performance_change_explanations']
                    if 'dimension_differences' in explanations:
                        for dimension, dim_analysis in explanations['dimension_differences'].items():
                            if dim_analysis.get('has_significant_difference', False):
                                dimension_insights_count[dimension] = dimension_insights_count.get(dimension, 0) + 1

        for dimension, count in sorted(dimension_insights_count.items(), key=lambda x: x[1], reverse=True):
            summary += f"- {dimension}: {count} ä¸ªè®¾å¤‡å‘ç°æ˜¾è‘—å·®å¼‚\n"

        summary += f"\n**ç”Ÿæˆæ–‡ä»¶:**\n"
        summary += f"ğŸ“Š è´¨é‡ç­‰çº§æ¦‚è§ˆå›¾: {output_dir}/quality_level_performance_overview.png\n"
        summary += f"ğŸ”¥ è®¾å¤‡æŒ‡æ ‡çƒ­åŠ›å›¾: {output_dir}/device_metrics_heatmap.png\n"
        summary += f"ğŸ“ˆ æ¶åŒ–è®¾å¤‡æ’å: {output_dir}/degraded_devices_ranking.png\n"
        summary += f"ğŸ“‹ è¯¦ç»†åˆ†æç»“æœ: {output_dir}/detailed_analysis_results.csv\n"
        summary += f"ğŸ“‹ ç»´åº¦åˆ†æç»“æœ: {output_dir}/dimension_analysis_results.csv\n"
        summary += f"ğŸ“ ç»¼åˆåˆ†ææ•°æ®: {output_dir}/comprehensive_analysis.json\n"

        # æ·»åŠ Unityç‰ˆæœ¬åˆ†å‰²åçš„CSVæ–‡ä»¶è·¯å¾„ä¿¡æ¯
        unity_versions_dir = os.path.join(output_dir, 'unity_versions')
        if os.path.exists(unity_versions_dir):
            summary += f"\n**Unityç‰ˆæœ¬åˆ†å‰²æ•°æ®æ–‡ä»¶:**\n"
            summary += f"ğŸ“ Unityç‰ˆæœ¬æ•°æ®ç›®å½•: {unity_versions_dir}\n"

            # åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆçš„Unityç‰ˆæœ¬CSVæ–‡ä»¶
            try:
                unity_csv_files = [f for f in os.listdir(unity_versions_dir) if f.endswith('.csv') and f.startswith('unity_version_')]
                unity_csv_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº

                if unity_csv_files:
                    summary += f"ğŸ”¢ ç”Ÿæˆçš„Unityç‰ˆæœ¬æ•°æ®æ–‡ä»¶å…± {len(unity_csv_files)} ä¸ª:\n"
                    for csv_file in unity_csv_files:
                        file_path = os.path.join(unity_versions_dir, csv_file)
                        # ä»æ–‡ä»¶åæå–ç‰ˆæœ¬ä¿¡æ¯
                        version_name = csv_file.replace('unity_version_', '').replace('.csv', '').replace('_', '-')
                        summary += f"   â€¢ {version_name}: {file_path}\n"

                    # æ·»åŠ æ‘˜è¦æ–‡ä»¶
                    summary_file = os.path.join(unity_versions_dir, 'unity_versions_summary.csv')
                    if os.path.exists(summary_file):
                        summary += f"ğŸ“Š Unityç‰ˆæœ¬ç»Ÿè®¡æ‘˜è¦: {summary_file}\n"
                else:
                    summary += f"âš ï¸ æœªæ‰¾åˆ°Unityç‰ˆæœ¬åˆ†å‰²çš„CSVæ–‡ä»¶\n"

            except Exception as e:
                summary += f"âš ï¸ è¯»å–Unityç‰ˆæœ¬ç›®å½•æ—¶å‡ºé”™: {e}\n"

        for dimension in ['ischarged', 'cpumodel', 'refreshrate', 'time_flag', 'memory_category', 'unity_digital']:
            if dimension in available_common_columns:
                summary += f"ğŸ“Š {dimension} å½±å“åˆ†æ: {output_dir}/{dimension}_impact_analysis.png\n"

        summary += f"\n**å…³é”®å»ºè®®:**\n"
        summary += f"1. é‡ç‚¹å…³æ³¨æ€§èƒ½æ¶åŒ–ä¸¥é‡ä¸”ç½®ä¿¡åº¦é«˜çš„è®¾å¤‡å‹å·\n"
        summary += f"2. åˆ†æä¸åŒè´¨é‡ç­‰çº§è®¾å¤‡çš„æ€§èƒ½å·®å¼‚æ¨¡å¼\n"
        summary += f"3. è€ƒè™‘å……ç”µçŠ¶æ€ã€CPUå‹å·ç­‰ç»´åº¦å¯¹æ€§èƒ½çš„å½±å“\n"
        summary += f"4. é’ˆå¯¹ä¸åŒè´¨é‡ç­‰çº§åˆ¶å®šå·®å¼‚åŒ–çš„ä¼˜åŒ–ç­–ç•¥\n"
        summary += f"5. å…³æ³¨æ ·æœ¬é‡å……è¶³ä¸”å…·æœ‰ç»Ÿè®¡æ„ä¹‰çš„å¯¹æ¯”ç»“æœ\n"

        return summary
