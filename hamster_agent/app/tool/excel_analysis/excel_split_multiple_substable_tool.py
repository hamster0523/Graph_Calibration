import json
import os
from typing import Dict, List

import pandas as pd

from app.config import config
from app.exceptions import ToolError
from app.logger import logger
from app.tool.base import BaseTool, ToolResult


class ExcelSplitMultipleSubtableTool(BaseTool):

    name: str = "excel_split_multiple_subtable_tool"
    description: str = (
        "åŸºäºJSONå…ƒæ•°æ®æ–‡ä»¶ä¸­çš„åˆ—åˆ†ç±»ä¿¡æ¯å¯¹CSVæ–‡ä»¶è¿›è¡Œä¸¤çº§æ‹†åˆ†çš„å·¥å…·ã€‚é¦–å…ˆæŒ‰ç…§æ€§èƒ½æŒ‡æ ‡çš„å¤§åˆ†ç±»ï¼ˆå†…å­˜ã€åŠŸè€—ã€å…¶ä»–ã€å¹³å‡å¸§ç‡ã€å¡é¡¿ã€æ¸©åº¦ï¼‰å°†æ•°æ®æ‹†åˆ†ä¸º6ä¸ªä¸»è¦ç±»åˆ«çš„å­è¡¨ï¼Œç„¶ååœ¨æ¯ä¸ªå¤§åˆ†ç±»å†…éƒ¨å†æŒ‰ç…§è®¾å¤‡è´¨é‡ç­‰çº§ï¼ˆå¦‚realpicturequalityå­—æ®µçš„ä¸åŒå€¼ï¼‰è¿›è¡Œè¿›ä¸€æ­¥ç»†åˆ†ï¼Œæœ€ç»ˆç”Ÿæˆåˆ†å±‚æ¬¡çš„å¤šä¸ªå­è¡¨æ–‡ä»¶ï¼Œä¾¿äºè¿›è¡Œåˆ†ç±»åˆ«å’Œåˆ†è´¨é‡ç­‰çº§çš„æ·±å…¥åˆ†æã€‚"
    )
    parameters: dict = {
        "type": "object",
        "properties": {
            "csv_file_path": {
                "type": "string",
                "description": "éœ€è¦æ‹†åˆ†çš„å·²æ¸…ç†CSVæ–‡ä»¶çš„è·¯å¾„",
            },
            "common_columns": {
                "type": "array",
                "items": {"type": "string"},
                "description": "éœ€è¦åŒ…å«åœ¨æ¯ä¸ªå­è¡¨ä¸­çš„å…¬å…±åˆ—ååˆ—è¡¨ã€‚è¿™äº›é€šå¸¸æ˜¯åŒ…å«å…ƒæ•°æ®æˆ–ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åˆ—ï¼Œå¦‚param_valueã€devicemodelã€platformã€hardware_modelã€OS_versionæˆ–å…¶ä»–æ ‡è¯†ç¬¦ï¼Œè¿™äº›åˆ—ä¸ºæ‰€æœ‰æ€§èƒ½æŒ‡æ ‡æä¾›ä¸Šä¸‹æ–‡ï¼Œåº”è¯¥å­˜åœ¨äºæ¯ä¸ªå­è¡¨ä¸­ä»¥ä¾¿è¿›è¡Œé€‚å½“çš„åˆ†æã€‚",
                "default": ["param_value", "devicemodel"],
            },
            "quality_column": {
                "type": "string",
                "description": "ç”¨äºè®¾å¤‡è´¨é‡åˆ†ç±»çš„åˆ—åï¼ˆä¾‹å¦‚realpicturequalityï¼‰ï¼Œè¯¥åˆ—çš„ä¸åŒå€¼å°†ä½œä¸ºç¬¬äºŒçº§æ‹†åˆ†çš„ä¾æ®",
                "default": "realpicturequality",
            },
        },
        "required": ["csv_file_path"],
    }

    async def execute(
        self,
        csv_file_path: str,
        common_columns: List[str],
        quality_column: str = "realpicturequality",
    ) -> ToolResult:
        """
        Split a CSV file into category-based subtables and further split by device quality levels.

        The 6 categories are:
        - å†…å­˜ (Memory)
        - åŠŸè€— (Power)
        - å…¶ä»– (Others)
        - å¹³å‡å¸§ç‡ (FPS)
        - å¡é¡¿ (Lag)
        - æ¸©åº¦ (Temperature)

        Each category will be further split by device quality levels (e.g., realpicturequality).

        Args:
            csv_file_path: Path to the CSV file to be split
            json_metadata_path: Path to JSON file containing column metadata with Classification field
            common_columns: List of columns to include in all subtables
            quality_column: Column name for device quality classification
            output_dir: Optional output directory for subtables

        Returns:
            ToolResult with summary of created subtables
        """
        try:
            # Validate input files
            if not os.path.exists(csv_file_path):
                raise ToolError(f"CSV file not found: {csv_file_path}")

            if config.run_flow_config.meta_data_name is None:
                raise ToolError(
                    f"JSON metadata file name is not located in workspace root directory"
                )

            json_metadata_path = os.path.join(
                config.workspace_root, config.run_flow_config.meta_data_name
            )

            # Read JSON metadata
            try:
                with open(json_metadata_path, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
                logger.info(
                    f"Successfully loaded JSON metadata from {json_metadata_path}"
                )
            except json.JSONDecodeError as e:
                raise ToolError(f"Invalid JSON format in metadata file: {str(e)}")
            except Exception as e:
                raise ToolError(f"Error reading JSON metadata file: {str(e)}")

            # Read CSV file
            try:
                df = pd.read_csv(csv_file_path)
                logger.info(f"Successfully loaded CSV file with shape: {df.shape}")
            except Exception as e:
                raise ToolError(f"Error reading CSV file: {str(e)}")

            # Validate quality column exists
            if quality_column not in df.columns:
                raise ToolError(
                    f"Quality column '{quality_column}' not found in CSV file"
                )

            # Define the 6 classification categories
            categories = {
                "å†…å­˜": "memory",
                "åŠŸè€—": "power",
                "å…¶ä»–": "others",
                "å¹³å‡å¸§ç‡": "fps",
                "å¡é¡¿": "lag",
                "æ¸©åº¦": "temperature",
            }

            # Create field to classification mapping
            field_classification_map = {}

            # Handle both list and single object JSON formats
            if isinstance(metadata, list):
                metadata_items = metadata
            else:
                metadata_items = [metadata]

            for item in metadata_items:
                if (
                    isinstance(item, dict)
                    and "Field" in item
                    and "Classification" in item
                ):
                    field_name = item["Field"]
                    classification = item["Classification"]
                    field_classification_map[field_name] = classification

            logger.info(
                f"Created field classification mapping for {len(field_classification_map)} fields"
            )

            # Validate common columns exist in CSV
            missing_common_cols = [
                col for col in common_columns if col not in df.columns
            ]
            if missing_common_cols:
                logger.warning(
                    f"Common columns not found in CSV: {missing_common_cols}"
                )
                # Filter to only existing common columns
                common_columns = [col for col in common_columns if col in df.columns]

            if not common_columns:
                raise ToolError("No valid common columns found in the CSV file")

            # Get unique quality levels
            quality_levels = sorted(df[quality_column].unique())
            logger.info(f"Found {len(quality_levels)} quality levels: {quality_levels}")

            # Prepare output directory
            csv_dir = os.path.dirname(csv_file_path)
            output_dir = os.path.join(csv_dir, "splitsubtable_multiple")

            os.makedirs(output_dir, exist_ok=True)
            logger.info(f"Output directory prepared: {output_dir}")

            # Group columns by classification
            columns_by_category = {category: [] for category in categories.keys()}
            unclassified_columns = []

            for col in df.columns:
                if col in common_columns or col == quality_column:
                    continue  # Skip common columns and quality column as they'll be handled separately

                classification = field_classification_map.get(col)
                if classification in categories:
                    columns_by_category[classification].append(col)
                else:
                    unclassified_columns.append(col)
                    # Add unclassified columns to "å…¶ä»–" category
                    columns_by_category["å…¶ä»–"].append(col)

            if unclassified_columns:
                logger.info(
                    f"Unclassified columns added to 'å…¶ä»–' category: {unclassified_columns}"
                )

            # Create subtables
            created_subtables = {}
            total_files_created = 0

            for category_cn, category_en in categories.items():
                category_columns = columns_by_category[category_cn]

                if not category_columns:
                    logger.info(
                        f"No columns found for category '{category_cn}', skipping subtable creation"
                    )
                    continue

                # Create category directory
                category_dir = os.path.join(output_dir, f"{category_en}_subtables")
                os.makedirs(category_dir, exist_ok=True)
                logger.info(f"Created category directory: {category_dir}")

                # Prepare columns for this category (common columns + category-specific columns, without quality column for final output)
                subtable_columns_with_quality = (
                    common_columns + [quality_column] + category_columns
                )
                subtable_columns_final = common_columns + category_columns

                # Create base subtable with quality column for splitting
                base_subtable = df[subtable_columns_with_quality].copy()

                # Split by quality levels
                quality_subtables = {}
                for quality_level in quality_levels:
                    # Filter data for this quality level
                    quality_data = base_subtable[
                        base_subtable[quality_column] == quality_level
                    ].copy()

                    if quality_data.empty:
                        logger.warning(
                            f"No data found for quality level '{quality_level}' in category '{category_cn}'"
                        )
                        continue

                    # Remove quality column from final output
                    quality_data = quality_data[subtable_columns_final].copy()

                    # Save quality-specific subtable
                    quality_filename = f"{category_en}_{quality_level}_subtable.csv"
                    quality_path = os.path.join(category_dir, quality_filename)

                    try:
                        quality_data.to_csv(quality_path, index=False, encoding="utf-8")
                        logger.info(
                            f"Created quality subtable: {quality_path} with shape {quality_data.shape}"
                        )

                        quality_subtables[quality_level] = {
                            "filename": quality_filename,
                            "path": quality_path,
                            "shape": quality_data.shape,
                            "columns": subtable_columns_final,
                            "data_count": len(quality_data),
                        }
                        total_files_created += 1

                    except Exception as e:
                        logger.error(
                            f"Error saving quality subtable for category {category_cn}, quality {quality_level}: {str(e)}"
                        )
                        raise ToolError(
                            f"Failed to save quality subtable for category {category_cn}, quality {quality_level}: {str(e)}"
                        )

                # Store category information
                if quality_subtables:
                    created_subtables[category_cn] = {
                        "category_english": category_en,
                        "category_dir": category_dir,
                        "category_columns": category_columns,
                        "quality_subtables": quality_subtables,
                        "total_quality_levels": len(quality_subtables),
                        "total_columns": len(subtable_columns_final),
                        "feature_columns_count": len(category_columns),
                        "common_columns_count": len(common_columns),
                    }

            # Generate summary report
            summary = self._generate_summary_report(
                csv_file_path,
                json_metadata_path,
                output_dir,
                created_subtables,
                common_columns,
                field_classification_map,
                quality_column,
                quality_levels,
                total_files_created,
            )

            return ToolResult(output=summary)

        except ToolError as e:
            return ToolResult(error=str(e))
        except Exception as e:
            error_msg = (
                f"Unexpected error in excel split multiple subtable tool: {str(e)}"
            )
            logger.error(error_msg)
            return ToolResult(error=error_msg)

    def _generate_summary_report(
        self,
        csv_file_path: str,
        json_metadata_path: str,
        output_dir: str,
        created_subtables: Dict,
        common_columns: List[str],
        field_classification_map: Dict,
        quality_column: str,
        quality_levels: List,
        total_files_created: int,
    ) -> str:
        """Generate a comprehensive summary report of the splitting operation."""

        summary = "Excelå¤šçº§åˆ†è¡¨å·¥å…·æ‰§è¡Œå®Œæˆ\n"
        summary += "=" * 60 + "\n\n"

        summary += f"**è¾“å…¥æ–‡ä»¶ä¿¡æ¯:**\n"
        summary += f"- CSVæ–‡ä»¶: {csv_file_path}\n"
        summary += f"- JSONå…ƒæ•°æ®æ–‡ä»¶: {json_metadata_path}\n"
        summary += f"- è¾“å‡ºç›®å½•: {output_dir}\n"
        summary += f"- è®¾å¤‡è´¨é‡åˆ†ç±»åˆ—: {quality_column}\n\n"

        summary += f"**è®¾å¤‡è´¨é‡åˆ†çº§:**\n"
        summary += f"- å‘ç° {len(quality_levels)} ä¸ªè´¨é‡ç­‰çº§: {', '.join(map(str, quality_levels))}\n\n"

        summary += f"**å…¬å…±åˆ—é…ç½®:**\n"
        summary += f"- å…¬å…±åˆ—æ•°é‡: {len(common_columns)}\n"
        summary += f"- å…¬å…±åˆ—åˆ—è¡¨: {', '.join(common_columns)}\n\n"

        summary += f"**åˆ†ç±»ç»Ÿè®¡:**\n"
        summary += f"- æ€»å­—æ®µæ•°é‡: {len(field_classification_map)}\n"
        summary += f"- æˆåŠŸåˆ›å»ºåˆ†ç±»ç›®å½•æ•°é‡: {len(created_subtables)}\n"
        summary += f"- æ€»å…±ç”Ÿæˆå­è¡¨æ–‡ä»¶æ•°é‡: {total_files_created}\n\n"

        summary += f"**ç”Ÿæˆçš„åˆ†ç±»å­è¡¨è¯¦æƒ…:**\n"
        for category_cn, info in created_subtables.items():
            summary += f"\nğŸ“Š **{category_cn}åˆ†ç±» ({info['category_english']})**\n"
            summary += f"   - åˆ†ç±»ç›®å½•: {info['category_dir']}\n"
            summary += f"   - ç‰¹å¾åˆ—æ•°é‡: {info['feature_columns_count']}\n"
            summary += f"   - å…¬å…±åˆ—æ•°é‡: {info['common_columns_count']}\n"
            summary += f"   - æ€»åˆ—æ•°: {info['total_columns']}\n"
            summary += f"   - è´¨é‡ç­‰çº§å­è¡¨æ•°é‡: {info['total_quality_levels']}\n"

            if info["category_columns"]:
                summary += f"   - ç‰¹å¾åˆ—åˆ—è¡¨: {', '.join(info['category_columns'])}\n"

            summary += f"   - è´¨é‡ç­‰çº§å­è¡¨è¯¦æƒ…:\n"
            for quality_level, quality_info in info["quality_subtables"].items():
                summary += f"     * {quality_level}: {quality_info['filename']} ({quality_info['shape'][0]} è¡Œ Ã— {quality_info['shape'][1]} åˆ—)\n"

        summary += f"\n**å­—æ®µåˆ†ç±»æ˜ å°„:**\n"
        categories = ["å†…å­˜", "åŠŸè€—", "å…¶ä»–", "å¹³å‡å¸§ç‡", "å¡é¡¿", "æ¸©åº¦"]
        for category in categories:
            category_fields = [
                field
                for field, classification in field_classification_map.items()
                if classification == category
            ]
            summary += f"- {category}: {len(category_fields)} ä¸ªå­—æ®µ\n"
            if category_fields:
                # Show first few fields, truncate if too many
                displayed_fields = category_fields[:5]
                if len(category_fields) > 5:
                    displayed_fields.append(f"... ç­‰{len(category_fields)-5}ä¸ªæ›´å¤šå­—æ®µ")
                summary += f"  â””â”€ {', '.join(displayed_fields)}\n"

        summary += f"\n**æ–‡ä»¶ç»“æ„:**\n"
        summary += f"è¾“å‡ºç›®å½•ç»“æ„:\n"
        summary += f"{output_dir}/\n"
        for category_cn, info in created_subtables.items():
            category_en = info["category_english"]
            summary += f"â”œâ”€â”€ {category_en}_subtables/\n"
            for quality_level in info["quality_subtables"].keys():
                summary += f"â”‚   â”œâ”€â”€ {category_en}_{quality_level}_subtable.csv\n"

        summary += f"\n**æ“ä½œç»“æœ:**\n"
        summary += f"âœ… æˆåŠŸå°†åŸå§‹CSVæ–‡ä»¶æŒ‰ {len(created_subtables)} ä¸ªåˆ†ç±»è¿›è¡Œåˆ†å‰²\n"
        summary += f"âœ… æ¯ä¸ªåˆ†ç±»æŒ‰ {len(quality_levels)} ä¸ªè´¨é‡ç­‰çº§è¿›ä¸€æ­¥ç»†åˆ†\n"
        summary += f"âœ… æ€»å…±ç”Ÿæˆ {total_files_created} ä¸ªå­è¡¨æ–‡ä»¶\n"
        summary += f"âœ… æ‰€æœ‰å­è¡¨å‡åŒ…å« {len(common_columns)} ä¸ªå…¬å…±åˆ—ç”¨äºæ•°æ®å…³è”\n"
        summary += f"âœ… æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³: {output_dir}\n"

        if len(created_subtables) < 6:
            missing_categories = set(
                ["å†…å­˜", "åŠŸè€—", "å…¶ä»–", "å¹³å‡å¸§ç‡", "å¡é¡¿", "æ¸©åº¦"]
            ) - set(created_subtables.keys())
            summary += f"\nâš ï¸ æ³¨æ„: ä»¥ä¸‹åˆ†ç±»æœªæ‰¾åˆ°å¯¹åº”å­—æ®µï¼Œæœªç”Ÿæˆå­è¡¨: {', '.join(missing_categories)}\n"

        summary += f"\n**ä½¿ç”¨å»ºè®®:**\n"
        summary += f"- å„è´¨é‡ç­‰çº§çš„å­è¡¨å¯ç‹¬ç«‹è¿›è¡Œæ€§èƒ½åˆ†æ\n"
        summary += f"- åˆ©ç”¨å…¬å…±åˆ—è¿›è¡Œè·¨å­è¡¨æ•°æ®å…³è”åˆ†æ\n"
        summary += f"- å¯æŒ‰è´¨é‡ç­‰çº§è¿›è¡ŒåŒç±»è®¾å¤‡çš„æ¨ªå‘å¯¹æ¯”åˆ†æ\n"
        summary += f"- å¯æŒ‰æ€§èƒ½æŒ‡æ ‡ç±»åˆ«è¿›è¡Œä¸åŒè´¨é‡ç­‰çº§çš„çºµå‘åˆ†æ\n"
        summary += f"- é’ˆå¯¹ä¸åŒæ€§èƒ½æŒ‡æ ‡ç±»åˆ«å’Œè´¨é‡ç­‰çº§é‡‡ç”¨ä¸“é—¨çš„åˆ†ææ–¹æ³•\n"
        summary += f"- å»ºè®®å¯¹å„ç±»åˆ«å’Œè´¨é‡ç­‰çº§åˆ†åˆ«è¿›è¡Œç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–\n"

        return summary
