import os
from typing import List

import pandas as pd

from app.exceptions import ToolError
from app.tool.base import BaseTool


class ExcelCleanTool(BaseTool):
    """
    A tool for cleaning Excel files (CSV/TSV) by removing columns that are entirely empty
    or contain identical values throughout the column.
    åœ¨group_columnåˆ—ä¸­ï¼ŒæŽ§åˆ¶ç»„å’Œå®žéªŒç»„çš„å€¼åˆ†åˆ«ä¸º"Control"å’Œ"Test"ï¼Œç”¨äºŽåŒºåˆ†ä¸¤ç»„æ•°æ®ã€‚
    åœ¨åŒä¸€ä¸ªdevicemodelä¸‹ï¼Œå¦‚æžœcontrolç»„å’Œtestç»„ä¸­æœ‰ä¸€ä¸ªç»„çš„æ•°ç›®ä¸ºé›¶ï¼Œä¹Ÿéœ€è¦è¿›è¡Œå‰”é™¤
    """

    name: str = "excel_clean_tool"
    description: str = (
        "Clean Excel files (CSV/TSV) by removing empty columns and columns with identical values."
    )
    parameters: dict = {
        "type": "object",
        "properties": {
            "file_path": {
                "type": "string",
                "description": "Path to the CSV or TSV file to clean",
            },
            "null_threshold": {
                "type": "number",
                "description": "Threshold for null values ratio (0.0-1.0). Columns with null ratio >= threshold will be removed",
                "default": 1.0,
            },
            "remove_constant_columns": {
                "type": "boolean",
                "description": "Whether to remove columns with identical non-null values",
                "default": True,
            },
            "group_column": {
                "type": "string",
                "description": "Column name that identifies control/test groups",
                "default": "param_value",
            },
            "control_value": {
                "type": "string",
                "description": "Value in group_column that represents the control group",
                "default": "control",
            },
            "test_value": {
                "type": "string",
                "description": "Value in group_column that represents the test group",
                "default": "test",
            },
            "encoding": {
                "type": "string",
                "description": "File encoding to use (default: utf-8)",
                "default": "utf-8",
            },
        },
        "required": ["file_path"],
    }

    async def execute(
        self,
        file_path: str,
        null_threshold: float = 1.0,
        remove_constant_columns: bool = True,
        group_column: str = "param_value",
        control_value: str = "control",
        test_value: str = "test",
        encoding: str = "utf-8",
    ) -> str:
        """
        Clean Excel file by removing columns that are entirely empty or contain identical values.
        """
        try:
            # Validate input file path
            if not os.path.exists(file_path):
                raise ToolError(f"File not found: {file_path}")

            # Determine file type and separator
            file_ext = os.path.splitext(file_path)[1].lower()
            if file_ext == ".csv":
                sep = ","
            elif file_ext == ".tsv":
                sep = "\t"
            else:
                raise ToolError(
                    f"Unsupported file extension: {file_ext}. Supported: .csv, .tsv"
                )

            # Read the input file
            try:
                df = pd.read_csv(
                    file_path, sep=sep, encoding=encoding, low_memory=False
                )
            except Exception as e:
                raise ToolError(f"Error reading file: {str(e)}")

            # ðŸ”§ å¤„ç†é‡å¤åˆ—åé—®é¢˜
            original_columns = df.columns.tolist()
            if len(original_columns) != len(set(original_columns)):
                # å‘çŽ°é‡å¤åˆ—åï¼Œè¿›è¡Œå¤„ç†
                seen = {}
                new_columns = []
                for col in original_columns:
                    if col in seen:
                        seen[col] += 1
                        new_col = f"{col}_{seen[col]}"
                        new_columns.append(new_col)
                        print(f"âš ï¸  é‡å¤åˆ—å '{col}' é‡å‘½åä¸º '{new_col}'")
                    else:
                        seen[col] = 0
                        new_columns.append(col)

                df.columns = new_columns
                print(
                    f"âœ… å¤„ç†äº† {len(original_columns) - len(set(original_columns))} ä¸ªé‡å¤åˆ—å"
                )

            # ðŸ”§ é‡ç½®ç´¢å¼•ä»¥é¿å…é‡å¤ç´¢å¼•é—®é¢˜
            df = df.reset_index(drop=True)

            # Store original shape for reporting
            original_rows, original_cols = df.shape

            # Track dropped columns and reasons
            columns_dropped = []
            columns_dropped_reasons = {}

            # ðŸ”§ éªŒè¯å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            if group_column not in df.columns:
                raise ToolError(
                    f"Group column '{group_column}' not found in the file. Available columns: {list(df.columns)}"
                )

            if "devicemodel" not in df.columns:
                raise ToolError(
                    f"'devicemodel' column not found in the file. Available columns: {list(df.columns)}"
                )

            # Identify columns to drop
            columns_to_drop = []

            # 1. Check for columns with null values exceeding the threshold
            for col in df.columns:
                null_ratio = df[col].isna().mean()
                if null_ratio >= null_threshold:
                    columns_to_drop.append(col)
                    columns_dropped_reasons[col] = f"Empty ratio: {null_ratio:.2%}"

            # 2. Check for columns with identical values (if enabled)
            if remove_constant_columns:
                for col in df.columns:
                    if col not in columns_to_drop:  # Skip already identified columns
                        # Check if all non-null values are identical
                        unique_values = df[col].dropna().unique()
                        if len(unique_values) <= 1:
                            columns_to_drop.append(col)
                            if len(unique_values) == 0:
                                columns_dropped_reasons[col] = "All values are null"
                            else:
                                columns_dropped_reasons[col] = (
                                    f"All values are identical: {unique_values[0]}"
                                )

            # Drop the identified columns
            df_cleaned = df.drop(columns=columns_to_drop)
            columns_dropped = columns_to_drop

            # ðŸ”§ æ£€æŸ¥åˆ†ç»„æ•°æ®å‰ï¼Œå…ˆç¡®ä¿æ•°æ®ç±»åž‹æ­£ç¡®
            try:
                # ç¡®ä¿group_columnçš„å€¼æ˜¯å­—ç¬¦ä¸²ç±»åž‹ï¼Œä¾¿äºŽæ¯”è¾ƒ
                df_cleaned[group_column] = (
                    df_cleaned[group_column].astype(str).str.lower()
                )
                control_value_lower = control_value.lower()
                test_value_lower = test_value.lower()

                # check for group_column and count every devicemodel control and test size
                control_data = df_cleaned[
                    df_cleaned[group_column] == control_value_lower
                ].copy()
                test_data = df_cleaned[
                    df_cleaned[group_column] == test_value_lower
                ].copy()

                print(
                    f"ðŸ“Š Controlç»„è®°å½•æ•°: {len(control_data)}, Testç»„è®°å½•æ•°: {len(test_data)}"
                )

                if len(control_data) == 0:
                    raise ToolError(
                        f"No records found for control group value '{control_value}' in column '{group_column}'"
                    )
                if len(test_data) == 0:
                    raise ToolError(
                        f"No records found for test group value '{test_value}' in column '{group_column}'"
                    )

            except Exception as e:
                raise ToolError(f"Error processing group data: {str(e)}")

            # device model set
            control_devices = set(control_data["devicemodel"].dropna().unique())
            test_devices = set(test_data["devicemodel"].dropna().unique())
            all_devices = control_devices.union(test_devices)

            print(
                f"ðŸ“± å‘çŽ°è®¾å¤‡åž‹å·: Control={len(control_devices)}, Test={len(test_devices)}, æ€»è®¡={len(all_devices)}"
            )

            # è®°å½•éœ€è¦åˆ é™¤çš„è®¾å¤‡æ¨¡åž‹
            devices_to_remove = set()
            for device in all_devices:
                control_count = control_data[
                    control_data["devicemodel"] == device
                ].shape[0]
                test_count = test_data[test_data["devicemodel"] == device].shape[0]

                # å¦‚æžœæŽ§åˆ¶ç»„æˆ–æµ‹è¯•ç»„çš„æ•°é‡ä¸ºé›¶ï¼Œåˆ™æ·»åŠ åˆ°åˆ é™¤åˆ—è¡¨
                if control_count == 0 or test_count == 0:
                    devices_to_remove.add(device)
                    print(
                        f"ðŸ—‘ï¸  å°†åˆ é™¤è®¾å¤‡ {device}: Control={control_count}, Test={test_count}"
                    )

            # å¦‚æžœæœ‰éœ€è¦åˆ é™¤çš„è®¾å¤‡æ¨¡åž‹ï¼Œæ‰§è¡Œåˆ é™¤æ“ä½œ
            if devices_to_remove:
                # è®°å½•è¢«åˆ é™¤çš„è®¾å¤‡æ¨¡åž‹åŠå…¶åŽŸå› 
                for device in devices_to_remove:
                    control_count = control_data[
                        control_data["devicemodel"] == device
                    ].shape[0]
                    test_count = test_data[test_data["devicemodel"] == device].shape[0]
                    reason = f"Control count: {control_count}, Test count: {test_count}"
                    columns_dropped_reasons[f"device_{device}"] = reason

                # ä»Žcleanedæ•°æ®ä¸­åˆ é™¤è¿™äº›è®¾å¤‡æ¨¡åž‹çš„æ‰€æœ‰è®°å½•
                df_cleaned = df_cleaned[
                    ~df_cleaned["devicemodel"].isin(devices_to_remove)
                ]
                print(f"âœ… åˆ é™¤äº† {len(devices_to_remove)} ä¸ªè®¾å¤‡åž‹å·çš„æ•°æ®")

            # ðŸ”§ æœ€ç»ˆæ¸…ç†ï¼šé‡ç½®ç´¢å¼•å¹¶ç¡®ä¿æ²¡æœ‰é‡å¤
            df_cleaned = df_cleaned.reset_index(drop=True)

            # Generate output file path
            output_path = self._generate_output_path(file_path)

            # Save the cleaned dataframe
            df_cleaned.to_csv(output_path, sep=sep, index=False, encoding=encoding)

            # Prepare result message
            cleaned_rows, cleaned_cols = df_cleaned.shape
            removed_cols = original_cols - cleaned_cols
            removed_rows = original_rows - cleaned_rows

            print(
                f"âœ… æ¸…ç†å®Œæˆ: {original_rows}Ã—{original_cols} â†’ {cleaned_rows}Ã—{cleaned_cols}"
            )

            # Generate detailed report
            result_message = self._generate_result_message(
                file_path,
                output_path,
                original_rows,
                original_cols,
                cleaned_rows,
                cleaned_cols,
                columns_dropped,
                columns_dropped_reasons,
                removed_rows,
            )

            return result_message

        except ToolError as e:
            return str(e)
        except Exception as e:
            error_msg = f"Unexpected error while cleaning file: {str(e)}"
            print(f"âŒ {error_msg}")
            return error_msg

    def _generate_output_path(self, file_path: str) -> str:
        """Generate output file path by adding '_cleaned' before the extension"""
        base_name, extension = os.path.splitext(file_path)
        return f"{base_name}_cleaned{extension}"

    def _generate_result_message(
        self,
        input_path: str,
        output_path: str,
        original_rows: int,
        original_cols: int,
        cleaned_rows: int,
        cleaned_cols: int,
        columns_dropped: List[str],
        columns_dropped_reasons: dict,
        removed_rows: int = 0,
    ) -> str:
        """Generate a detailed result message"""

        # Basic statistics
        removed_cols = original_cols - cleaned_cols
        col_removed_pct = (
            (removed_cols / original_cols * 100) if original_cols > 0 else 0
        )
        row_removed_pct = (
            (removed_rows / original_rows * 100) if original_rows > 0 else 0
        )

        message = [
            "âœ… Excel File Cleaning Complete!",
            "",
            "ðŸ“Š Cleaning Summary:",
            f"â€¢ Input File: {input_path}",
            f"â€¢ Output File: {output_path}",
            f"â€¢ Original Shape: {original_rows} rows Ã— {original_cols} columns",
            f"â€¢ Cleaned Shape: {cleaned_rows} rows Ã— {cleaned_cols} columns",
            f"â€¢ Columns Removed: {removed_cols} ({col_removed_pct:.1f}% of total)",
            f"â€¢ Rows Removed: {removed_rows} ({row_removed_pct:.1f}% of total)",
            "",
        ]

        # Add detailed info about removed columns
        if columns_dropped:
            message.append("ðŸ—‘ï¸ Removed Columns:")
            # Show at most 10 columns with reasons
            for i, col in enumerate(columns_dropped[:10]):
                reason = columns_dropped_reasons.get(col, "Unknown reason")
                message.append(f"â€¢ {col}: {reason}")

            if len(columns_dropped) > 10:
                message.append(f"â€¢ ... and {len(columns_dropped) - 10} more columns")
        else:
            message.append("â„¹ï¸ No columns were removed.")

        # æ·»åŠ è¢«åˆ é™¤çš„è®¾å¤‡æ¨¡åž‹ä¿¡æ¯
        device_removed = [k for k in columns_dropped_reasons if k.startswith("device_")]
        if device_removed:
            message.append("\nðŸ“± Removed Device Models (Control/Test count mismatch):")
            for i, device_key in enumerate(device_removed[:10]):
                device = device_key.replace("device_", "")
                reason = columns_dropped_reasons.get(device_key, "Unknown reason")
                message.append(f"â€¢ {device}: {reason}")

            if len(device_removed) > 10:
                message.append(
                    f"â€¢ ... and {len(device_removed) - 10} more device models"
                )

        return "\n".join(message)
